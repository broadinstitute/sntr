{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import ast\n",
    "import datetime\n",
    "import json\n",
    "from scipy.stats import mannwhitneyu\n",
    "import random\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import networkx as nx\n",
    "from scipy.stats import mannwhitneyu\n",
    "import statistics\n",
    "from networkx.algorithms import assortativity\n",
    "from scipy.signal import savgol_filter\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy import signal\n",
    "import math\n",
    "import string\n",
    "import scipy\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barplot_annotate_brackets(num1, num2, data, center, height, yerr=None, dh=.055, barh=.055, fs=None, maxasterix=None):\n",
    "    if type(data) is str:\n",
    "        text = data\n",
    "    else:\n",
    "        # * is p < 0.05, ** is p < 0.005, *** is p < 0.0005, etc.\n",
    "        text = ''\n",
    "        p = .05\n",
    "        counter = 0\n",
    "        text = ''\n",
    "        while data < p:\n",
    "            p /= 10.\n",
    "            counter+=1\n",
    "            if maxasterix and counter == maxasterix:\n",
    "                text = 'P < .0005'\n",
    "                break\n",
    "        if counter == 0:\n",
    "            text = 'not significant'\n",
    "        if counter == 1:\n",
    "            text = 'P < .05'\n",
    "        if counter == 2: \n",
    "            text = 'P < .005'\n",
    "    lx, ly = center[num1], height[num1]\n",
    "    rx, ry = center[num2], height[num2]\n",
    "    if yerr:\n",
    "        ly += yerr[num1]\n",
    "        ry += yerr[num2]\n",
    "    ax_y0, ax_y1 = plt.gca().get_ylim()\n",
    "    dh *= (ax_y1 - ax_y0)\n",
    "    barh *= (ax_y1 - ax_y0)\n",
    "    y = max(ly, ry) + dh\n",
    "    barx = [lx, lx, rx, rx]\n",
    "    bary = [y, y+barh, y+barh, y]\n",
    "    mid = ((lx+rx)/2, y+barh)\n",
    "    plt.plot(barx, bary, c='black')\n",
    "    kwargs = dict(ha='center', va='bottom')\n",
    "    if fs is not None:\n",
    "        kwargs['fontsize'] = fs\n",
    "    plt.text(*mid, text, **kwargs)\n",
    "\n",
    "def fix_format_and_replace_pos_barcodes(sem_interactions,barcode_dict,filename):\n",
    "    new_rows = []\n",
    "    count = 0\n",
    "    for row,val in sem_interactions.iterrows():\n",
    "        interactions_list = val['interactions']\n",
    "        if (pd.isna(interactions_list)):\n",
    "            user = val['user']\n",
    "            converted_user = user\n",
    "            if user in barcode_dict.keys():\n",
    "                converted_user = barcode_dict[user]\n",
    "            to_add = [val['day'],converted_user,val['building'],float(\"nan\"),float(\"nan\")]\n",
    "            new_rows.append(to_add)\n",
    "        else:\n",
    "            interactions_list = [interactions_list[1:-1]]\n",
    "            for x in interactions_list:\n",
    "                y = ast.literal_eval(x.replace('[','[\\\"').replace(',','\",').replace('\\'\\\",','\\','))\n",
    "                interactions = []\n",
    "                if type(y)!=str:\n",
    "                    for item in list(y):\n",
    "                        new_int = ast.literal_eval(item)\n",
    "                        id = new_int[0]\n",
    "                        if id in barcode_dict.keys():\n",
    "                            converted_id = barcode_dict[id]\n",
    "                            new_int = [converted_id,new_int[1]]\n",
    "                        interactions.append(new_int)\n",
    "                else:\n",
    "                    new_int = ast.literal_eval(y)\n",
    "                    id = new_int[0]\n",
    "                    if id in barcode_dict.keys():\n",
    "                        converted_id = barcode_dict[id]\n",
    "                        new_int = [converted_id,new_int[1]]\n",
    "                    interactions.append(new_int)\n",
    "            user = val['user']\n",
    "            converted_user= user\n",
    "            if user in barcode_dict.keys():\n",
    "                converted_user = barcode_dict[user]\n",
    "            prefix = [val['day'],converted_user,val['building']]\n",
    "            for x in interactions:\n",
    "                new_list = prefix + x\n",
    "                to_add = new_list\n",
    "                new_rows.append(to_add)\n",
    "    interactions_full = pd.DataFrame(new_rows,columns = ['day','user_1','building','user_2','duration'])\n",
    "    interactions_full.to_csv(filename,index = False)\n",
    "\n",
    "def make_by_ap_cleaned_interaction_lists(sem_interactions,guest_list,barcode_dict,filename,non_students_list):\n",
    "    sem_interactions = sem_interactions.drop(columns = ['start_ts','end_ts','interactions'])\n",
    "    sem_interactions = sem_interactions[~sem_interactions['user'].isin(guest_list)]\n",
    "    sem_interactions = sem_interactions[~sem_interactions['user'].isin(non_students_list)]\n",
    "\n",
    "    new_rows = []\n",
    "    for row, val in sem_interactions.iterrows():\n",
    "        user = val['user']\n",
    "        day = val['day']\n",
    "        duration = val['duration']\n",
    "        building = val['building']\n",
    "        ap = val['ap']\n",
    "        int_count = val['interaction_count']\n",
    "        if user in barcode_dict.keys():\n",
    "            user = barcode_dict[user]\n",
    "        to_add = [day,user,duration,building,ap,int_count]\n",
    "        new_rows.append(to_add)\n",
    "    sem_interactions_final = pd.DataFrame(new_rows,columns = ['day','user','duration','building','ap','int_count'])\n",
    "    sem_interactions_final.to_csv(filename,index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in all guest/student flag file\n",
    "authentication_info = pd.read_csv('authenticated_student_file.csv', compression='gzip', header=0, sep=',', quotechar='\"')\n",
    "\n",
    "students = authentication_info[authentication_info['is_student'] == True]\n",
    "students = list(students['identity'].values)\n",
    "\n",
    "non_students = authentication_info[authentication_info['is_student'] == False]\n",
    "non_students_list= list(non_students['identity'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in concatenated wifi data\n",
    "cleaned_new_full = pd.read_csv('new_full_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cut dates\n",
    "cut_summer = cleaned_new_full[cleaned_new_full['day']>='2020-08-17']\n",
    "cut_summer2 = cut_summer[cut_summer['day']<='2021-04-30']\n",
    "cut_break = cut_summer2.drop(cut_summer2[((cut_summer2['day']<='2021-01-18') & (cut_summer2['day']>='2020-11-20'))].index)\n",
    "new_cleandate_file = cut_break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull in barcode key cmu 2 da\n",
    "barcode_key = pd.read_csv('cmu_2_da.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean na barcodes\n",
    "barcode_key_cleaned = barcode_key.dropna().reset_index()\n",
    "\n",
    "#make barcode dict mapping user to cmu barcode\n",
    "barcode_dict = {}\n",
    "for row,val in barcode_key_cleaned.iterrows():\n",
    "    barcode_dict[val['user']] = val['barcode']\n",
    "\n",
    "#create guest list (ie.users that only came on campus 1-3 days during year valid dates\n",
    "guest_list = []\n",
    "for row, val in new_cleandate_file.groupby('user').count().iterrows():\n",
    "    if val['day'] < 4:\n",
    "        guest_list.append(row)\n",
    "\n",
    "#create fall interactions file\n",
    "fall_interactions = new_cleandate_file[new_cleandate_file['day'] >= '2020-08-17']\n",
    "fall_interactions = fall_interactions[fall_interactions['day'] <= '2020-11-20']\n",
    "\n",
    "#create spring interactions file\n",
    "spring_interactions = new_cleandate_file[new_cleandate_file['day'] >= '2021-01-18']\n",
    "spring_interactions = spring_interactions[spring_interactions['day'] <= '2021-04-30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count all users precleaning\n",
    "all_users = []\n",
    "for row, val in new_cleandate_file.groupby('user').count().iterrows():\n",
    "    all_users.append(row)\n",
    "\n",
    "#count all users in fall semester precleaning\n",
    "fall_users = []\n",
    "for row, val in fall_interactions.groupby('user').count().iterrows():\n",
    "    fall_users.append(row)\n",
    "\n",
    "#count all users in spring semester precleaning\n",
    "spring_users = []\n",
    "for row, val in spring_interactions.groupby('user').count().iterrows():\n",
    "    spring_users.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count all users before and after cleaning with no repeats of users by using sets\n",
    "users_after_cleaning_new = set(all_users) - (set(non_students_list + guest_list))\n",
    "print('all users before cleaning',len(all_users))\n",
    "print('new_cleaning', len(users_after_cleaning_new))\n",
    "\n",
    "#count fall users before and after cleaning with no repeats of users by using sets\n",
    "users_after_cleaning_new = set(fall_users) - (set(non_students_list + guest_list))\n",
    "print('fall users before cleaning',len(fall_users))\n",
    "print('new user count fall', len(users_after_cleaning_new))\n",
    "\n",
    "\n",
    "#count spring users before and after cleaning with no repeats of users by using sets\n",
    "users_after_cleaning_new = set(spring_users) - (set(non_students_list + guest_list))\n",
    "print('spring users before cleaning',len(spring_users))\n",
    "print('new user count spring', len(users_after_cleaning_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing solo ints\n",
    "fall_interactions_no_solos = fall_interactions.dropna().reset_index()\n",
    "spring_interactions_no_solos = spring_interactions.dropna().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix_format_and_replace_pos_barcodes(fall_interactions,barcode_dict,'fall_interactions_full.csv')\n",
    "fall_interactions_full = pd.read_csv('fall_interactions_full.csv') #does not include one way ints\n",
    "\n",
    "#fix_format_and_replace_pos_barcodes(spring_interactions,barcode_dict,'spring_interactions_full.csv')\n",
    "spring_interactions_full = pd.read_csv('spring_interactions_full.csv') #does not include one way ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('running')\n",
    "#make_by_ap_cleaned_interaction_lists(fall_interactions,guest_list,barcode_dict,'fall_interactions_by_ap_cleaned.csv',non_students_list)\n",
    "#print('halfway')\n",
    "#make_by_ap_cleaned_interaction_lists(spring_interactions,guest_list,barcode_dict,'spring_interactions_by_ap_cleaned.csv',non_students_list)\n",
    "#print('just reading')\n",
    "fall_interactions_by_ap = pd.read_csv('fall_interactions_by_ap_cleaned.csv')\n",
    "spring_interactions_by_ap = pd.read_csv('spring_interactions_by_ap_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fall_interactions_by_ap_for_counts = fall_interactions_by_ap.copy()\n",
    "fall_interactions_by_ap_for_counts['int_count'] = [0 if x == -1 else x for x in fall_interactions_by_ap_for_counts['int_count']]\n",
    "\n",
    "spring_interactions_by_ap_for_counts = spring_interactions_by_ap.copy()\n",
    "spring_interactions_by_ap_for_counts['int_count'] = [0 if x == -1 else x for x in spring_interactions_by_ap_for_counts['int_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spring_interactions_by_ap_for_counts = spring_interactions_by_ap_for_counts[~spring_interactions_by_ap_for_counts['user'].isin(guest_list)]\n",
    "fall_interactions_by_ap_for_counts = fall_interactions_by_ap_for_counts[~fall_interactions_by_ap_for_counts['user'].isin(guest_list)]\n",
    "spring_interactions_by_ap_for_counts = spring_interactions_by_ap_for_counts[~spring_interactions_by_ap_for_counts['user'].isin(non_students_list)]\n",
    "fall_interactions_by_ap_for_counts = fall_interactions_by_ap_for_counts[~fall_interactions_by_ap_for_counts['user'].isin(non_students_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove guests and stagant devices from spring and fall full un-duped interactions files\n",
    "fall_interactions_cleaned = fall_interactions_full[~fall_interactions_full['user_1'].isin(guest_list)]\n",
    "fall_interactions_cleaned = fall_interactions_cleaned[~fall_interactions_cleaned['user_2'].isin(guest_list)]\n",
    "fall_interactions_cleaned = fall_interactions_cleaned[~fall_interactions_cleaned['user_1'].isin(non_students_list)]\n",
    "fall_interactions_cleaned = fall_interactions_cleaned[~fall_interactions_cleaned['user_2'].isin(non_students_list)]\n",
    "\n",
    "spring_interactions_cleaned = spring_interactions_full[~spring_interactions_full['user_1'].isin(guest_list)]\n",
    "spring_interactions_cleaned = spring_interactions_cleaned[~spring_interactions_cleaned['user_2'].isin(guest_list)]\n",
    "spring_interactions_cleaned = spring_interactions_cleaned[~spring_interactions_cleaned['user_1'].isin(non_students_list)]\n",
    "spring_interactions_cleaned = spring_interactions_cleaned[~spring_interactions_cleaned['user_2'].isin(non_students_list)]\n",
    "\n",
    "print(len(fall_interactions_cleaned),len(fall_interactions_full))\n",
    "print(len(spring_interactions_cleaned),len(spring_interactions_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('metadata_all.csv')\n",
    "sequenced = metadata[~metadata['Sequence_ID'].isnull()]\n",
    "peacock = metadata[metadata['pango_lineage'] == 'B.1.429.1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fall:\n",
    "fall_interactions_cleaned_barcodes_user1s = list(fall_interactions_cleaned['user_1'].values)\n",
    "fall_interactions_cleaned_barcodes_user2s = list(fall_interactions_cleaned['user_2'].values)\n",
    "fall_interactions_cleaned_barcodes = list(set(fall_interactions_cleaned_barcodes_user1s + fall_interactions_cleaned_barcodes_user2s))\n",
    "fall_interactions_cleaned_barcodes = [x for x in fall_interactions_cleaned_barcodes if pd.isna(x) == False]\n",
    "\n",
    "#spring:\n",
    "spring_interactions_cleaned_barcodes_user1s = list(spring_interactions_cleaned['user_1'].values)\n",
    "spring_interactions_cleaned_barcodes_user2s = list(spring_interactions_cleaned['user_2'].values)\n",
    "spring_interactions_cleaned_barcodes = list(set(spring_interactions_cleaned_barcodes_user1s + spring_interactions_cleaned_barcodes_user2s))\n",
    "spring_interactions_cleaned_barcodes = [x for x in spring_interactions_cleaned_barcodes if pd.isna(x) == False]\n",
    "\n",
    "sequenced_barcodes = sequenced.dropna(subset = ['barcode'])['barcode'].values\n",
    "positive_barcodes = metadata.dropna(subset = ['barcode'])['barcode'].values\n",
    "\n",
    "metadata_of_positives = metadata.dropna(subset = ['Class_Year'])\n",
    "fall_metadata_of_positives = metadata_of_positives[metadata_of_positives['Test_day'] <= '2020-11-20']\n",
    "fall_metadata_of_positives = fall_metadata_of_positives[fall_metadata_of_positives['Test_day'] >= '2020-08-17'].reset_index(drop = True)\n",
    "fall_metadata_of_positives_barcodes = fall_metadata_of_positives.dropna(subset = ['barcode'])['barcode'].values\n",
    "\n",
    "spring_metadata_of_positives = metadata_of_positives[metadata_of_positives['Test_day'] <= '2021-04-30']\n",
    "spring_metadata_of_positives = spring_metadata_of_positives[spring_metadata_of_positives['Test_day'] >= '2021-01-18'].reset_index(drop = True)\n",
    "spring_metadata_of_positives_barcodes = spring_metadata_of_positives.dropna(subset = ['barcode'])['barcode'].values\n",
    "\n",
    "sequenced_with_classyear = sequenced.dropna(subset = ['Class_Year'])\n",
    "sequenced_with_classyear = sequenced_with_classyear[sequenced_with_classyear['Class_Year'] != 'Year E']\n",
    "\n",
    "fall_sequenced = sequenced_with_classyear[sequenced_with_classyear['Test Date'] <= '2020-11-20']\n",
    "fall_sequenced = fall_sequenced[fall_sequenced['Test Date'] >= '2020-08-17'].reset_index(drop = True)\n",
    "fall_sequenced_barcodes = fall_sequenced.dropna(subset = ['barcode'])['barcode'].values\n",
    "\n",
    "spring_sequenced = sequenced_with_classyear[sequenced_with_classyear['Test Date'] <= '2021-04-30']\n",
    "spring_sequenced = spring_sequenced[spring_sequenced['Test Date'] >= '2021-01-18'].reset_index(drop = True)\n",
    "spring_sequenced_barcodes = spring_sequenced.dropna(subset = ['barcode'])['barcode'].values\n",
    "\n",
    "fall_wifi_count_positives = []\n",
    "fall_wifi_count_sequenced = []\n",
    "fall_wifi_count_pos_and_seq = []\n",
    "\n",
    "for user in fall_interactions_cleaned_barcodes:\n",
    "    if (user in fall_metadata_of_positives_barcodes) and (user in fall_sequenced_barcodes):\n",
    "        fall_wifi_count_pos_and_seq.append(user)\n",
    "    if user in fall_sequenced_barcodes:\n",
    "        fall_wifi_count_sequenced.append(user)\n",
    "    if user in fall_metadata_of_positives_barcodes:\n",
    "        fall_wifi_count_positives.append(user)\n",
    "\n",
    "\n",
    "print('all fall seq regardless of wifi', len(fall_sequenced_barcodes))\n",
    "print('all fall pos regardless of wifi', len(fall_metadata_of_positives_barcodes))\n",
    "print('fall pos', len(fall_wifi_count_positives))\n",
    "print('fall seq', len(fall_wifi_count_sequenced))\n",
    "print('fall pos and seq', len(fall_wifi_count_pos_and_seq))\n",
    "print()\n",
    "print()\n",
    "\n",
    "spring_wifi_count_positives = []\n",
    "spring_wifi_count_sequenced = []\n",
    "spring_wifi_count_pos_and_seq = []\n",
    "\n",
    "for user in spring_interactions_cleaned_barcodes:\n",
    "    if (user in spring_metadata_of_positives_barcodes) and (user in spring_sequenced_barcodes):\n",
    "        spring_wifi_count_pos_and_seq.append(user)\n",
    "    if user in spring_sequenced_barcodes:\n",
    "        spring_wifi_count_sequenced.append(user)\n",
    "    if user in spring_metadata_of_positives_barcodes:\n",
    "        spring_wifi_count_positives.append(user)\n",
    "\n",
    "\n",
    "print('all spring seq regardless of wifi', len(spring_sequenced_barcodes))\n",
    "print('all spring pos regardless of wifi', len(spring_metadata_of_positives_barcodes))\n",
    "print('spring pos', len(spring_wifi_count_positives))\n",
    "print('spring seq', len(spring_wifi_count_sequenced))\n",
    "print('spring pos and seq', len(spring_wifi_count_pos_and_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dedup Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_sem_no_solos(semester_interactions_cleaned):\n",
    "    interactions_no_solos = semester_interactions_cleaned.dropna().reset_index()\n",
    "    interactions_no_solos['pair'] = '[\\\"' + interactions_no_solos['user_1']+'\\\",\\\"'+interactions_no_solos['user_2'] +'\\\"]'\n",
    "    interactions_no_solos['pair'] = [json.loads(x) for x in interactions_no_solos['pair']]\n",
    "    interactions_no_solos['pair'] = [str(sorted(x)) for x in interactions_no_solos['pair']]\n",
    "    interactions_no_solos = interactions_no_solos.reset_index(drop = True)\n",
    "    interactions_no_solos = interactions_no_solos[interactions_no_solos['user_1'] != interactions_no_solos['user_2']]\n",
    "    grouped_by_pair = interactions_no_solos.groupby(['day','pair','user_1']).agg({'duration':['sum','median','count']}).reset_index()\n",
    "    grouped_by_pair.columns = ['day','pair','user1','duration_sum','duration_median','int_count']\n",
    "    return grouped_by_pair,interactions_no_solos\n",
    "\n",
    "def run_dedup_and_clean_fall(grouped_by_pair_fall,fall_interactions_no_solos):\n",
    "    #producing only pairs to see if we have two for each pair (what we would expect)\n",
    "    pair_counts = grouped_by_pair_fall.groupby(['day','pair']).size().reset_index()\n",
    "\n",
    "    #finding any pairs that don't have 2 interactions when all interaction info is summed\n",
    "    pair_counts.columns = ['day','pair','count']\n",
    "    faulty_pair_counts = pair_counts[pair_counts['count'] != 2]\n",
    "\n",
    "    new_holder_cleaned = grouped_by_pair_fall\n",
    "\n",
    "    #spliting into two lists that create pairwise comparisons of the same pairings\n",
    "    new_holder1 = new_holder_cleaned[::2]\n",
    "    new_holder2 =  new_holder_cleaned[1::2]\n",
    "\n",
    "    #checking pairwise matching after clean of wonky 1-ways\n",
    "    vals_h1 = list(new_holder1['pair'].values)\n",
    "    vals_h2 = list(new_holder2['pair'].values)\n",
    "    print(vals_h1 == vals_h2)\n",
    "\n",
    "    new_rows_deduped = []\n",
    "    for x in range(len(new_holder1)):\n",
    "        if new_holder1['int_count'].iloc[x] != new_holder2['int_count'].iloc[x]:\n",
    "            if new_holder1['int_count'].iloc[x] > new_holder2['int_count'].iloc[x]:\n",
    "                file = new_holder1.iloc[x]\n",
    "            else : #new_holder1.iloc['int_count'][x] < new_holder2.iloc['int_count'][x]\n",
    "                file = new_holder2.iloc[x]\n",
    "        else: #new_holder1.iloc['int_count'][x] == new_holder2.iloc['int_count']\n",
    "            file = new_holder1.iloc[x]\n",
    "        date = file['day']\n",
    "        pair = file['pair']\n",
    "        sum = file['duration_sum']\n",
    "        median = file['duration_median']\n",
    "        count = file['int_count']\n",
    "        to_add = [date,pair,sum,median,count]\n",
    "        new_rows_deduped.append(to_add)\n",
    "    final_semester_deduped = pd.DataFrame(data = new_rows_deduped,columns = ['day','pair','sum','median','count'])\n",
    "\n",
    "    #checking to make sure half were removed\n",
    "    print(len(final_semester_deduped))\n",
    "    print(len(new_holder_cleaned))\n",
    "    print(len(new_holder_cleaned)/2)\n",
    "\n",
    "    return final_semester_deduped\n",
    "\n",
    "def run_dedup_and_clean_spring(grouped_by_pair_spring,spring_interactions_no_solos):\n",
    "    #producing only pairs to see if we have two for each pair (what we would expect)\n",
    "    pair_counts = grouped_by_pair_spring.groupby(['day','pair']).size().reset_index()\n",
    "\n",
    "    #finding any pairs that don't have 2 interactions when all interaction info is summed\n",
    "    pair_counts.columns = ['day','pair','count']\n",
    "\n",
    "    new_holder_cleaned = grouped_by_pair_spring\n",
    "\n",
    "    #spliting into two lists that create pairwise comparisons of the same pairings\n",
    "    new_holder1 = new_holder_cleaned[::2]\n",
    "    new_holder2 =  new_holder_cleaned[1::2]\n",
    "\n",
    "    #checking pairwise matching after clean of wonky 1-ways\n",
    "    vals_h1 = list(new_holder1['pair'].values)\n",
    "    vals_h2 = list(new_holder2['pair'].values)\n",
    "    print(vals_h1 == vals_h2)\n",
    "\n",
    "    new_rows_deduped = []\n",
    "    for x in range(len(new_holder1)):\n",
    "        if new_holder1['int_count'].iloc[x] != new_holder2['int_count'].iloc[x]:\n",
    "            if new_holder1['int_count'].iloc[x] > new_holder2['int_count'].iloc[x]:\n",
    "                file = new_holder1.iloc[x]\n",
    "            else : #new_holder1.iloc['int_count'][x] < new_holder2.iloc['int_count'][x]\n",
    "                file = new_holder2.iloc[x]\n",
    "        else: #new_holder1.iloc['int_count'][x] == new_holder2.iloc['int_count']\n",
    "            file = new_holder1.iloc[x]\n",
    "        date = file['day']\n",
    "        pair = file['pair']\n",
    "        sum = file['duration_sum']\n",
    "        median = file['duration_median']\n",
    "        count = file['int_count']\n",
    "        to_add = [date,pair,sum,median,count]\n",
    "        new_rows_deduped.append(to_add)\n",
    "    final_semester_deduped = pd.DataFrame(data = new_rows_deduped,columns = ['day','pair','sum','median','count'])\n",
    "\n",
    "    #checking to make sure half were removed\n",
    "    print(len(final_semester_deduped))\n",
    "    print(len(new_holder_cleaned))\n",
    "    print(len(new_holder_cleaned)/2)\n",
    "    return final_semester_deduped\n",
    "    \n",
    "def create_by_date_pos_list(semester_final,barcode_to_test_date,window_size):\n",
    "    barcode_to_test_date_dict = {}\n",
    "    by_date_pos_list = {}\n",
    "    days = list(set(semester_final['day']))\n",
    "    positives = list(set(barcode_to_test_date['barcode']))\n",
    "    for row,index in barcode_to_test_date.iterrows():\n",
    "        barcode_to_test_date_dict[index['barcode']] = index['Test Date']\n",
    "    for day in days:\n",
    "        by_date_pos_list[day] = []\n",
    "        for positive in positives:\n",
    "            test_date = datetime.fromisoformat(barcode_to_test_date_dict[positive])\n",
    "            current_date = datetime.fromisoformat(day)\n",
    "            diff = (test_date-current_date).days\n",
    "            if diff <=window_size and diff >= 0:\n",
    "                by_date_pos_list[day].append(positive)\n",
    "    return by_date_pos_list, positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barcode_to_test_date = metadata[['barcode','Test Date']]\n",
    "barcode_to_symptom_onset = metadata[['barcode','Symptom Onset']] #not currently used\n",
    "\n",
    "barcode_to_test_date_dict = pd.Series(barcode_to_test_date['Test Date'].values,index = barcode_to_test_date['barcode']).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouped_by_pair_fall,fall_interactions_no_solos = group_sem_no_solos(fall_interactions_cleaned)\n",
    "#final_fall = run_dedup_and_clean_fall(grouped_by_pair_fall,fall_interactions_no_solos)\n",
    "#final_fall['pair'] = [x.replace('\\'','') for x in final_fall['pair']]\n",
    "#final_fall['pair'] = [x.strip('][').split(', ') for x in final_fall['pair']]\n",
    "#final_fall['user1'] = [x[0] for x in final_fall['pair']]\n",
    "#final_fall['user2'] = [x[1] for x in final_fall['pair']]\n",
    "#final_fall = final_fall.drop(columns = 'pair')\n",
    "#final_fall = final_fall.reindex(columns = ['day','user1','user2','sum','median','count'])\n",
    "#final_fall.to_csv('final_fall.csv',index = False)\n",
    "\n",
    "final_fall = pd.read_csv('final_fall.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouped_by_pair_spring,spring_interactions_no_solos = group_sem_no_solos(spring_interactions_cleaned)\n",
    "#final_spring = run_dedup_and_clean_spring(grouped_by_pair_spring,spring_interactions_no_solos)\n",
    "#final_spring['pair'] = [x.replace('\\'','') for x in final_spring['pair']]\n",
    "#final_spring['pair'] = [x.strip('][').split(', ') for x in final_spring['pair']]\n",
    "#final_spring['user1'] = [x[0] for x in final_spring['pair']]\n",
    "#final_spring['user2'] = [x[1] for x in final_spring['pair']]\n",
    "#final_spring = final_spring.drop(columns = 'pair')\n",
    "#final_spring = final_spring.reindex(columns = ['day','user1','user2','sum','median','count'])\n",
    "#final_spring.to_csv('final_spring.csv',index = False)\n",
    "\n",
    "\n",
    "final_spring = pd.read_csv('final_spring.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_pallet = [\"#000000\",\"#F0E442\",\"#CC79A7\",\"#E69F00\",\"#56B4E9\",\"#0072B2\",\"#D55E00\"]\n",
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager as fm\n",
    "fe = fm.FontEntry(\n",
    "    fname='C:/Users/brync/anaconda3/Lib/site-packages/matplotlib/mpl-data/fonts/ttf/Montserrat.ttf',\n",
    "    name='Montserrat')\n",
    "fm.fontManager.ttflist.insert(0, fe)\n",
    "mpl.rcParams['font.family'] = fe.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counts for Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencing_info = pd.read_csv('sequencing_info.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencing_info = sequencing_info[['specimen','IS_CMU','faculty_or_staff','study_semester','genbank_accession']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencing_info = sequencing_info[sequencing_info['IS_CMU'] == True]\n",
    "sequencing_info = sequencing_info[sequencing_info['faculty_or_staff'] == 'No']\n",
    "sequencing_info = sequencing_info.dropna()\n",
    "sequencing_info_fall = list(sequencing_info[sequencing_info['study_semester'] == 'Fall'].reset_index(drop = True)['specimen'].values)\n",
    "sequencing_info_spring = list(sequencing_info[sequencing_info['study_semester'] == 'Spring'].reset_index(drop = True)['specimen'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peacock_list = list(peacock['barcode'])\n",
    "positiveIDs = list(metadata['barcode'])\n",
    "all_sequenced = sequencing_info_fall+sequencing_info_spring\n",
    "\n",
    "user_first = list(fall_interactions_by_ap['user'].values)\n",
    "new_users = list(set(user_first))\n",
    "list_of_users_fall = new_users\n",
    "print('fall users',len(list_of_users_fall))\n",
    "\n",
    "user_first = list(spring_interactions_by_ap['user'].values)\n",
    "new_users = list(set(user_first))\n",
    "list_of_users_spring = new_users\n",
    "print('spring_users',len(list_of_users_spring))\n",
    "\n",
    "all_users = list(set(list_of_users_spring + list_of_users_fall))\n",
    "print('full year users',len(all_users))\n",
    "\n",
    "counter_positiveIDs = 0\n",
    "counter_sequencedIDs = 0\n",
    "counter_peacock = 0\n",
    "counter_fall_possible = 0\n",
    "for x in list_of_users_fall:\n",
    "    if x in positiveIDs:\n",
    "        counter_positiveIDs +=1\n",
    "    if x in sequencing_info_fall:\n",
    "        counter_sequencedIDs +=1\n",
    "    if x in peacock_list:\n",
    "        counter_peacock +=1\n",
    "    if x in fall_metadata_of_positives_barcodes:\n",
    "        counter_fall_possible +=1\n",
    "print('sequenced wifi fall',len(sequencing_info_fall),counter_sequencedIDs)\n",
    "print('positive wifi fall',len(positiveIDs),counter_positiveIDs)\n",
    "print('peacock wifi fall',len(peacock_list),counter_peacock)\n",
    "print('fall pos wifi fall',len(fall_metadata_of_positives_barcodes),counter_fall_possible)\n",
    "\n",
    "\n",
    "counter_positiveIDs = 0\n",
    "counter_sequencedIDs = 0\n",
    "counter_peacock = 0\n",
    "counter_spring_possible = 0\n",
    "for x in list_of_users_spring:\n",
    "    if x in positiveIDs:\n",
    "        counter_positiveIDs +=1\n",
    "    if x in sequencing_info_spring:\n",
    "        counter_sequencedIDs +=1\n",
    "    if x in peacock_list:\n",
    "        counter_peacock +=1\n",
    "    if x in spring_metadata_of_positives_barcodes:\n",
    "        counter_spring_possible +=1\n",
    "print('sequenced wifi spring',len(sequencing_info_spring),counter_sequencedIDs)\n",
    "print('positive wifi spring',len(positiveIDs),counter_positiveIDs)\n",
    "print('peacock wifi spring',len(peacock_list),counter_peacock)\n",
    "print('spring pos wifi spring',len(spring_metadata_of_positives_barcodes),counter_spring_possible)\n",
    "\n",
    "\n",
    "counter_positiveIDs = 0\n",
    "counter_sequencedIDs = 0\n",
    "counter_peacock = 0\n",
    "for x in all_users:\n",
    "    if x in positiveIDs:\n",
    "        counter_positiveIDs +=1\n",
    "    if x in all_sequenced:\n",
    "        counter_sequencedIDs +=1\n",
    "    if x in peacock_list:\n",
    "        counter_peacock +=1\n",
    "print('sequenced wifi all',len(all_sequenced),counter_sequencedIDs)\n",
    "print('positive wifi all', len(positiveIDs),counter_positiveIDs)\n",
    "print('peaock wifi all',len(peacock_list),counter_peacock)\n",
    "\n",
    "on_campus_positives = metadata[metadata.Lives_on_campus == 'Yes'].reset_index(drop = True)\n",
    "on_campus_positives_list = list(on_campus_positives['barcode'])\n",
    "\n",
    "off_campus_positives = metadata[metadata.Lives_on_campus == 'No'].reset_index(drop = True)\n",
    "off_campus_positives_list = list(off_campus_positives['barcode'])\n",
    "\n",
    "counter_on_campus_positives = 0\n",
    "counter_off_campus_positives = 0\n",
    "for x in all_users:\n",
    "    if x in on_campus_positives_list:\n",
    "        counter_on_campus_positives +=1\n",
    "    if x in off_campus_positives_list:\n",
    "        counter_off_campus_positives +=1\n",
    "        \n",
    "print('on campus',len(on_campus_positives_list),counter_on_campus_positives)\n",
    "print('off campus',len(off_campus_positives_list),counter_off_campus_positives)\n",
    "\n",
    "sports_list = []\n",
    "for row,index in metadata.iterrows():\n",
    "    if \"Team\" in str(index['Sports_team']):\n",
    "        sports_list.append(index['barcode'])\n",
    "\n",
    "counter_sports = 0\n",
    "for x in all_users:\n",
    "    if x in sports_list:\n",
    "        counter_sports +=1\n",
    "print('sports',len(sports_list),counter_sports)\n",
    "\n",
    "faculty_list = []\n",
    "for row,index in metadata.iterrows():\n",
    "    if \"Yes\" in str(index['Faculty_or_Staff?']):\n",
    "        faculty_list.append(index['barcode'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plot(data, fill_color,i):\n",
    "    bp = ax.boxplot(data, patch_artist=True,positions = [i],widths = .75,medianprops=dict(color='black'),flierprops = dict(color = fill_color))\n",
    "    for box in bp['boxes']:\n",
    "        box.set(color='black', linewidth=2)\n",
    "    for fliers in bp['fliers']:\n",
    "        flierprops=dict(color= fill_color, markeredgecolor='fill_color')\n",
    "    #for element in ['boxes', 'whiskers', 'fliers', 'means', 'medians', 'caps']:\n",
    "    #    plt.setp(bp[element], color=edge_color)\n",
    "\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set(facecolor=fill_color,alpha = .6)       \n",
    "        \n",
    "    return bp\n",
    "\n",
    "def box_plot_no_fliers(data, fill_color,i):\n",
    "    bp = ax.boxplot(data, patch_artist=True,positions = [i],widths = .75,medianprops=dict(color='black'),showfliers=False)\n",
    "    for box in bp['boxes']:\n",
    "        box.set(color='black', linewidth=2)\n",
    "    for fliers in bp['fliers']:\n",
    "        flierprops=dict(color= fill_color, markeredgecolor='fill_color')\n",
    "    #for element in ['boxes', 'whiskers', 'fliers', 'means', 'medians', 'caps']:\n",
    "    #    plt.setp(bp[element], color=edge_color)\n",
    "\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set(facecolor=fill_color,alpha = .6)       \n",
    "        \n",
    "    return bp\n",
    "\n",
    "def label_diff(i,j,text,X,Y):\n",
    "    x = (X[i]+X[j])/2\n",
    "    y = 1.1*max(Y[i], Y[j])\n",
    "    dx = abs(X[i]-X[j])\n",
    "\n",
    "    props = {'connectionstyle':'bar','arrowstyle':'-',\\\n",
    "                 'shrinkA':20,'shrinkB':20,'linewidth':2}\n",
    "    ax.annotate(text, xy=(X[i],y+7), zorder=10)\n",
    "    ax.annotate('', xy=(X[i],y), xytext=(X[j],y), arrowprops=props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_date_pos_list_fall,positives = create_by_date_pos_list(final_fall,barcode_to_test_date,10)\n",
    "by_date_pos_list_spring,positives = create_by_date_pos_list(final_spring,barcode_to_test_date,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fall = final_fall.groupby(['day'])\n",
    "final_fall = [group for x, group in final_fall]\n",
    "final_spring = final_spring.groupby(['day'])\n",
    "final_spring = [group for x, group in final_spring]\n",
    "\n",
    "fall_interactions_cleaned = fall_interactions_cleaned.groupby(['day'])\n",
    "fall_interactions_cleaned = [group for x, group in fall_interactions_cleaned]\n",
    "spring_interactions_cleaned = spring_interactions_cleaned.groupby(['day'])\n",
    "spring_interactions_cleaned = [group for x, group in spring_interactions_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fall_interactions_by_ap_for_counts['user'] = [barcode_dict[x] if x in list(barcode_dict.keys()) else x for x in fall_interactions_by_ap_for_counts['user']]\n",
    "spring_interactions_by_ap_for_counts['user'] = [barcode_dict[x] if x in list(barcode_dict.keys()) else x for x in spring_interactions_by_ap_for_counts['user']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fall_interactions_by_ap_for_counts = fall_interactions_by_ap.groupby(['day'])\n",
    "fall_interactions_by_ap_for_counts = [group for x, group in fall_interactions_by_ap_for_counts]\n",
    "spring_interactions_by_ap_for_counts = spring_interactions_by_ap_for_counts.groupby(['day'])\n",
    "spring_interactions_by_ap_for_counts = [group for x, group in spring_interactions_by_ap_for_counts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_network_to_make_groups_largerfile(interactions_by_pairing):\n",
    "    days = []\n",
    "    node_count = []\n",
    "    edge_count = []\n",
    "    node_list = {}\n",
    "    for x in interactions_by_pairing:\n",
    "        day = str(x['day'].iloc[0])\n",
    "        days.append(day)\n",
    "        g = nx.from_pandas_edgelist(x,'user_1','user_2', ['duration'], create_using = nx.Graph())\n",
    "        for node in g:\n",
    "            if node in node_list:\n",
    "                node_list[node][1].append(1)\n",
    "            else:\n",
    "                node_list[node] = [[day],[1]]\n",
    "        num_nodes = g.number_of_nodes()\n",
    "        num_edges = g.number_of_edges()\n",
    "\n",
    "        node_count.append(num_nodes)\n",
    "        edge_count.append(num_edges)\n",
    "    return node_list,node_count,edge_count,days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_by_date_pos_list_without10days(by_date_pos_list,sem_node_list,positives,barcode_to_test_date_dict):\n",
    "    sem_remove = []\n",
    "    for x in sem_node_list:\n",
    "        if x in positives:\n",
    "            first = datetime.fromisoformat(sem_node_list[x][0][0])\n",
    "            second = datetime.strptime(barcode_to_test_date_dict[x],\"%Y-%m-%d\")\n",
    "            diff = (first-second).days\n",
    "            if (diff < 1 and diff > -10):\n",
    "                sem_remove.append(x) \n",
    "\n",
    "    by_date_pos_list_dict_new = {}\n",
    "    for x in by_date_pos_list:\n",
    "        new_list = []\n",
    "        for code in by_date_pos_list[x]:\n",
    "            if code not in sem_remove:\n",
    "                new_list.append(code)\n",
    "        by_date_pos_list_dict_new[x] = new_list\n",
    "    \n",
    "    return by_date_pos_list_dict_new\n",
    "\n",
    "def create_numvisits_per_node_chart_sem1(node_list_sem1,node_list_sem2):\n",
    "    \n",
    "    values = []\n",
    "    for x in node_list_sem1.values():\n",
    "        values.append(x[1])\n",
    "    sum_visits_sem1 = []\n",
    "    for x in values:\n",
    "        sum_visits_sem1.append(sum(x))\n",
    "        \n",
    "    values = []\n",
    "    for x in node_list_sem2.values():\n",
    "        values.append(x[1])\n",
    "    sum_visits_sem2 = []\n",
    "    for x in values:\n",
    "        sum_visits_sem2.append(sum(x))\n",
    "    \n",
    "    plt.figure(figsize = (10,8))\n",
    "    \n",
    "    plt.hist(sum_visits_sem1,bins = range(100),alpha = .6,color = color_pallet[3],label = 'Fall 2020')\n",
    "    plt.hist(sum_visits_sem2,bins = range(100),alpha = .6, color = color_pallet[5], label = 'Spring 2021')\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.xlabel('Number of Days on Campus During Semester',fontsize = 16)\n",
    "    plt.ylabel('Number of Users',fontsize = 16)\n",
    "    plt.legend(loc = 'best', fontsize = 16,frameon = True)\n",
    "\n",
    "    image_format = 'svg' # e.g .png, .svg, etc.\n",
    "    image_name = 'Supp5B.svg'\n",
    "\n",
    "    plt.savefig(image_name, format=image_format, dpi=1200)\n",
    "    plt.show()\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_network_to_make_groups_nonpaired(interactions_by_pairing):\n",
    "    days = []\n",
    "    node_count = []\n",
    "    edge_count = []\n",
    "    node_list = {}\n",
    "    for x in interactions_by_pairing:\n",
    "        x.columns = ['day','user','duration','building','ap','int count']\n",
    "        day = str(x['day'].iloc[0])\n",
    "        days.append(day)\n",
    "        x_new = x.groupby(['user'])\n",
    "        x_new = [group for y, group in x_new]\n",
    "        for nodes in x_new:\n",
    "            node = nodes['user'].iloc[0]\n",
    "            if node in node_list:\n",
    "                node_list[node][1].append(1)\n",
    "            else:\n",
    "                node_list[node] = [[day],[1]]\n",
    "        node_count.append(len(x_new))\n",
    "    return node_list,node_count,days\n",
    "\n",
    "fall_node_list_non_paired,node_count_non_paired,days = run_network_to_make_groups_nonpaired(fall_interactions_by_ap_for_counts)\n",
    "spring_node_list_non_paired,node_count_non_paired,days = run_network_to_make_groups_nonpaired(spring_interactions_by_ap_for_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_numvisits_per_node_chart_sem1(fall_node_list_non_paired,spring_node_list_non_paired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fall_interactions_full = fall_interactions_full.groupby(['day'])\n",
    "fall_interactions_full = [group for x, group in fall_interactions_full]\n",
    "spring_interactions_full = spring_interactions_full.groupby(['day'])\n",
    "spring_interactions_full = [group for x, group in spring_interactions_full]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fall_uncleaned_node_list, fall_uncleaned_node_count, fall_uncleaned_edge_count, days_uncleaned_fall = run_network_to_make_groups_largerfile(fall_interactions_full)\n",
    "spring_uncleaned_node_list, spring_uncleaned_node_count, spring_uncleaned_edge_count, days_uncleaned_spring = run_network_to_make_groups_largerfile(spring_interactions_full)\n",
    "create_numvisits_per_node_chart_sem1(fall_uncleaned_node_list,spring_uncleaned_node_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_date_pos_list_fall_dict = create_by_date_pos_list_without10days(by_date_pos_list_fall,fall_uncleaned_node_list,positives,barcode_to_test_date_dict)  \n",
    "by_date_pos_list_spring_dict = create_by_date_pos_list_without10days(by_date_pos_list_spring,spring_uncleaned_node_list,positives,barcode_to_test_date_dict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fall_positive_visits = []\n",
    "fall_negative_visits = []\n",
    "for x in fall_node_list_non_paired:\n",
    "    if x in fall_metadata_of_positives_barcodes:\n",
    "        fall_positive_visits.append(sum(fall_node_list_non_paired[x][1]))\n",
    "    else:\n",
    "        fall_negative_visits.append(sum(fall_node_list_non_paired[x][1]))\n",
    "        \n",
    "spring_positive_visits = []\n",
    "spring_negative_visits = []\n",
    "for x in spring_node_list_non_paired:\n",
    "    if x in spring_metadata_of_positives_barcodes:\n",
    "        spring_positive_visits.append(sum(spring_node_list_non_paired[x][1])) \n",
    "    else:\n",
    "        spring_negative_visits.append(sum(spring_node_list_non_paired[x][1]))\n",
    "        \n",
    "plt.figure(figsize = (10,8))\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xlabel('Number of Days on Campus During Semester',fontsize = 16)\n",
    "plt.ylabel('Number of Semester Positive Users',fontsize = 16)\n",
    "plt.hist(fall_positive_visits,bins = range(100), color = color_pallet[3],alpha = .6, label = 'Fall 2020')\n",
    "plt.hist(spring_positive_visits,bins = range(100), color = color_pallet[5],alpha = .6, label = 'Spring 2021')\n",
    "plt.legend(loc = 'upper left', fontsize = 16,frameon = True)\n",
    "image_format = 'svg' # e.g .png, .svg, etc.\n",
    "image_name = 'Supp4C.svg'\n",
    "\n",
    "plt.savefig(image_name, format=image_format, dpi=1200)\n",
    "plt.show()\n",
    "\n",
    "u_fall,p_fall = mannwhitneyu(fall_positive_visits,fall_negative_visits)\n",
    "u_spring,p_spring = mannwhitneyu(spring_positive_visits,spring_negative_visits)\n",
    "print(np.median(fall_positive_visits),np.median(fall_negative_visits))\n",
    "print(np.median(spring_positive_visits),np.median(spring_negative_visits))\n",
    "print(u_fall,p_fall)\n",
    "print(u_spring,p_spring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (9,12))\n",
    "bp1 = box_plot_no_fliers(fall_positive_visits,color_pallet[3],1)\n",
    "bp2 = box_plot_no_fliers(fall_negative_visits,color_pallet[4],2)\n",
    "bp3 = box_plot_no_fliers(spring_positive_visits,color_pallet[3],3)\n",
    "bp4 = box_plot_no_fliers(spring_negative_visits,color_pallet[4],4)\n",
    "ax.legend([bp1[\"boxes\"][0], bp2[\"boxes\"][0]],  ['Positive','Negative'],fontsize = 20,frameon=True,loc = 'best')\n",
    "ax.set_xticks([1.45,3.5])\n",
    "ax.set_xticklabels(labels = ['Fall 2020','Spring 2021'],fontsize = 22)\n",
    "ax.set_yticks(np.arange(0,150,step =10))\n",
    "ax.set_yticklabels(labels = np.arange(0,150,step =10),fontsize = 22)\n",
    "plt.grid(axis = 'x',visible = False)\n",
    "plt.grid(axis = 'y',alpha = .3)\n",
    "ax.set_ylabel('Number of Days on Campus During Semester',fontsize = 20)\n",
    "barplot_annotate_brackets(0,1,p_fall,[1,2],[100,1],maxasterix=3,fs = 16)\n",
    "barplot_annotate_brackets(0,1,p_spring,[3,4],[100,3],maxasterix=3,fs = 16)\n",
    "ax.set_ylim(0,140)\n",
    "image_format = 'svg' # e.g .png, .svg, etc.\n",
    "image_name = 'new_boxplot_App5.svg'\n",
    "\n",
    "plt.savefig(image_name, format=image_format, dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_fall,p_fall = mannwhitneyu(fall_positive_visits,fall_negative_visits)\n",
    "u_spring,p_spring = mannwhitneyu(spring_positive_visits,spring_negative_visits)\n",
    "print(np.median(fall_positive_visits),np.median(fall_negative_visits))\n",
    "print(np.median(spring_positive_visits),np.median(spring_negative_visits))\n",
    "print(u_fall,p_fall)\n",
    "print(u_spring,p_spring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_proportion_interactions_per_buildingtype_perday(sem_interactions):\n",
    "    #create list of dates, get the interaction count and duration for each residence hall type\n",
    "    dates = []\n",
    "    reshall_bydate_count = []\n",
    "    academichall_bydate_count = []\n",
    "    otherhall_bydate_count = []\n",
    "\n",
    "    reshall_bydate_duration = []\n",
    "    academichall_bydate_duration = []\n",
    "    otherhall_bydate_duration = []\n",
    "\n",
    "    total_ints_count = []\n",
    "    hall = \"residence\"\n",
    "    academic = \"academic\"\n",
    "    other = \"other\"\n",
    "    for x in sem_interactions:\n",
    "        dataframe = x\n",
    "        dates.append(x['day'].iloc[0])\n",
    "        residential_building_count = len(x.loc[x.building.str.contains(hall)])\n",
    "        residential_building_duration = x.loc[x.building.str.contains(hall)]['duration'].median()\n",
    "\n",
    "        academic_building_count = len(x.loc[x.building.str.contains(academic)])\n",
    "        academic_building_duration = x.loc[x.building.str.contains(academic)]['duration'].median()\n",
    "\n",
    "        other_building_count = len(x.loc[x.building.str.contains(other)])\n",
    "        other_building_duration = x.loc[x.building.str.contains(other)]['duration'].median()\n",
    "\n",
    "        reshall_bydate_count.append(residential_building_count)\n",
    "        academichall_bydate_count.append(academic_building_count)\n",
    "        otherhall_bydate_count.append(other_building_count)  \n",
    "        total_ints_count.append(len(x))\n",
    "\n",
    "        reshall_bydate_duration.append(residential_building_duration)\n",
    "        academichall_bydate_duration.append(academic_building_duration)\n",
    "        otherhall_bydate_duration.append(other_building_duration) \n",
    "        by_date_building_int_counts = pd.DataFrame({'date': dates, 'total ints':total_ints_count,'res hall ints':reshall_bydate_count,'academic hall ints':academichall_bydate_count,'other hall ints':otherhall_bydate_count})\n",
    "        by_date_building_duration = pd.DataFrame({'date': dates, 'res hall duration':reshall_bydate_duration,'academic hall duration':academichall_bydate_duration,'other hall duration':otherhall_bydate_duration})\n",
    "    return by_date_building_duration,by_date_building_int_counts,dates\n",
    "\n",
    "def create_day_of_week(dates,by_date_building_int_counts,by_date_building_duration):\n",
    "    #create dictionary of days of the week\n",
    "    days = dates\n",
    "    day_2_DOW = {}\n",
    "    for day in days:\n",
    "        day_2_DOW[day] = datetime.strptime(day, '%Y-%M-%d').strftime('%A')\n",
    "    by_date_building_int_counts['DOW'] = by_date_building_int_counts.apply(lambda row: day_2_DOW[row.date], axis=1)\n",
    "    by_date_building_duration['DOW'] = by_date_building_duration.apply(lambda row: day_2_DOW[row.date], axis=1)\n",
    "    return by_date_building_int_counts,by_date_building_duration,day_2_DOW \n",
    "\n",
    "def create_sem_bydate_buildingcounts_withmedian(by_date_building_int_counts):\n",
    "    sem_by_date_building_int_counts = by_date_building_int_counts.groupby(['DOW']).median().reset_index(drop = False)\n",
    "    sem_by_date_building_int_counts = sem_by_date_building_int_counts.reindex([1,5,6,4,0,2,3])\n",
    "    return sem_by_date_building_int_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sem_chart_building_intcounts(sem_by_date_building_int_counts, SemesterName):\n",
    "    #make sem chart\n",
    "    labels = sem_by_date_building_int_counts['DOW']\n",
    "    res_hall = sem_by_date_building_int_counts['res hall ints']\n",
    "    academic_hall = sem_by_date_building_int_counts['academic hall ints']\n",
    "    other_hall = sem_by_date_building_int_counts['other hall ints']\n",
    "    total = sem_by_date_building_int_counts['total ints']\n",
    "\n",
    "    width = .7\n",
    "    \n",
    "    plt.figure(figsize = (11,11))\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.yticks(fontsize = 14)\n",
    "    plt.xlabel('Day of Week',fontsize = 16)\n",
    "    plt.ylabel('Interaction Count By Building Type',fontsize = 16)\n",
    "    plt.bar(labels,total,width,color = color_pallet[0],alpha = .2,label = 'All Buildings')\n",
    "    plt.plot(labels,res_hall, label = \"Residence\",color = color_pallet[6],alpha = .7,linewidth = 4,marker = 'o')\n",
    "    plt.plot(labels,academic_hall, label = \"Academic\",color = color_pallet[5],alpha = .7,linewidth = 4,marker = 'o')\n",
    "    plt.plot(labels,other_hall,label = \"Other\", color = color_pallet[3],alpha = .7,linewidth = 4,marker = 'o')\n",
    "    plt.grid(axis = 'x',visible = False)\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    order = [3,0,1,2]\n",
    "    plt.ylim(0,35000)\n",
    "    plt.legend([handles[idx] for idx in order],[labels[idx] for idx in order],loc = 'best',fontsize = 15,frameon = True) \n",
    "    #plt.ylim(0,max(res_hall+other_hall+academic_hall)*1.19)\n",
    "    image_format = 'svg' # e.g .png, .svg, etc.\n",
    "    image_name = 'Supp6BD' + SemesterName + '.svg'\n",
    "    plt.savefig(image_name, format=image_format, dpi=1200)\n",
    "    plt.show()\n",
    "    return \n",
    "\n",
    "def create_sem_bydate_duration_withmedian(by_date_building_duration):\n",
    "    sem_by_date_building_duration_med = by_date_building_duration.groupby(['DOW']).median().reset_index(drop = False)\n",
    "    sem_by_date_building_duration_med = sem_by_date_building_duration_med.reindex([1,5,6,4,0,2,3])\n",
    "    sem_by_date_building_duration = by_date_building_duration.groupby('DOW')\n",
    "    one = sem_by_date_building_duration.get_group('Monday')\n",
    "    two = sem_by_date_building_duration.get_group('Tuesday')\n",
    "    three = sem_by_date_building_duration.get_group('Wednesday')\n",
    "    four = sem_by_date_building_duration.get_group('Thursday')\n",
    "    five = sem_by_date_building_duration.get_group('Friday')\n",
    "    six = sem_by_date_building_duration.get_group('Saturday')\n",
    "    seven = sem_by_date_building_duration.get_group('Sunday')\n",
    "    one = np.median(list(one['res hall duration'])+list(one['academic hall duration'])+list(one['other hall duration']))\n",
    "    two = np.median(list(two['res hall duration'])+list(two['academic hall duration'])+list(two['other hall duration']))\n",
    "    three = np.median(list(three['res hall duration'])+list(three['academic hall duration'])+list(three['other hall duration']))\n",
    "    four = np.median(list(four['res hall duration'])+list(four['academic hall duration'])+list(four['other hall duration']))\n",
    "    five = np.median(list(five['res hall duration'])+list(five['academic hall duration'])+list(five['other hall duration']))\n",
    "    six = np.median(list(six['res hall duration'])+list(six['academic hall duration'])+list(six['other hall duration']))\n",
    "    seven = np.median(list(seven['res hall duration'])+list(seven['academic hall duration'])+list(seven['other hall duration']))\n",
    "    median_week = [one,two,three,four,five,six,seven]\n",
    "    return sem_by_date_building_duration_med,median_week\n",
    "\n",
    "def make_sem_chart_building_duration(sem_by_date_building_duration, SemesterName,non_med):\n",
    "    #make sem chart\n",
    "    labels = sem_by_date_building_duration['DOW']\n",
    "    res_hall = sem_by_date_building_duration['res hall duration']\n",
    "    academic_hall = sem_by_date_building_duration['academic hall duration']\n",
    "    other_hall = sem_by_date_building_duration['other hall duration'] \n",
    "  \n",
    "    width = .7\n",
    "    plt.figure(figsize = (11,11))\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.yticks(fontsize = 14)\n",
    "    plt.ylim(0,140)\n",
    "    plt.grid(axis = 'x',visible = False)\n",
    "    plt.plot(labels,non_med,label = 'All Buildings',color = color_pallet[0],alpha = .8,linestyle = '--',linewidth = 7,marker = 'o')\n",
    "    plt.plot(labels,academic_hall, label = \"Academic\",color = color_pallet[6],alpha = .7,linewidth = 4,marker = 'o')\n",
    "    plt.plot(labels,res_hall, label = \"Residence\",color = color_pallet[5],alpha = .7,linewidth = 4,marker = 'o')\n",
    "    plt.plot(labels,other_hall, label = \"Other\",color = color_pallet[3],alpha = .7,linewidth = 4,marker = 'o')\n",
    "    plt.ylabel('Median Duration (minutes)',fontsize = 16)\n",
    "    plt.xlabel('Day of Week',fontsize = 16)\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    order = [0,1,2,3]\n",
    "    plt.legend([handles[idx] for idx in order],[labels[idx] for idx in order],loc = 'best',fontsize = 15,frameon=True) \n",
    "    #plt.ylim((15,40))\n",
    "    image_format = 'svg' \n",
    "    image_name = 'Supp6AC' + SemesterName + '.svg'\n",
    "    plt.savefig(image_name, format=image_format, dpi=1200)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "by_date_building_duration_fall,by_date_building_int_counts_fall,dates_fall = find_proportion_interactions_per_buildingtype_perday(fall_interactions_by_ap_for_counts)\n",
    "by_date_building_duration_spring,by_date_building_int_counts_spring,dates_spring = find_proportion_interactions_per_buildingtype_perday(spring_interactions_by_ap_for_counts)\n",
    "by_date_building_int_counts_fall,by_date_building_duration_fall,day_2_DOW_fall  = create_day_of_week(dates_fall,by_date_building_int_counts_fall,by_date_building_duration_fall)\n",
    "by_date_building_int_counts_spring,by_date_building_duration_spring,day_2_DOW_spring  = create_day_of_week(dates_spring,by_date_building_int_counts_spring,by_date_building_duration_spring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fall_by_date_building_int_counts = create_sem_bydate_buildingcounts_withmedian(by_date_building_int_counts_fall)\n",
    "spring_by_date_building_int_counts = create_sem_bydate_buildingcounts_withmedian(by_date_building_int_counts_spring)  \n",
    "\n",
    "fall_by_date_building_duration_med,fall_by_date_building_duration = create_sem_bydate_duration_withmedian(by_date_building_duration_fall)\n",
    "spring_by_date_building_duration_med,spring_by_date_building_duration = create_sem_bydate_duration_withmedian(by_date_building_duration_spring)\n",
    "make_sem_chart_building_intcounts(fall_by_date_building_int_counts, 'Fall 2020')\n",
    "make_sem_chart_building_intcounts(spring_by_date_building_int_counts, 'Spring 2021')\n",
    "\n",
    "\n",
    "make_sem_chart_building_duration(fall_by_date_building_duration_med, 'Fall 2020',fall_by_date_building_duration)\n",
    "make_sem_chart_building_duration(spring_by_date_building_duration_med, 'Spring 2021',spring_by_date_building_duration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = []\n",
    "node_degree = {}\n",
    "node_meddur = {}\n",
    "node_sumdur = {}\n",
    "\n",
    "\n",
    "print('running')\n",
    "type_to_color = {0:'b',1:'r'}\n",
    "for x in final_fall:\n",
    "    day = x['day'].iloc[0]\n",
    "    days.append(day)\n",
    "    g = nx.from_pandas_edgelist(x,'s1','s2',['sum','median','count'], create_using = nx.Graph())\n",
    "    for node in g:\n",
    "        degree = g.degree(node)\n",
    "        if node in node_degree:\n",
    "            node_degree[node].append(degree)\n",
    "        else:\n",
    "            node_degree[node] = [degree]\n",
    "\n",
    "        sum_duration_list = []\n",
    "        for info in g.edges(node,data = True):\n",
    "            sum_duration_list.append(info[2]['sum'])\n",
    "\n",
    "        if node in node_sumdur:\n",
    "            node_sumdur[node].append(sum(sum_duration_list))\n",
    "        else:\n",
    "            node_sumdur[node] = [sum(sum_duration_list)]\n",
    "\n",
    "node_expos_time = {}                   \n",
    "for item,value in node_degree.items() :\n",
    "    node_expos_time[item] = [value]\n",
    "\n",
    "for item,value in node_sumdur.items() :\n",
    "    node_expos_time[item].append(value)\n",
    "\n",
    "for item, value in node_expos_time.items():\n",
    "    res = [i / j for i, j in zip(value[1], value[0])]\n",
    "    node_expos_time[item] = np.median(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "days2 = []\n",
    "node_degree2 = {}\n",
    "node_sumdur2 = {}\n",
    "\n",
    "print('running')\n",
    "type_to_color = {0:'b',1:'r'}\n",
    "for x in final_spring:\n",
    "    day = x['day'].iloc[0]\n",
    "    days2.append(day)\n",
    "    g = nx.from_pandas_edgelist(x,'s1','s2',['sum','median','count'], create_using = nx.Graph())\n",
    "    for node in g:\n",
    "        degree = g.degree(node)\n",
    "        if node in node_degree2:\n",
    "            node_degree2[node].append(degree)\n",
    "        else:\n",
    "            node_degree2[node] = [degree]\n",
    "\n",
    "        sum_duration_list = []\n",
    "        for info in g.edges(node,data = True):\n",
    "            sum_duration_list.append(info[2]['sum'])\n",
    "\n",
    "        if node in node_sumdur2:\n",
    "            node_sumdur2[node].append(sum(sum_duration_list))\n",
    "        else:\n",
    "            node_sumdur2[node] = [sum(sum_duration_list)]\n",
    "\n",
    "node_expos_time2 = {}                   \n",
    "for item,value in node_degree2.items() :\n",
    "    node_expos_time2[item] = [value]\n",
    "\n",
    "for item,value in node_sumdur2.items() :\n",
    "    node_expos_time2[item].append(value)\n",
    "\n",
    "for item, value in node_expos_time2.items():\n",
    "    res = [i / j for i, j in zip(value[1], value[0])]\n",
    "    node_expos_time2[item] = np.median(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plot(data, fill_color,i):\n",
    "    bp = ax.boxplot(data, patch_artist=True,positions = [i],widths = .75,medianprops=dict(color='black'),flierprops = dict(color = fill_color))\n",
    "    for box in bp['boxes']:\n",
    "        box.set(color='black', linewidth=2)\n",
    "    for fliers in bp['fliers']:\n",
    "        flierprops=dict(color= fill_color, markeredgecolor='fill_color')\n",
    "    #for element in ['boxes', 'whiskers', 'fliers', 'means', 'medians', 'caps']:\n",
    "    #    plt.setp(bp[element], color=edge_color)\n",
    "\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set(facecolor=fill_color,alpha = .6)       \n",
    "        \n",
    "    return bp\n",
    "\n",
    "def box_plot_no_fliers(data, fill_color,i):\n",
    "    bp = ax.boxplot(data, patch_artist=True,positions = [i],widths = .75,medianprops=dict(color='black'),showfliers=False)\n",
    "    for box in bp['boxes']:\n",
    "        box.set(color='black', linewidth=2)\n",
    "    for fliers in bp['fliers']:\n",
    "        flierprops=dict(color= fill_color, markeredgecolor='fill_color')\n",
    "    #for element in ['boxes', 'whiskers', 'fliers', 'means', 'medians', 'caps']:\n",
    "    #    plt.setp(bp[element], color=edge_color)\n",
    "\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set(facecolor=fill_color,alpha = .6)       \n",
    "        \n",
    "    return bp\n",
    "\n",
    "def label_diff(i,j,text,X,Y):\n",
    "    x = (X[i]+X[j])/2\n",
    "    y = 1.1*max(Y[i], Y[j])\n",
    "    dx = abs(X[i]-X[j])\n",
    "\n",
    "    props = {'connectionstyle':'bar','arrowstyle':'-',\\\n",
    "                 'shrinkA':20,'shrinkB':20,'linewidth':2}\n",
    "    ax.annotate(text, xy=(X[i],y+7), zorder=10)\n",
    "    ax.annotate('', xy=(X[i],y), xytext=(X[j],y), arrowprops=props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expos_sem = []\n",
    "expos_sem_neg = []\n",
    "\n",
    "for item,value in node_expos_time.items():\n",
    "    if item in list(fall_metadata_of_positives_barcodes):\n",
    "        expos_sem.append(value)\n",
    "    else: expos_sem_neg.append(value)\n",
    "   \n",
    "        \n",
    "U1, p_fall = mannwhitneyu(expos_sem, expos_sem_neg)\n",
    "print('mann whit',p_fall)\n",
    "print(\"median pos sem\", statistics.median(expos_sem))\n",
    "print(\"median neg sem\",statistics.median(expos_sem_neg))\n",
    "print(\"stdev pos sem\",statistics.stdev(expos_sem))\n",
    "print(\"stdev neg sem\",statistics.stdev(expos_sem_neg))\n",
    "print()\n",
    "\n",
    "\n",
    "expos_sem2 = []\n",
    "expos_sem_neg2 = []\n",
    "\n",
    "for item,value in node_expos_time2.items():\n",
    "    if item in list(spring_metadata_of_positives_barcodes):\n",
    "        expos_sem2.append(value)\n",
    "    else: expos_sem_neg2.append(value)\n",
    "\n",
    "      \n",
    "U1, p_spring = mannwhitneyu(expos_sem2, expos_sem_neg2)\n",
    "print('mann whit2',p_spring)\n",
    "print(\"median pos sem\", statistics.median(expos_sem2))\n",
    "print(\"median neg sem\",statistics.median(expos_sem_neg2))\n",
    "print(\"stdev pos sem\",statistics.stdev(expos_sem2))\n",
    "print(\"stdev neg sem\",statistics.stdev(expos_sem_neg2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize = (9,12))\n",
    "bp1 = box_plot_no_fliers(expos_sem,color_pallet[3],1)\n",
    "bp2 = box_plot_no_fliers(expos_sem_neg,color_pallet[4],2)\n",
    "bp3 = box_plot_no_fliers(expos_sem2,color_pallet[3],3)\n",
    "bp4 = box_plot_no_fliers(expos_sem_neg2,color_pallet[4],4)\n",
    "ax.legend([bp1[\"boxes\"][0], bp2[\"boxes\"][0]],  ['Positive','Negative'],fontsize = 21,frameon=True)\n",
    "ax.set_xticks([1.45,3.5])\n",
    "ax.set_xticklabels(labels = ['Fall 2020','Spring 2021'],fontsize = 25)\n",
    "ax.set_yticks(np.arange(0,1000,step =50))\n",
    "ax.set_yticklabels(labels = np.arange(0,1000,step =50),fontsize = 25)\n",
    "plt.grid(axis = 'x',visible = False)\n",
    "plt.grid(axis = 'y',visible = False)\n",
    "\n",
    "ax.set_ylabel('Exposure Time Per Contact',fontsize = 25)\n",
    "barplot_annotate_brackets(0,1,p_fall,[1,2],[325,1],maxasterix=3,fs = 16)\n",
    "barplot_annotate_brackets(0,1,p_spring,[3,4],[360,3],maxasterix=3,fs = 16)\n",
    "ax.set_ylim(0,500)\n",
    "image_format = 'svg' \n",
    "image_name = 'Main2B.svg'\n",
    "plt.savefig(image_name, format=image_format, dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_deg_sem = []\n",
    "node_deg_sem_neg = []\n",
    "\n",
    "for item,value in node_degree.items():\n",
    "    if item in list(fall_metadata_of_positives_barcodes):\n",
    "        node_deg_sem.append(np.median(value))\n",
    "    else: node_deg_sem_neg.append(np.median(value))\n",
    "   \n",
    "        \n",
    "U1, p_fall = mannwhitneyu(node_deg_sem, node_deg_sem_neg)\n",
    "print('mann whit',p_fall)\n",
    "print(\"median pos sem\", statistics.median(node_deg_sem))\n",
    "print(\"median neg sem\",statistics.median(node_deg_sem_neg))\n",
    "print(\"stdev pos sem\",statistics.stdev(node_deg_sem))\n",
    "print(\"stdev neg sem\",statistics.stdev(node_deg_sem_neg))\n",
    "print()\n",
    "\n",
    "node_deg_sem2 = []\n",
    "node_deg_sem_neg2 = []\n",
    "\n",
    "for item,value in node_degree2.items():\n",
    "    if item in list(spring_metadata_of_positives_barcodes):\n",
    "        node_deg_sem2.append(np.median(value))\n",
    "    else: node_deg_sem_neg2.append(np.median(value))\n",
    "   \n",
    "        \n",
    "U1, p_spring = mannwhitneyu(node_deg_sem2, node_deg_sem_neg2)\n",
    "print('mann whit2',p_spring)\n",
    "print(\"median pos sem\", statistics.median(node_deg_sem2))\n",
    "print(\"median neg sem\",statistics.median(node_deg_sem_neg2))\n",
    "print(\"stdev pos sem\",statistics.stdev(node_deg_sem2))\n",
    "print(\"stdev neg sem\",statistics.stdev(node_deg_sem_neg2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize = (9,12))\n",
    "bp1 = box_plot_no_fliers(node_deg_sem,color_pallet[3],1)\n",
    "bp2 = box_plot_no_fliers(node_deg_sem_neg,color_pallet[4],2)\n",
    "bp3 = box_plot_no_fliers(node_deg_sem2,color_pallet[3],3)\n",
    "bp4 = box_plot_no_fliers(node_deg_sem_neg2,color_pallet[4],4)\n",
    "ax.legend([bp1[\"boxes\"][0], bp2[\"boxes\"][0]],  ['Positive','Negative'],fontsize = 21,frameon = True,loc = 'best')\n",
    "ax.set_xticks([1.45,3.5])\n",
    "ax.set_xticklabels(labels = ['Fall 2020','Spring 2021'],fontsize = 25)\n",
    "ax.set_yticks(np.arange(0,185,step =15))\n",
    "ax.set_yticklabels(labels = np.arange(0,185,step =15),fontsize = 25)\n",
    "plt.grid(axis = 'x',visible = False)\n",
    "plt.grid(axis = 'y',visible = False)\n",
    "ax.set_ylabel('Number of Contacts Per Day',fontsize = 25)\n",
    "barplot_annotate_brackets(0,1,p_fall,[1,2],[130,1],maxasterix=3,fs = 16)\n",
    "barplot_annotate_brackets(0,1,p_spring,[3,4],[140,3],maxasterix=3,fs = 16)\n",
    "ax.set_ylim(0,190)\n",
    "image_format = 'svg' \n",
    "image_name = 'Main2A.svg'\n",
    "plt.savefig(image_name, format=image_format, dpi=1200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_positives_pn_meddur = []\n",
    "rolling_positives_pp_sumdur = []\n",
    "rolling_positives_pn_sumdur = []\n",
    "rolling_positives_nn_meddur = []\n",
    "rolling_positives_pp_meddur = []\n",
    "rolling_positives_nn_sumdur = []\n",
    "edge_list = []\n",
    "print('running')\n",
    "days = []\n",
    "for j in final_fall:\n",
    "    day = j['day'].iloc[0]\n",
    "    days.append(day)\n",
    "    g = nx.from_pandas_edgelist(j,'user1','user2',['sum','median','count'], create_using = nx.Graph())\n",
    "    rolling_positives = by_date_pos_list_fall_dict[str(day)]\n",
    "    sem_positives = list(fall_metadata_of_positives_barcodes)\n",
    "    edges = list(g.edges(data = True))\n",
    "    for x in edges:\n",
    "        edge_list.append(x[0])\n",
    "        edge_list.append(x[1])\n",
    "        if str(x[0]) in rolling_positives:\n",
    "            if str(x[1]) in rolling_positives:\n",
    "                rolling_positives_pp_sumdur.append(x[2]['sum'])\n",
    "                rolling_positives_pp_meddur.append(x[2]['median'])\n",
    "            else : \n",
    "                rolling_positives_pn_sumdur.append(x[2]['sum'])\n",
    "                rolling_positives_pn_meddur.append(x[2]['median'])\n",
    "        else:\n",
    "            if x[1] in rolling_positives:\n",
    "                rolling_positives_pn_sumdur.append(x[2]['sum'])\n",
    "                rolling_positives_pn_meddur.append(x[2]['median'])\n",
    "            else:\n",
    "                rolling_positives_nn_sumdur.append(x[2]['sum'])\n",
    "                rolling_positives_nn_meddur.append(x[2]['median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_positives_pn_meddur2 = []\n",
    "rolling_positives_pp_sumdur2 = []\n",
    "rolling_positives_pn_sumdur2 = []\n",
    "rolling_positives_nn_meddur2 = []\n",
    "rolling_positives_pp_meddur2 = []\n",
    "rolling_positives_nn_sumdur2 = []\n",
    "edge_list2 = []\n",
    "print('running')\n",
    "days = []\n",
    "for j in final_spring:\n",
    "    day = j['day'].iloc[0]\n",
    "    g = nx.from_pandas_edgelist(j,'user1','user2',['sum','median','count'], create_using = nx.Graph())\n",
    "    rolling_positives = by_date_pos_list_spring_dict[str(day)]\n",
    "    sem_positives = list(spring_metadata_of_positives_barcodes)\n",
    "    edges = list(g.edges(data = True))\n",
    "    for x in edges:\n",
    "        edge_list2.append(x[0])\n",
    "        edge_list2.append(x[1])\n",
    "        if str(x[0]) in rolling_positives:\n",
    "            if str(x[1]) in rolling_positives:\n",
    "                rolling_positives_pp_sumdur2.append(x[2]['sum'])\n",
    "                rolling_positives_pp_meddur2.append(x[2]['median'])\n",
    "            else : \n",
    "                rolling_positives_pn_sumdur2.append(x[2]['sum'])\n",
    "                rolling_positives_pn_meddur2.append(x[2]['median'])\n",
    "        else:\n",
    "            if x[1] in rolling_positives:\n",
    "                rolling_positives_pn_sumdur2.append(x[2]['sum'])\n",
    "                rolling_positives_pn_meddur2.append(x[2]['median'])\n",
    "            else:\n",
    "                rolling_positives_nn_sumdur2.append(x[2]['sum'])\n",
    "                rolling_positives_nn_meddur2.append(x[2]['median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1, p_pp_pn_fall = mannwhitneyu(rolling_positives_pp_sumdur, rolling_positives_pn_sumdur)\n",
    "print('pp-pn',p_pp_pn_fall)\n",
    "\n",
    "U1, p_pp_nn_fall = mannwhitneyu(rolling_positives_pp_sumdur, rolling_positives_nn_sumdur)\n",
    "print('pp-nn',p_pp_nn_fall)\n",
    "\n",
    "U1, p_pn_nn_fall = mannwhitneyu(rolling_positives_pn_sumdur, rolling_positives_nn_sumdur)\n",
    "print('pn-nn',p_pn_nn_fall)\n",
    "\n",
    "U1, p_pp_pn_spring = mannwhitneyu(rolling_positives_pp_sumdur2, rolling_positives_pn_sumdur2)\n",
    "print('pp-pn',p_pp_pn_spring)\n",
    "\n",
    "U1, p_pp_nn_spring = mannwhitneyu(rolling_positives_pp_sumdur2, rolling_positives_nn_sumdur2)\n",
    "print('pp-nn',p_pp_nn_spring)\n",
    "\n",
    "U1, p_pn_nn_spring = mannwhitneyu(rolling_positives_pn_sumdur2, rolling_positives_nn_sumdur2)\n",
    "print('pn-nn',p_pn_nn_spring)\n",
    "\n",
    "bp1 = fig, ax = plt.subplots(figsize = (11,11))\n",
    "bp1 = box_plot(rolling_positives_pp_sumdur,color_pallet[2],1)\n",
    "bp2 = box_plot(rolling_positives_pn_sumdur,color_pallet[4],2)\n",
    "bp3 = box_plot(rolling_positives_nn_sumdur,color_pallet[5],3)\n",
    "bp4 = box_plot(rolling_positives_pp_sumdur2,color_pallet[2],5)\n",
    "bp5 = box_plot(rolling_positives_pn_sumdur2,color_pallet[4],6)\n",
    "bp6 = box_plot(rolling_positives_nn_sumdur2,color_pallet[5],7)\n",
    "plt.grid(axis = 'x',visible = False)\n",
    "plt.grid(axis = 'y',visible = True)\n",
    "\n",
    "ax.set_xticks([2.02,6.02])\n",
    "ax.set_yticks(np.arange(0,4500,step =500))\n",
    "ax.set_ylim(0,3500)\n",
    "ax.set_yticklabels(labels = np.arange(0,4500,step =500),fontsize = 16)\n",
    "ax.set_xticklabels(labels = ['Fall 2020','Spring 2021'],fontsize = 17)\n",
    "ax.set_ylabel('Total Interaction Duration Per Pair Per Day (minutes)',fontsize = 17)\n",
    "ax.legend([bp1[\"boxes\"][0], bp2[\"boxes\"][0],bp3[\"boxes\"][0]], ['Positive/Positive','Positive/Negative', 'Negative/Negative'],fontsize = 15,loc = 'best',frameon = True)\n",
    "image_format = 'svg' # e.g .png, .svg, etc.\n",
    "image_name = 'Supp7E.svg'\n",
    "plt.savefig(image_name, format=image_format, dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('fall 2020')\n",
    "print(\"median pp sem\", statistics.median(rolling_positives_pp_sumdur))\n",
    "print(\"median pn sem\", statistics.median(rolling_positives_pn_sumdur))\n",
    "print(\"median nn sem\",statistics.median(rolling_positives_nn_sumdur))\n",
    "print(\"stdev pp sem\",statistics.stdev(rolling_positives_pp_sumdur))\n",
    "print(\"stdev pn sem\",statistics.stdev(rolling_positives_pn_sumdur))\n",
    "print(\"stdev n sem\",statistics.stdev(rolling_positives_nn_sumdur))\n",
    "\n",
    "print('spring 2021')\n",
    "print(\"median pp sem\", statistics.median(rolling_positives_pp_sumdur2))\n",
    "print(\"median pn sem\", statistics.median(rolling_positives_pn_sumdur2))\n",
    "print(\"median nn sem\",statistics.median(rolling_positives_nn_sumdur2))\n",
    "print(\"stdev pp sem\",statistics.stdev(rolling_positives_pp_sumdur2))\n",
    "print(\"stdev pn sem\",statistics.stdev(rolling_positives_pn_sumdur2))\n",
    "print(\"stdev n sem\",statistics.stdev(rolling_positives_nn_sumdur2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp1 = fig, ax = plt.subplots(figsize = (11,11))\n",
    "bp1 = box_plot_no_fliers(rolling_positives_pp_sumdur,color_pallet[2],1)\n",
    "bp2 = box_plot_no_fliers(rolling_positives_pn_sumdur,color_pallet[4],2)\n",
    "bp3 = box_plot_no_fliers(rolling_positives_nn_sumdur,color_pallet[5],3)\n",
    "bp4 = box_plot_no_fliers(rolling_positives_pp_sumdur2,color_pallet[2],5)\n",
    "bp5 = box_plot_no_fliers(rolling_positives_pn_sumdur2,color_pallet[4],6)\n",
    "bp6 = box_plot_no_fliers(rolling_positives_nn_sumdur2,color_pallet[5],7)\n",
    "plt.grid(axis = 'x',visible = False)\n",
    "ax.set_xticks([2.02,6.02])\n",
    "ax.set_yticks(np.arange(0,2000,step =100))\n",
    "ax.set_yticklabels(labels = np.arange(0,2000,step =100),fontsize = 22)\n",
    "ax.set_xticklabels(labels = ['Fall 2020','Spring 2021'],fontsize = 22)\n",
    "ax.set_ylabel('Total Interaction Duration Per Pair Per Day (minutes)',fontsize = 15)\n",
    "ax.legend([bp1[\"boxes\"][0], bp2[\"boxes\"][0],bp3[\"boxes\"][0]], ['Positive/Positive','Positive/Negative', 'Negative/Negative'],fontsize = 16,loc = 'best',frameon=True)\n",
    "barplot_annotate_brackets(0,1,p_pp_pn_fall,[1,1.97],[550,5],maxasterix=3,fs = 16)\n",
    "barplot_annotate_brackets(0,1,p_pn_nn_fall,[2.03,3.05],[550,5],maxasterix=3,fs = 16)\n",
    "barplot_annotate_brackets(0,1,p_pp_nn_fall,[1.0,3.05],[750,5],maxasterix=3,fs = 16)\n",
    "barplot_annotate_brackets(0,1,p_pp_pn_spring,[5,5.98],[1400,5],maxasterix=3,fs = 16)\n",
    "barplot_annotate_brackets(0,1,p_pn_nn_spring,[6.02,7.05],[1300,5],maxasterix=3,fs = 16)\n",
    "barplot_annotate_brackets(0,1,p_pp_nn_spring,[5.0,7.05],[1550,5],maxasterix=3,fs = 16)\n",
    "image_format = 'svg' # e.g .png, .svg, etc.\n",
    "image_name = 'Supp7D2.svg'\n",
    "plt.savefig(image_name, format=image_format, dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1, p_pp_pn_fall = mannwhitneyu(rolling_positives_pp_meddur, rolling_positives_pn_meddur)\n",
    "print('pp-pn',p_pp_pn_fall)\n",
    "\n",
    "U1, p_pp_nn_fall = mannwhitneyu(rolling_positives_pp_meddur, rolling_positives_nn_meddur)\n",
    "print('pp-nn',p_pp_nn_fall)\n",
    "\n",
    "U1, p_pn_nn_fall = mannwhitneyu(rolling_positives_pn_meddur, rolling_positives_nn_meddur)\n",
    "print('pn-nn',p_pn_nn_fall)\n",
    "\n",
    "U1, p_pp_pn_spring = mannwhitneyu(rolling_positives_pp_meddur2, rolling_positives_pn_meddur2)\n",
    "print('pp-pn',p_pp_pn_spring)\n",
    "\n",
    "U1, p_pp_nn_spring = mannwhitneyu(rolling_positives_pp_meddur2, rolling_positives_nn_meddur2)\n",
    "print('pp-nn',p_pp_nn_spring)\n",
    "\n",
    "U1, p_pn_nn_spring = mannwhitneyu(rolling_positives_pn_meddur2, rolling_positives_nn_meddur2)\n",
    "print('pn-nn',p_pn_nn_spring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp1 = fig, ax = plt.subplots(figsize = (11,11))\n",
    "bp1 = box_plot(rolling_positives_pp_meddur,color_pallet[2],1)\n",
    "bp2 = box_plot(rolling_positives_pn_meddur,color_pallet[4],2)\n",
    "bp3 = box_plot(rolling_positives_nn_meddur,color_pallet[5],3)\n",
    "bp4 = box_plot(rolling_positives_pp_meddur2,color_pallet[2],5)\n",
    "bp5 = box_plot(rolling_positives_pn_meddur2,color_pallet[4],6)\n",
    "bp6 = box_plot(rolling_positives_nn_meddur2,color_pallet[5],7)\n",
    "plt.grid(axis = 'x',visible = False)\n",
    "ax.set_xticks([2.02,6.02])\n",
    "ax.set_xticklabels(labels = ['Fall 2020','Spring 2021'],fontsize = 17)\n",
    "ax.set_ylabel('Median Interaction Duration Per Pair',fontsize = 17)\n",
    "ax.set_yticks(np.arange(0,1770,step =100))\n",
    "ax.set_yticklabels(labels = np.arange(0,1770,step =100),fontsize = 18)\n",
    "ax.legend([bp1[\"boxes\"][0], bp2[\"boxes\"][0],bp3[\"boxes\"][0]], ['Positive/Positive','Positive/Negative', 'Negative/Negative'],fontsize = 15,loc = 'best',frameon= True)\n",
    "image_format = 'svg' # e.g .png, .svg, etc.\n",
    "image_name = 'Supp7E1.svg'\n",
    "plt.savefig(image_name, format=image_format, dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bp1 = fig, ax = plt.subplots(figsize = (11,11))\n",
    "bp1 = box_plot_no_fliers(rolling_positives_pp_meddur,color_pallet[2],1)\n",
    "bp2 = box_plot_no_fliers(rolling_positives_pn_meddur,color_pallet[4],2)\n",
    "bp3 = box_plot_no_fliers(rolling_positives_nn_meddur,color_pallet[5],3)\n",
    "bp4 = box_plot_no_fliers(rolling_positives_pp_meddur2,color_pallet[2],5)\n",
    "bp5 = box_plot_no_fliers(rolling_positives_pn_meddur2,color_pallet[4],6)\n",
    "bp6 = box_plot_no_fliers(rolling_positives_nn_meddur2,color_pallet[5],7)\n",
    "plt.grid(axis = 'x',visible = False)\n",
    "plt.grid(axis = 'y',alpha = .3)\n",
    "ax.set_xticks([2.02,6.02])\n",
    "ax.set_xticklabels(labels = ['Fall 2020','Spring 2021'],fontsize = 17)\n",
    "ax.set_ylabel('Median Interaction Duration Per Pair',fontsize = 17)\n",
    "ax.set_yticks(np.arange(0,800,step =20))\n",
    "ax.set_yticklabels(labels = np.arange(0,800,step =20),fontsize = 18)\n",
    "ax.legend([bp1[\"boxes\"][0], bp2[\"boxes\"][0],bp3[\"boxes\"][0]], ['Positive/Positive','Positive/Negative', 'Negative/Negative'],fontsize = 16,loc = 'best',frameon=True)\n",
    "barplot_annotate_brackets(0,1,p_pp_pn_fall,[1,1.97],[300,5],maxasterix=3,fs = 16)\n",
    "barplot_annotate_brackets(0,1,p_pn_nn_fall,[2.03,3.05],[310,5],maxasterix=3,fs = 16)\n",
    "barplot_annotate_brackets(0,1,p_pp_nn_fall,[1.0,3.05],[370,5],maxasterix=3,fs = 16)\n",
    "barplot_annotate_brackets(0,1,p_pp_pn_spring,[5,5.98],[340,5],maxasterix=3,fs = 16)\n",
    "barplot_annotate_brackets(0,1,p_pn_nn_spring,[6.02,7.05],[330,5],maxasterix=3,fs = 16)\n",
    "barplot_annotate_brackets(0,1,p_pp_nn_spring,[5.0,7.05],[375,5],maxasterix=3,fs = 16)\n",
    "image_format = 'svg' # e.g .png, .svg, etc.\n",
    "ax.set_ylim(0,540)\n",
    "image_name = 'Supp7E2.svg'\n",
    "plt.savefig(image_name, format=image_format, dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print('fall 2020')\n",
    "print(\"median pp sem\", statistics.median(rolling_positives_pp_meddur))\n",
    "print(\"median pn sem\", statistics.median(rolling_positives_pn_meddur))\n",
    "print(\"median nn sem\",statistics.median(rolling_positives_nn_meddur))\n",
    "print(\"stdev pp sem\",statistics.stdev(rolling_positives_pp_meddur))\n",
    "print(\"stdev pn sem\",statistics.stdev(rolling_positives_pn_meddur))\n",
    "print(\"stdev n sem\",statistics.stdev(rolling_positives_nn_meddur))\n",
    "\n",
    "print('spring 2021')\n",
    "print(\"median pp sem\", statistics.median(rolling_positives_pp_meddur2))\n",
    "print(\"median pn sem\", statistics.median(rolling_positives_pn_meddur2))\n",
    "print(\"median nn sem\",statistics.median(rolling_positives_nn_meddur2))\n",
    "print(\"stdev pp sem\",statistics.stdev(rolling_positives_pp_meddur2))\n",
    "print(\"stdev pn sem\",statistics.stdev(rolling_positives_pn_meddur2))\n",
    "print(\"stdev n sem\",statistics.stdev(rolling_positives_nn_meddur2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1, p = mannwhitneyu(rolling_positives_pp_meddur, rolling_positives_pn_meddur)\n",
    "print('pp-pn',p)\n",
    "\n",
    "U1, p = mannwhitneyu(rolling_positives_pp_meddur, rolling_positives_nn_meddur)\n",
    "print('pp-nn',p)\n",
    "\n",
    "U1, p = mannwhitneyu(rolling_positives_pn_meddur, rolling_positives_nn_meddur)\n",
    "print('nn-pn',p)\n",
    "\n",
    "\n",
    "U1, p = mannwhitneyu(rolling_positives_pp_meddur2, rolling_positives_pn_meddur2)\n",
    "print('pp-pn',p)\n",
    "\n",
    "U1, p = mannwhitneyu(rolling_positives_pp_meddur2, rolling_positives_nn_meddur2)\n",
    "print('pp-nn',p)\n",
    "\n",
    "U1, p = mannwhitneyu(rolling_positives_pn_meddur2, rolling_positives_nn_meddur2)\n",
    "print('nn-pn',p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holder_counter = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_att_network_sem_test(by_date_all_interactions_bypairing,positives_list,rolling,holder_counter,title):\n",
    "    days = []\n",
    "    pos_att = []\n",
    "    pos_att_random = []\n",
    "    counter_pos = 0\n",
    "    counter_pos_list = []\n",
    "    counter_neg_list = []\n",
    "    counter_neg = 0\n",
    "    random_list = []\n",
    "    lower_ci_95 = []\n",
    "    higher_ci_95 = []\n",
    "    holder_counter = []\n",
    "    \n",
    "    for x in by_date_all_interactions_bypairing:\n",
    "        day = str(x['day'].iloc[0])\n",
    "        #if rolling == True and day == '2021-03-19':\n",
    "        #    break\n",
    "\n",
    "        if rolling == True:\n",
    "            positives = positives_list[str(day)]\n",
    "        else : positives = list(positives_list)\n",
    "        random_list = []\n",
    "        days.append(day)\n",
    "        g = nx.from_pandas_edgelist(x,'user1','user2',['sum','median', 'count'], create_using = nx.Graph())\n",
    "        counter_pos = 0\n",
    "        counter_neg = 0\n",
    "        for nodes in g:\n",
    "            if nodes in positives: \n",
    "                g.nodes[nodes]['pos'] = 1 \n",
    "                counter_pos+=1\n",
    "            else: \n",
    "                g.nodes[nodes]['pos'] = 0\n",
    "                counter_neg+=1\n",
    "\n",
    "        pos_attribute_assort_coef = assortativity.numeric_assortativity_coefficient(g, 'pos')\n",
    "        pos_att.append(pos_attribute_assort_coef)\n",
    "        counter_pos_list.append(counter_pos)\n",
    "        counter_neg_list.append(counter_neg)\n",
    "        \n",
    "        #create permuted version\n",
    "        for x in range(40):\n",
    "            for nodes in g:\n",
    "                g.nodes[nodes]['eventual_pos'] = 0\n",
    "                choose_prob = random.uniform(0,1)\n",
    "                if (choose_prob <= (counter_pos/counter_neg)):\n",
    "                    g.nodes[nodes]['eventual_pos'] = 1 \n",
    "                else:\n",
    "                    g.nodes[nodes]['eventual_pos'] = 0\n",
    "            pos_attribute_assort_coef = assortativity.numeric_assortativity_coefficient(g, 'eventual_pos')\n",
    "            random_list.append(pos_attribute_assort_coef)\n",
    "        random_list = [item for item in random_list if not(math.isnan(item)) == True]\n",
    "        lower_ci_95.append(list(scipy.stats.t.interval(alpha=0.95, df=len(random_list)-1, loc=np.mean(random_list), scale=scipy.stats.sem(random_list)))[0])\n",
    "        higher_ci_95.append(list(scipy.stats.t.interval(alpha=0.95, df=len(random_list)-1, loc=np.mean(random_list), scale=scipy.stats.sem(random_list)))[1])\n",
    "\n",
    "    pos_att_results = pos_att\n",
    "    holder_counter.append(counter_pos_list)\n",
    "    \n",
    "    #create_dist_chart(counter_pos_list,counter_neg_list,days,title)\n",
    "    \n",
    "    return pos_att_results,holder_counter,lower_ci_95,higher_ci_95,days, counter_neg_list,counter_pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_diff_list(by_date_all_interactions_bypairing,positives_list):\n",
    "    keep_list = []\n",
    "    for x in by_date_all_interactions_bypairing:\n",
    "        day = str(x['day'].iloc[0])\n",
    "        positives = positives_list\n",
    "        \n",
    "        g = nx.from_pandas_edgelist(x,'user1','user2',['sum','median', 'count'], create_using = nx.Graph())\n",
    "        for edges in g.edges:\n",
    "            if edges[0] in positives and edges[1] in positives:\n",
    "                if ((abs((datetime.strptime(day,'%Y-%m-%d')-datetime.strptime(barcode_to_test_date_dict[edges[1]],'%Y-%m-%d')).days))<=12):\n",
    "                    if((abs((datetime.strptime(day,'%Y-%m-%d')-datetime.strptime(barcode_to_test_date_dict[edges[0]],'%Y-%m-%d')).days))<=12):\n",
    "                        to_keep = [edges[0],edges[1],day, barcode_to_test_date_dict[edges[0]],barcode_to_test_date_dict[edges[1]],(datetime.strptime(barcode_to_test_date_dict[edges[1]],'%Y-%m-%d')-datetime.strptime(barcode_to_test_date_dict[edges[0]],'%Y-%m-%d')).days,abs((datetime.strptime(day,'%Y-%m-%d')-datetime.strptime(barcode_to_test_date_dict[edges[0]],'%Y-%m-%d')).days),abs((datetime.strptime(day,'%Y-%m-%d')-datetime.strptime(barcode_to_test_date_dict[edges[1]],'%Y-%m-%d')).days)]\n",
    "                        keep_list.append(to_keep)\n",
    "    return keep_list\n",
    "fall_keep_list = make_diff_list(final_fall,fall_metadata_of_positives_barcodes)\n",
    "spring_keep_list = make_diff_list(final_spring,spring_metadata_of_positives_barcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fall_keeps = pd.DataFrame(data = fall_keep_list, columns = ['user1','user2','interaction date','user1 test date','user2 test date','test days diff','int days before user1 test','int days before user2 test'])\n",
    "spring_keeps = pd.DataFrame(data = spring_keep_list, columns = ['user1','user2','interaction date','user1 test date','user2 test date','test days diff','int days before user1 test','int days before user2 test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fall_keeps['test days diff'] = [abs(x) for x in fall_keeps['test days diff']]\n",
    "spring_keeps['test days diff'] = [abs(x) for x in spring_keeps['test days diff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fall_keeps.to_csv('fall_twelveday.csv',index = False)\n",
    "spring_keeps.to_csv('spring_twelveday.csv',index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dist_charts_data(interactions_by_pairing,by_date_pos_list):\n",
    "    days = []\n",
    "    node_count = []\n",
    "    edge_count = []\n",
    "    counter_pos_list = []\n",
    "    counter_neg_list = []\n",
    "    for x in interactions_by_pairing:\n",
    "        x.columns = ['day','user','duration','building','ap','int count']\n",
    "        counter_pos = 0\n",
    "        counter_neg = 0\n",
    "        day = str(x['day'].iloc[0])\n",
    "        days.append(day)\n",
    "        x_new = x.groupby(['user'])\n",
    "        x_new = [group for y, group in x_new]\n",
    "        for nodes in x_new:\n",
    "            node = nodes['user'].iloc[0]\n",
    "            if node in positives:\n",
    "                counter_pos +=1\n",
    "            else:\n",
    "                counter_neg +=1\n",
    "        counter_pos_list.append(counter_pos)\n",
    "        counter_neg_list.append(counter_neg)\n",
    "    return counter_neg_list,counter_pos_list,days\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_neg_list_f, counter_pos_list_f,fall_days = make_dist_charts_data(fall_interactions_by_ap_for_counts,fall_metadata_of_positives_barcodes)\n",
    "counter_neg_list_s, counter_pos_list_s,spring_days = make_dist_charts_data(spring_interactions_by_ap_for_counts,spring_metadata_of_positives_barcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dist_chart_sem1(counter_pos_list,counter_neg_list,days):\n",
    "    formatted_dates = []\n",
    "    not_formatted_dates = []\n",
    "    for x in days:\n",
    "        new = datetime.strptime(x,'%Y-%m-%d')\n",
    "        not_formatted_dates.append(new)\n",
    "        new_time = datetime.strftime(new, '%b-%d')\n",
    "        formatted_dates.append(new_time)\n",
    "\n",
    "    plt.figure(figsize = (16,8))\n",
    "    plt.plot(formatted_dates,counter_neg_list, label = \"Negative\",color = color_pallet[4],linewidth = 3)\n",
    "    plt.plot(formatted_dates,counter_pos_list,color = color_pallet[3],label = \"Positive\", linewidth = 3)\n",
    "    plt.legend(loc=\"best\",fontsize = 20,frameon=True)\n",
    "    plt.ylabel('Number of Individuals On Campus',fontsize = 20)\n",
    "    plt.xlabel('Date (2020)',fontsize = 20)\n",
    "    plt.yticks(fontsize = 20)\n",
    "    plt.grid(axis = 'x',visible = False)\n",
    "    plt.grid(axis = 'y',alpha = False)\n",
    "    plt.xticks(np.arange(0, len(formatted_dates), 7),fontsize = 20,rotation = 70)\n",
    "    \n",
    "    image_format = 'svg' # e.g .png, .svg, etc.\n",
    "    image_name = 'SuppD1.svg'\n",
    "\n",
    "    plt.savefig(image_name, format=image_format, dpi=1200)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    prop_negs = []\n",
    "    for x in counter_neg_list:\n",
    "        new = x/max(counter_neg_list)\n",
    "        prop_negs.append(new)\n",
    "    prop_pos = []\n",
    "    for x in counter_pos_list:\n",
    "        new = x/max(counter_pos_list)\n",
    "        prop_pos.append(new)\n",
    "    \n",
    "    plt.figure(figsize = (16,8))\n",
    "    plt.plot(formatted_dates,prop_negs, label = \"Negative\",color = color_pallet[4],linewidth = 3)\n",
    "    plt.plot(formatted_dates,prop_pos,color = color_pallet[3],label = \"Positive\", linewidth = 3)\n",
    "    plt.legend(loc=\"best\",fontsize = 20,frameon=True)\n",
    "    plt.ylabel('Proportion of Individuals on Campus',fontsize = 20)\n",
    "    plt.xlabel('Date (2020)',fontsize = 20)\n",
    "    plt.yticks(fontsize = 16)\n",
    "    plt.grid(axis = 'x',visible = False)\n",
    "    plt.grid(axis = 'y',alpha = False)\n",
    "    plt.xticks(np.arange(0, len(formatted_dates), 7),fontsize = 20,rotation = 70)\n",
    "    \n",
    "    image_format = 'svg' # e.g .png, .svg, etc.\n",
    "    image_name = 'SuppD2.svg'\n",
    "\n",
    "    plt.savefig(image_name, format=image_format, dpi=1200)\n",
    "    plt.show()\n",
    "\n",
    "    fall_u,fall_p = mannwhitneyu(prop_negs,prop_pos)\n",
    "    print(fall_u,fall_p)\n",
    "    p_fall = scipy.stats.pearsonr(prop_negs,prop_pos)\n",
    "    print(p_fall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dist_chart_sem2(counter_pos_list,counter_neg_list,days):\n",
    "    formatted_dates = []\n",
    "    not_formatted_dates = []\n",
    "    for x in days:\n",
    "        new = datetime.strptime(x,'%Y-%m-%d')\n",
    "        not_formatted_dates.append(new)\n",
    "        new_time = datetime.strftime(new, '%b-%d')\n",
    "        formatted_dates.append(new_time)\n",
    "\n",
    "    plt.figure(figsize = (16,8))\n",
    "    plt.plot(formatted_dates,counter_neg_list, label = \"Negative\",color = color_pallet[4],linewidth = 3)\n",
    "    plt.plot(formatted_dates,counter_pos_list,color = color_pallet[3],label = \"Positive\", linewidth = 3)\n",
    "    plt.legend(loc=\"best\",fontsize = 20,frameon=True)\n",
    "    plt.ylabel('Number of Individuals On Campus',fontsize = 20)\n",
    "    plt.xlabel('Date (2021)',fontsize = 20)\n",
    "    plt.yticks(fontsize = 20)\n",
    "    plt.ylim(0,5000)\n",
    "    plt.grid(axis = 'x',visible = False)\n",
    "    plt.grid(axis = 'y',alpha = False)\n",
    "    plt.xticks(np.arange(0, len(formatted_dates), 7),fontsize = 20,rotation = 70)\n",
    "    image_format = 'svg' # e.g .png, .svg, etc.\n",
    "    image_name = 'SuppE1.svg'\n",
    "\n",
    "    plt.savefig(image_name, format=image_format, dpi=1200)\n",
    "    plt.show()\n",
    "    \n",
    "    prop_negs = []\n",
    "    for x in counter_neg_list:\n",
    "        new = x/max(counter_neg_list)\n",
    "        prop_negs.append(new)\n",
    "    prop_pos = []\n",
    "    for x in counter_pos_list:\n",
    "        new = x/max(counter_pos_list)\n",
    "        prop_pos.append(new)\n",
    "    \n",
    "    plt.figure(figsize = (16,8))\n",
    "    plt.plot(formatted_dates,prop_negs, label = \"Negative\",color = color_pallet[4],linewidth = 3)\n",
    "    plt.plot(formatted_dates,prop_pos,color = color_pallet[3],label = \"Positive\", linewidth = 3)\n",
    "    plt.legend(loc=\"lower right\",fontsize = 20,frameon=True)\n",
    "    plt.ylabel('Proportion of Individuals on Campus',fontsize = 20)\n",
    "    plt.xlabel('Date (2021)',fontsize = 20)\n",
    "    plt.yticks(fontsize = 20)\n",
    "    plt.grid(axis = 'x',visible = False)\n",
    "    plt.grid(axis = 'y',alpha = False)\n",
    "    plt.xticks(np.arange(0, len(formatted_dates), 7),fontsize = 20,rotation = 70)\n",
    "    image_format = 'svg' # e.g .png, .svg, etc.\n",
    "    image_name = 'SuppE2.svg'\n",
    "\n",
    "    plt.savefig(image_name, format=image_format, dpi=1200)\n",
    "    plt.show()\n",
    "\n",
    "    spring_u,spring_p = mannwhitneyu(prop_negs,prop_pos)\n",
    "    print(spring_u,spring_p)\n",
    "\n",
    "    p_spring = scipy.stats.pearsonr(prop_negs,prop_pos)\n",
    "    print(p_spring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dist_chart_sem1(counter_pos_list_f,counter_neg_list_f,fall_days)\n",
    "create_dist_chart_sem2(counter_pos_list_s,counter_neg_list_s,spring_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('running')\n",
    "rolling_pos_att,holder_counter,lower_ci_95_rolling, higher_ci_95_rolling,days,counter_neg_list,counter_pos_list = pos_att_network_sem_test(final_fall,by_date_pos_list_fall_dict,True,holder_counter,'Rolling Count of Positive Attribute Individuals versus Negative Attribute Individuals: Fall 2020')\n",
    "print('halfway')\n",
    "sem1_pos_att,holder_counter,lower_ci_95_sem1, higher_ci_95_sem1,days,counter_neg_list,counter_pos_list = pos_att_network_sem_test(final_fall,fall_metadata_of_positives_barcodes,False,holder_counter,'Sem1 Non-Rolling Count of Positive Attribute Individuals On Campus Versus Negative Attribute Individuals On Campus: Fall 2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dist_chart_sem1(counter_pos_list,counter_neg_list,days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_AA_charts(days,pos,lower_ci_95,higher_ci_95,title,color,label1,label2,type_name):\n",
    "    formatted_dates = []\n",
    "    not_formatted_dates = []\n",
    "    for x in days:\n",
    "        new = datetime.strptime(x,'%Y-%m-%d')\n",
    "        not_formatted_dates.append(new)\n",
    "        new_time = datetime.strftime(new, '%b-%d')\n",
    "        formatted_dates.append(new_time)\n",
    "    \n",
    "    plt.figure(figsize = (19,10))\n",
    "    plt.plot(formatted_dates,pos, label = label1,color = color,linewidth = 3.5)\n",
    "    plt.fill_between(formatted_dates,higher_ci_95,lower_ci_95,alpha = .3,color = color_pallet[6],label = label2)\n",
    "    plt.legend(loc=\"best\",fontsize = 21,frameon = True)\n",
    "    plt.xticks(np.arange(0, len(formatted_dates), 7),fontsize = 20,rotation = 70)\n",
    "    plt.yticks(fontsize = 20)\n",
    "    plt.ylabel(str('Assortativity Coefficient'+ type_name),fontsize = 20)\n",
    "    plt.xlabel('Fall 2020',fontsize = 20)\n",
    "    plt.grid(axis = 'x',alpha = .5)\n",
    "    plt.grid(axis = 'y',alpha = .8)\n",
    "    image_format = 'svg' # e.g .png, .svg, etc.\n",
    "    image_name = 'Supp8A.svg'\n",
    "    plt.savefig(image_name, format=image_format, dpi=1200)\n",
    "    plt.show()\n",
    "\n",
    "    counter_in = 0\n",
    "    counter_full = 0\n",
    "    counter_out = 0\n",
    "    for x in range(len(pos)):\n",
    "        counter_full +=1\n",
    "        if ((pos[x] >= lower_ci_95[x]) and (pos[x] <= higher_ci_95[x])):\n",
    "                counter_in +=1\n",
    "        else:\n",
    "            counter_out +=1\n",
    "    \n",
    "    print(counter_in,counter_out,counter_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_AA_charts(days,sem1_pos_att,lower_ci_95_sem1,higher_ci_95_sem1,'First Sem Positives and Permuted First Sem Positives, Attribute Assortativity: 8/17/2020-11/23/2020',color_pallet[2],\"AA\",\"Permuted Network AA\",' (Full Semester)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_AA_charts(days,rolling_pos_att,lower_ci_95_rolling,higher_ci_95_rolling,'Rolling 10 Day Window Positives and Permuted Rolling 10 Day Window Positives, Attribute Assortativity: 8/17/2020-11/23/2020',color_pallet[5],\"AA\",\"Permuted Network AA\",' (10-Day Rolling)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_stuff_to_be_splitup(first_sem_positives,rolling_pos_att,days):\n",
    "    formatted_dates = []\n",
    "    not_formatted_dates = []\n",
    "    for x in days:\n",
    "        new = datetime.strptime(x,'%Y-%m-%d')\n",
    "        not_formatted_dates.append(new)\n",
    "        new_time = datetime.strftime(new, '%b-%d')\n",
    "        formatted_dates.append(new_time)\n",
    "    \n",
    "    case_count_per_day = []\n",
    "    for x in range(len(days)):\n",
    "        if (days[x] in list(first_sem_positives[['Test_day', 'barcode']].groupby('Test_day').count().reset_index()['Test_day'])):\n",
    "            case_count_per_day.append(first_sem_positives[['Test_day', 'barcode']].groupby('Test_day').count().reset_index()['barcode'][list(first_sem_positives[['Test_day', 'barcode']].groupby('Test_day').count().reset_index()['Test_day']).index(days[x])])\n",
    "        else:\n",
    "            case_count_per_day.append(0)\n",
    "    \n",
    "    rolling_pos_att = [0 if math.isnan(x) else x for x in rolling_pos_att]\n",
    "\n",
    "    rolling_smoothed = savgol_filter(rolling_pos_att, 17, 4,mode = 'nearest')\n",
    "    case_count_smoothed = savgol_filter(case_count_per_day, 17, 4,mode = 'nearest')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (20,10))\n",
    "    ax.set_xlabel('Date (2020)',fontsize = 20)\n",
    "    ax.set_ylabel('Attribute Assortativity (10-day rolling)',fontsize = 20)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "    plt.tick_params(axis='x', which='major', labelsize=20,rotation = 70)\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.grid(axis = 'y',alpha = .1)\n",
    "    ax.grid(axis = 'y',alpha = .1)\n",
    "    ax2.grid(axis = 'x',alpha = .3)\n",
    "    ax.grid(axis = 'x',alpha = .3)\n",
    "    ax2.plot(formatted_dates,case_count_per_day,color = color_pallet[6],label = 'Case Count',linewidth = 3,linestyle=\":\")\n",
    "    ax.plot(formatted_dates,rolling_pos_att,label = \"AA\",color = color_pallet[5],linewidth = 2,linestyle=\"--\")\n",
    "    ax.plot(formatted_dates,rolling_smoothed,color = color_pallet[5],linewidth = 4, label = 'Smoothed AA')\n",
    "    ax.set_ylim(0, max(rolling_pos_att)*1.0)\n",
    "    ax2.set_ylim(0, max(case_count_per_day)*1.0)\n",
    "    ax2.plot(formatted_dates,case_count_smoothed,color = color_pallet[6], linewidth = 4,label = 'Smoothed Case Count')\n",
    "    leg = ax.legend(loc=\"upper left\",fontsize = 20,frameon=True)\n",
    "    for line in leg.get_lines():\n",
    "        line.set_linewidth(5.0)\n",
    "        \n",
    "    leg = ax2.legend(loc=\"upper right\",fontsize = 20,frameon=True)\n",
    "    for line in leg.get_lines():\n",
    "        line.set_linewidth(5.0)\n",
    "\n",
    "    ax.set_yticks(np.linspace(ax.get_yticks()[0], ax.get_yticks()[-1], len(ax2.get_yticks())))\n",
    "\n",
    "    ax2.set_yticks(np.linspace(ax2.get_yticks()[0], ax2.get_yticks()[-1], len(ax.get_yticks())))\n",
    "    plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "    ax.sharex(ax2)\n",
    "    ax2.set_ylabel('Positive Cases Per Day',fontsize =20)\n",
    "    plt.xticks(np.arange(0, len(formatted_dates), 7),fontsize = 25,rotation = 90)\n",
    "    image_format = 'svg' # e.g .png, .svg, etc.\n",
    "    image_name = 'Main2E.svg'\n",
    "    plt.savefig(image_name, format=image_format, dpi=1200)\n",
    "    plt.show()\n",
    "    \n",
    "    print('unsmoothed pearson: ',scipy.stats.pearsonr(rolling_pos_att, case_count_per_day))\n",
    "    print('smoothed pearson: ',scipy.stats.pearsonr(rolling_smoothed,case_count_smoothed))\n",
    "    \n",
    "    return case_count_per_day,rolling_smoothed,case_count_smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def crosscorr(datax, datay, lag=0, wrap=False):\n",
    "    \n",
    "        return datax.corr(datay.shift(lag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_finder(rolling,casecount,smoothed_rolling,smoothed_casecount):\n",
    "    print('Non-Smoothed')\n",
    "    d1 = casecount\n",
    "    d2 = rolling\n",
    "    seconds = 1\n",
    "    fps = 10\n",
    "    rs = [crosscorr(pd.Series(d1),pd.Series(d2), lag) for lag in range(-int(4),int(11))]\n",
    "    offset = np.floor(len(rs)/3.63)-np.argmax(rs)\n",
    "    print(offset)\n",
    "    f,ax=plt.subplots(figsize=(14,3))\n",
    "    ax.plot(rs,color = color_pallet[5],linewidth = 2,linestyle = '--',label = 'Correlation')\n",
    "    ax.axvline(4,color='k',linestyle=':',label='Lag = 0')\n",
    "    print(rs)\n",
    "    #ax.axvline(np.argmax(rs),color='r',linestyle=':',label='Peak synchrony')\n",
    "    ax.set_xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14])\n",
    "    ax.set_xticklabels([-4.0,-3.0,-2.0,-1.0,0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0],fontsize = 16)\n",
    "    ax.set_xlabel('Lag, in days',fontsize = 15)\n",
    "    ax.set_ylabel('Pearson correlation',fontsize = 15)\n",
    "    ax.grid(axis = 'y',alpha = .1)\n",
    "    ax.grid(axis = 'x',alpha = .3)\n",
    "    leg = plt.legend(fontsize = 16,loc = 'best',frameon = True)\n",
    "    for line in leg.get_lines():\n",
    "        line.set_linewidth(3.0)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=16)\n",
    "    image_format = 'svg' # e.g .png, .svg, etc.\n",
    "    image_name = 'Supp8D1.svg'\n",
    "    plt.savefig(image_name, format=image_format, dpi=1200)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    print(\"Smoothed\")\n",
    "    d1 = smoothed_casecount\n",
    "    d2 = smoothed_rolling\n",
    "\n",
    "    seconds = 1\n",
    "    fps = 10\n",
    "    rs = [crosscorr(pd.Series(d1),pd.Series(d2), lag) for lag in range(-int(4),int(11))]\n",
    "    offset = np.floor(len(rs)/3.63)-np.argmax(rs)\n",
    "    f,ax=plt.subplots(figsize=(14,3))\n",
    "    plt.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "    print(offset)\n",
    "    ax.plot(rs,color = color_pallet[5],linewidth = 3,label = 'Smoothed Correlation')\n",
    "    ax.axvline(4,color='k',linestyle=':',label='Lag = 0')\n",
    "    print(rs)\n",
    "    #ax.axvline(np.argmax(rs),color='r',linestyle=':',label='Peak synchrony')\n",
    "    ax.set_xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14])\n",
    "    ax.set_xticklabels([-4.0,-3.0,-2.0,-1.0,0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0],fontsize = 16)\n",
    "    ax.set_xlabel('Lag, in days',fontsize = 15)\n",
    "    ax.set_ylabel('Pearson correlation',fontsize = 15)\n",
    "    ax.grid(axis = 'y',alpha = .1)\n",
    "    ax.grid(axis = 'x',alpha = .3)\n",
    "    leg = plt.legend(fontsize = 16,loc = 'best',frameon = True)\n",
    "    for line in leg.get_lines():\n",
    "        line.set_linewidth(3.0)\n",
    "    image_format = 'svg' # e.g .png, .svg, etc.\n",
    "    image_name = 'Supp8D2.svg'\n",
    "    plt.savefig(image_name, format=image_format, dpi=1200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_count_per_day,rolling_smoothed,case_count_smoothed = lag_stuff_to_be_splitup(fall_metadata_of_positives,rolling_pos_att,days)\n",
    "lag_finder(rolling_pos_att,case_count_per_day,rolling_smoothed,case_count_smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('running')\n",
    "rolling_pos_att2,holder_counter2,lower_ci_95_rolling2, higher_ci_95_rolling2,days1,counter_neg_list2,counter_pos_list2 = pos_att_network_sem_test(final_spring,by_date_pos_list_spring_dict,True,holder_counter,'Rolling Count of Positive Attribute Individuals versus Negative Attribute Individuals: Spring 2021')\n",
    "print('halfway')\n",
    "sem2_pos_att,holder_counter,lower_ci_95_sem2, higher_ci_95_sem2,days2,counter_neg_list2,counter_pos_list2 = pos_att_network_sem_test(final_spring,spring_metadata_of_positives_barcodes,False,holder_counter,'Sem2 Non-Rolling Count of Positive Attribute Individuals On Campus Versus Negative Attribute Individuals On Campus: Spring 2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dist_chart_sem2(counter_pos_list2,counter_neg_list2,days2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_AA_charts_sem2(days,pos,lower_ci_95,higher_ci_95,title,color,label1,label2,type_name):\n",
    "    formatted_dates = []\n",
    "    not_formatted_dates = []\n",
    "    for x in days:\n",
    "        new = datetime.strptime(x,'%Y-%m-%d')\n",
    "        not_formatted_dates.append(new)\n",
    "        new_time = datetime.strftime(new, '%b-%d')\n",
    "        formatted_dates.append(new_time)\n",
    "    \n",
    "    plt.figure(figsize = (20,10))\n",
    "    plt.plot(formatted_dates,pos, label = label1,color = color,linewidth = 3.5)\n",
    "    plt.fill_between(formatted_dates,higher_ci_95,lower_ci_95,alpha = .35,color = color_pallet[6],label = label2)\n",
    "    plt.legend(loc=\"best\",fontsize = 20,frameon = True)\n",
    "    plt.xticks(np.arange(0, len(formatted_dates), 7),fontsize = 18,rotation = 70)\n",
    "    plt.yticks(fontsize = 16)\n",
    "    plt.ylabel(str('Assortativity Coefficient'+ type_name),fontsize = 20)\n",
    "    plt.xlabel('Spring 2021',fontsize = 20)\n",
    "    plt.grid(axis = 'x',alpha = .5)\n",
    "    plt.grid(axis = 'y',alpha = .8)\n",
    "    image_format = 'svg' # e.g .png, .svg, etc.\n",
    "    image_name = 'Supp8B.svg'\n",
    "    plt.savefig(image_name, format=image_format, dpi=1200)\n",
    "\n",
    "    counter_in = 0\n",
    "    counter_full = 0\n",
    "    counter_out = 0\n",
    "    for x in range(len(pos)):\n",
    "        counter_full +=1\n",
    "        if ((pos[x] >= lower_ci_95[x]) and (pos[x] <= higher_ci_95[x])):\n",
    "                counter_in +=1\n",
    "        else:\n",
    "            counter_out +=1\n",
    "    \n",
    "    print(counter_in,counter_out,counter_full)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_finder2(rolling,casecount,smoothed_rolling,smoothed_casecount):\n",
    "    print('Non-Smoothed')\n",
    "    d1 = casecount\n",
    "    d2 = rolling\n",
    "    seconds = 1\n",
    "    fps = 10\n",
    "    rs = [crosscorr(pd.Series(d1),pd.Series(d2), lag) for lag in range(-int(4),int(11))]\n",
    "    offset = np.floor(len(rs)/3.63)-np.argmax(rs)\n",
    "    print(offset)\n",
    "    f,ax=plt.subplots(figsize=(14,3))\n",
    "    ax.plot(rs,color = color_pallet[5],linewidth = 2,linestyle = '--',label = 'Correlation')\n",
    "    ax.axvline(4,color='k',linestyle=':',label='Lag = 0')\n",
    "    print(rs)\n",
    "    #ax.axvline(np.argmax(rs),color='r',linestyle=':',label='Peak synchrony')\n",
    "    ax.set_xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14])\n",
    "    ax.set_xticklabels([-4.0,-3.0,-2.0,-1.0,0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0],fontsize = 16)\n",
    "    ax.set_xlabel('Lag, in days',fontsize = 15)\n",
    "    ax.set_ylabel('Pearson correlation',fontsize = 15)\n",
    "    ax.grid(axis = 'y',alpha = .1)\n",
    "    ax.grid(axis = 'x',alpha = .3)\n",
    "    leg = plt.legend(fontsize = 16,loc = 'best',frameon = True)\n",
    "    for line in leg.get_lines():\n",
    "        line.set_linewidth(3.0)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=16)\n",
    "    image_format = 'svg' # e.g .png, .svg, etc.\n",
    "    image_name = 'Supp8E1.svg'\n",
    "    plt.savefig(image_name, format=image_format, dpi=1200)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    print(\"Smoothed\")\n",
    "    d1 = smoothed_casecount\n",
    "    d2 = smoothed_rolling\n",
    "\n",
    "    seconds = 1\n",
    "    fps = 10\n",
    "    rs = [crosscorr(pd.Series(d1),pd.Series(d2), lag) for lag in range(-int(4),int(11))]\n",
    "    offset = np.floor(len(rs)/3.63)-np.argmax(rs)\n",
    "    f,ax=plt.subplots(figsize=(14,3))\n",
    "    plt.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "    print(offset)\n",
    "    ax.plot(rs,color = color_pallet[5],linewidth = 3,label = 'Smoothed Correlation')\n",
    "    ax.axvline(4,color='k',linestyle=':',label='Lag = 0')\n",
    "    print(rs)\n",
    "    #ax.axvline(np.argmax(rs),color='r',linestyle=':',label='Peak synchrony')\n",
    "    ax.set_xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14])\n",
    "    ax.set_xticklabels([-4.0,-3.0,-2.0,-1.0,0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0],fontsize = 16)\n",
    "    ax.set_xlabel('Lag, in days',fontsize = 15)\n",
    "    ax.set_ylabel('Pearson correlation',fontsize = 15)\n",
    "    ax.grid(axis = 'y',alpha = .1)\n",
    "    ax.grid(axis = 'x',alpha = .3)\n",
    "    leg = plt.legend(fontsize = 16,loc = 'best',frameon = True)\n",
    "    for line in leg.get_lines():\n",
    "        line.set_linewidth(3.0)\n",
    "    image_format = 'svg' # e.g .png, .svg, etc.\n",
    "    image_name = 'Supp8E2.svg'\n",
    "    plt.savefig(image_name, format=image_format, dpi=1200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_AA_charts_sem2(days2,sem2_pos_att,lower_ci_95_sem2,higher_ci_95_sem2,'Second Sem Positives and Permuted Second Sem Positives, Attribute Assortativity: 8/17/2020-11/23/2020',color_pallet[2],\"AA\",\"Permuted Network AA\",' (Full Semester)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_AA_charts_sem2(days1,rolling_pos_att2,lower_ci_95_rolling2,higher_ci_95_rolling2,'Rolling 10 Day Window Positives and Permuted Rolling 10 Day Window Positives, Attribute Assortativity: 8/17/2020-11/23/2020',color_pallet[5],\"AA\",\"Permuted Network AA\",' (10-Day Rolling)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_stuff_to_be_splitup2(first_sem_positives,rolling_pos_att,days):\n",
    "    formatted_dates = []\n",
    "    not_formatted_dates = []\n",
    "    for x in days:\n",
    "        new = datetime.strptime(x,'%Y-%m-%d')\n",
    "        not_formatted_dates.append(new)\n",
    "        new_time = datetime.strftime(new, '%b-%d')\n",
    "        formatted_dates.append(new_time)\n",
    "    \n",
    "    case_count_per_day = []\n",
    "    for x in range(len(days)):\n",
    "        if (days[x] in list(first_sem_positives[['Test_day', 'barcode']].groupby('Test_day').count().reset_index()['Test_day'])):\n",
    "            case_count_per_day.append(first_sem_positives[['Test_day', 'barcode']].groupby('Test_day').count().reset_index()['barcode'][list(first_sem_positives[['Test_day', 'barcode']].groupby('Test_day').count().reset_index()['Test_day']).index(days[x])])\n",
    "        else:\n",
    "            case_count_per_day.append(0)\n",
    "    \n",
    "    rolling_pos_att = [0 if math.isnan(x) else x for x in rolling_pos_att]\n",
    "\n",
    "    \n",
    "    \n",
    "    rolling_smoothed = savgol_filter(rolling_pos_att, 17, 4,mode = 'nearest')\n",
    "    case_count_smoothed = savgol_filter(case_count_per_day, 17, 4,mode = 'nearest')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (20,10))\n",
    "    ax.set_xlabel('Date (2020)',fontsize = 20)\n",
    "    ax.set_ylabel('Attribute Assortativity (10-day rolling)',fontsize = 20)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "    plt.tick_params(axis='x', which='major', labelsize=20,rotation = 70)\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.grid(axis = 'y',alpha = .1)\n",
    "    ax.grid(axis = 'y',alpha = .1)\n",
    "    ax2.grid(axis = 'x',alpha = .3)\n",
    "    ax.grid(axis = 'x',alpha = .3)\n",
    "    ax2.plot(formatted_dates,case_count_per_day,color = color_pallet[6],label = 'Case Count',linewidth = 3,linestyle=\":\")\n",
    "    ax.plot(formatted_dates,rolling_pos_att,label = \"AA\",color = color_pallet[5],linewidth = 2,linestyle=\"--\")\n",
    "    ax.plot(formatted_dates,rolling_smoothed,color = color_pallet[5],linewidth = 4, label = 'Smoothed AA')\n",
    "    ax.set_ylim(0, max(rolling_pos_att)*1.0)\n",
    "    ax2.set_ylim(0, max(case_count_per_day)*1.0)\n",
    "    ax2.plot(formatted_dates,case_count_smoothed,color = color_pallet[6], linewidth = 4,label = 'Smoothed Case Count')\n",
    "    leg = ax.legend(loc=\"upper left\",fontsize = 20,frameon=True)\n",
    "    for line in leg.get_lines():\n",
    "        line.set_linewidth(5.0)\n",
    "        \n",
    "    leg = ax2.legend(loc=\"upper right\",fontsize = 20,frameon=True)\n",
    "    for line in leg.get_lines():\n",
    "        line.set_linewidth(5.0)\n",
    "\n",
    "    ax.set_yticks(np.linspace(ax.get_yticks()[0], ax.get_yticks()[-1], len(ax2.get_yticks())))\n",
    "\n",
    "    ax2.set_yticks(np.linspace(ax2.get_yticks()[0], ax2.get_yticks()[-1], len(ax.get_yticks())))\n",
    "    plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "    ax.sharex(ax2)\n",
    "    ax2.set_ylabel('Positive Cases Per Day',fontsize =20)\n",
    "    plt.xticks(np.arange(0, len(formatted_dates), 7),fontsize = 25,rotation = 90)\n",
    "    image_format = 'svg' # e.g .png, .svg, etc.\n",
    "    image_name = 'Main2F.svg'\n",
    "    plt.savefig(image_name, format=image_format, dpi=1200)\n",
    "    plt.show()\n",
    "    \n",
    "    print('unsmoothed pearson: ',scipy.stats.pearsonr(rolling_pos_att, case_count_per_day))\n",
    "    print('smoothed pearson: ',scipy.stats.pearsonr(rolling_smoothed,case_count_smoothed))\n",
    "    \n",
    "    \n",
    "    return case_count_per_day,rolling_smoothed,case_count_smoothed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_count_per_day2,rolling_smoothed2,case_count_smoothed2 = lag_stuff_to_be_splitup2(spring_metadata_of_positives,rolling_pos_att2,days1)\n",
    "lag_finder2(rolling_pos_att2,case_count_per_day2,rolling_smoothed2,case_count_smoothed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_year_positives = pd.DataFrame(columns = ['day','p1','p2','day sum duration','day median duration','day interaction count'])\n",
    "pos_IDs = positiveIDs\n",
    "print('running')\n",
    "\n",
    "for j in final_fall:\n",
    "    j.columns = ['day','s1','s2','sum','median','count']\n",
    "    day = j['day'].iloc[0]\n",
    "    print(day)\n",
    "    g = nx.from_pandas_edgelist(j,'s1','s2',['sum','median','count'], create_using = nx.Graph())\n",
    "    if str(day) in by_date_pos_list_fall_dict:\n",
    "        rolling_positives = by_date_pos_list_fall_dict[str(day)]\n",
    "    elif str(day) not in by_date_pos_list_fall_dict:\n",
    "        by_date_pos_list_fall_dict[str(day)] = []\n",
    "    \n",
    "    rolling_positives = by_date_pos_list_fall_dict[str(day)]\n",
    "    edges = list(g.edges(data = True))\n",
    "    for x in edges:\n",
    "        if (str(x[0]) in rolling_positives) and (str(x[0]) in pos_IDs):\n",
    "            if (str(x[1]) in rolling_positives) and (str(x[1]) in pos_IDs):\n",
    "                all_year_positives.loc[len(all_year_positives.index)] = [day,x[0],x[1],x[2]['sum'],x[2]['median'],x[2]['count']]\n",
    "\n",
    "print('running2')\n",
    "\n",
    "for j in final_spring:\n",
    "    j.columns = ['day','s1','s2','sum','median','count']\n",
    "    day = j['day'].iloc[0]\n",
    "    print(day)\n",
    "    g = nx.from_pandas_edgelist(j,'s1','s2',['sum','median','count'], create_using = nx.Graph())\n",
    "    if str(day) in by_date_pos_list_spring_dict:\n",
    "        rolling_positives = by_date_pos_list_spring_dict[str(day)]\n",
    "    elif str(day) not in by_date_pos_list_spring_dict:\n",
    "        by_date_pos_list_spring_dict[str(day)] = []\n",
    "    \n",
    "    rolling_positives = by_date_pos_list_spring_dict[str(day)]\n",
    "    edges = list(g.edges(data = True))\n",
    "    for x in edges:\n",
    "        if (str(x[0]) in rolling_positives) and (str(x[0]) in pos_IDs):\n",
    "            if (str(x[1]) in rolling_positives) and (str(x[1]) in pos_IDs):\n",
    "                all_year_positives.loc[len(all_year_positives.index)] = [day,x[0],x[1],x[2]['sum'],x[2]['median'],x[2]['count']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_year_pos_aggregated = all_year_positives.groupby(['p1','p2']).agg({'day sum duration':'sum','day median duration':'median','day interaction count':['sum','size']}).reset_index()\n",
    "new_columns = [''.join(t) for t in all_year_pos_aggregated.columns]\n",
    "all_year_pos_aggregated.columns = new_columns\n",
    "all_year_pos_aggregated.columns = ['p1','p2','total interactions','median duration median','sum time spent','total days interacted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_year_pos_aggregated['inverse total interaction time'] = 1/all_year_pos_aggregated['sum time spent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_year_pos_aggregated.to_csv('all_year_pos_aggregated_refreshed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_year_pos_aggregated = all_year_pos_aggregated[all_year_pos_aggregated['p1'].isin(metadata['barcode'].values)]\n",
    "all_year_pos_aggregated = all_year_pos_aggregated[all_year_pos_aggregated['p2'].isin(metadata['barcode'].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequenced_with_classyear['barcode'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_all_graph = nx.from_pandas_edgelist(all_year_pos_aggregated,'p1','p2',['sum time spent','median duration median', 'total interactions', 'total days interacted','inverse total interaction time'],create_using = nx.Graph())\n",
    "weight = nx.get_edge_attributes(all_all_graph,'inverse total interaction time')\n",
    "weight = list(weight.values())\n",
    "weight = [x/(max(weight))*2 for x in weight]\n",
    "#weight = .5\n",
    "plt.figure(figsize = (20,20))\n",
    "color_map = []\n",
    "node_color_dict = {}\n",
    "peacock_nodes = []\n",
    "non_peacock_nodes = []\n",
    "for node in all_all_graph:\n",
    "    if node in peacock_list:\n",
    "        node_color_dict[node] = color_pallet[0]\n",
    "        color_map.append(color_pallet[0])\n",
    "        peacock_nodes.append(node)\n",
    "    elif node in sequenced_with_classyear['barcode'].tolist():\n",
    "        node_color_dict[node] = color_pallet[5]\n",
    "        color_map.append(color_pallet[5])\n",
    "        non_peacock_nodes.append(node)\n",
    "    else: \n",
    "        node_color_dict[node] = color_pallet[3]\n",
    "        color_map.append(color_pallet[3])   \n",
    "\n",
    "nodes_to_hold = all_all_graph.nodes\n",
    "\n",
    "nx.set_node_attributes(all_all_graph,node_color_dict,'sequence_status')\n",
    "\n",
    "\n",
    "node_list_dict_peacock = {}\n",
    "node_list_dict_nonpeacock = {}\n",
    "\n",
    "peacock_dist = []\n",
    "non_peacock_dist = []\n",
    "for node in all_all_graph:\n",
    "    for node2 in all_all_graph:\n",
    "        if node != node2:\n",
    "            if nx.has_path(all_all_graph,node,node2):\n",
    "                if (str([node,node2]) not in node_list_dict_peacock) and (str([node2,node]) not in node_list_dict_peacock) and (str([node,node2]) not in node_list_dict_nonpeacock) and (str([node2,node]) not in node_list_dict_nonpeacock):\n",
    "                    if node in peacock_list and node2 in peacock_list:\n",
    "                        node_list_dict_peacock[str([node,node2])] = nx.shortest_path_length(all_all_graph,node,node2)\n",
    "                    elif (node not in peacock_list and node2 in peacock_list) or (node in peacock_list and node2 not in peacock_list):\n",
    "                        node_list_dict_nonpeacock[str([node,node2])] = nx.shortest_path_length(all_all_graph,node,node2)\n",
    "\n",
    "print(len(node_list_dict_peacock))\n",
    "print(len(node_list_dict_nonpeacock))\n",
    "\n",
    "nx.write_gexf(all_all_graph, \"main5.gexf\")\n",
    "\n",
    "\n",
    "nx.draw(all_all_graph,pos =nx.spring_layout(all_all_graph,seed = 4,k=0.089, iterations=45),width = weight,with_labels = False,node_size = 100,alpha = .6,linewidths = .9,edge_color = 'grey',node_color = color_map,font_size = 14)\n",
    "#nx.draw(all_all_graph,pos =nx.kamada_kawai_layout(all_all_graph,scale = 4),width = weight,with_labels = False,node_size = 100,alpha = .6,linewidths = .9,edge_color = 'royalblue',node_color = color_map,font_size = 14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "barcode_2_lineage = {}\n",
    "for x in range(len(sequenced['barcode'])):\n",
    "    barcode_2_lineage[sequenced['barcode'].iloc[x]] = sequenced['pango_lineage'].iloc[x]\n",
    "    \n",
    "{key:val for key, val in barcode_2_lineage.items() if val != None}\n",
    "{key:val for key, val in barcode_2_lineage.items() if val != np.nan}\n",
    "\n",
    "for node in all_all_graph:\n",
    "    if node not in barcode_2_lineage:\n",
    "        barcode_2_lineage[node] = 'NOT SEQUENCED'\n",
    "\n",
    "\n",
    "nx.set_node_attributes(all_all_graph,barcode_2_lineage,'sequence_lineage')\n",
    "pos_attribute_assort_coef = assortativity.attribute_assortativity_coefficient(all_all_graph, 'sequence_lineage')\n",
    "print(pos_attribute_assort_coef)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (20,20))\n",
    "nx.draw(all_all_graph,pos =nx.spring_layout(all_all_graph,seed = 27),with_labels = False,width = 5,node_size = 100,alpha = .6,linewidths = .9,edge_color = 'grey',node_color = color_map,font_size = 14)\n",
    "plt.show()\n",
    "\n",
    "nx.write_gexf(all_all_graph, \"to_see.gexf\")\n",
    "\n",
    "print(all_all_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lineage_names = nx.get_node_attributes(all_all_graph, \"sequence_lineage\")\n",
    "new_lineage_names = {key:str(value).replace('nan','None').replace('NOT SEQUENCED','None') for (key,value) in new_lineage_names.items()}\n",
    "nx.set_node_attributes(all_all_graph, new_lineage_names, name=\"sequence_lineage\")\n",
    "graph_attributes = list(set(new_lineage_names.values()))\n",
    "\n",
    "#[[PL],[MinPL,MaxPl,MedPL] [C, S,KT,KA]\n",
    "num_levels = 1\n",
    "\n",
    "for node in all_all_graph.nodes(data = True):\n",
    "    print('ROOT:',node[0],node[1]['sequence_lineage'])\n",
    "    node_details = {node[0]:{i:[[],[],[]] for i in graph_attributes}}\n",
    "    node_details[node[0]][node[1]['sequence_lineage']][0].append(0)\n",
    "    for counter in range(1,num_levels+1):\n",
    "        level = nx.descendants_at_distance(all_all_graph,node[0],counter)\n",
    "        for secondary_node in all_all_graph.nodes(data = True):\n",
    "            if secondary_node[0] in level:\n",
    "                node_details[node[0]][secondary_node[1]['sequence_lineage']][0].append(counter)\n",
    "    for attribute in node_details[node[0]]:\n",
    "        node_details[node[0]][attribute][1]= [min(node_details[node[0]][attribute][0],default = np.nan),max(node_details[node[0]][attribute][0],default = np.nan)]\n",
    "        if len(node_details[node[0]][attribute][0]) == 0:\n",
    "            node_details[node[0]][attribute][1].append(0)\n",
    "        else:\n",
    "            node_details[node[0]][attribute][1].append(float(sum(node_details[node[0]][attribute][0])/len(node_details[node[0]][attribute][0])))\n",
    "        node_details[node[0]][attribute][2].append(len(node_details[node[0]][attribute][0]))\n",
    "        node_details[node[0]][attribute][2].append(len(level)+1)\n",
    "        node_details[node[0]][attribute][2].append((len(node_details[node[0]][attribute][0]))/(len(level)+1))\n",
    "        if ((len(level)+1) - len(node_details[node[0]]['None'][0])) != 0:\n",
    "            node_details[node[0]][attribute][2].append(len(node_details[node[0]][attribute][0])/((len(level)+1) - len(node_details[node[0]]['None'][0])))\n",
    "        else:\n",
    "            node_details[node[0]][attribute][2].append(0.0)\n",
    "    node_details[node[0]]['None'][2][3]=0.0\n",
    "    print(node_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lineage_names = nx.get_node_attributes(all_all_graph, \"sequence_lineage\")\n",
    "new_lineage_names = {key:str(value).replace('nan','None').replace('NOT SEQUENCED','None') for (key,value) in new_lineage_names.items()}\n",
    "nx.set_node_attributes(all_all_graph, new_lineage_names, name=\"sequence_lineage\")\n",
    "graph_attributes = list(set(new_lineage_names.values()))\n",
    "\n",
    "#[[PL],[MinPL,MaxPl,MedPL] [C, S,KT,KA]\n",
    "num_levels = 4\n",
    "\n",
    "for node in all_all_graph.nodes(data = True):\n",
    "    print('ROOT:',node[0],node[1]['sequence_lineage'])\n",
    "    node_details = {node[0]:{i:[[]] for i in graph_attributes}}\n",
    "    node_details[node[0]][node[1]['sequence_lineage']][0].append(0)\n",
    "    for counter in range(1,num_levels+1):\n",
    "        level = nx.descendants_at_distance(all_all_graph,node[0],counter)\n",
    "        for secondary_node in all_all_graph.nodes(data = True):\n",
    "            if secondary_node[0] in level:\n",
    "                node_details[node[0]][secondary_node[1]['sequence_lineage']][0].append(counter)\n",
    "    for attribute in node_details[node[0]]:\n",
    "        if len(node_details[node[0]][attribute][0]) !=0:\n",
    "            node_details[node[0]][attribute][0] = sum(node_details[node[0]][attribute][0])/len(node_details[node[0]][attribute][0])\n",
    "        else: \n",
    "            node_details[node[0]][attribute][0] = np.nan\n",
    "    holder = {key:value[0] for (key,value) in node_details[node[0]].items()}\n",
    "    print(holder)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barcode_2_lineage = {}\n",
    "for x in range(len(sequenced['barcode'])):\n",
    "    barcode_2_lineage[sequenced['barcode'].iloc[x]] = sequenced['pango_lineage'].iloc[x]\n",
    "    \n",
    "{key:val for key, val in barcode_2_lineage.items() if val != None}\n",
    "{key:val for key, val in barcode_2_lineage.items() if val != np.nan}\n",
    "{key:val for key, val in barcode_2_lineage.items() if val in nodes_to_hold}\n",
    "\n",
    "barcode_2_lineage\n",
    "\n",
    "\n",
    "nodes_to_consider = non_peacock_nodes + peacock_nodes\n",
    "g = all_all_graph.copy()\n",
    "g = nx.subgraph(g,nodes_to_consider)\n",
    "node_sequence_dict = {}\n",
    "lineages_found = 0\n",
    "lineages = []\n",
    "color_map = []\n",
    "node_found = []\n",
    "for node in g:\n",
    "    node_sequence_dict[node] = barcode_2_lineage[node]\n",
    "    if barcode_2_lineage[node] not in lineages:\n",
    "        lineages_found += 1\n",
    "        if lineages_found == 7:\n",
    "            lineages_found = 0\n",
    "        lineages.append(barcode_2_lineage[node])\n",
    "    color_map.append(color_pallet[lineages_found])\n",
    "\n",
    "\n",
    "nx.set_node_attributes(g,node_color_dict,'sequence_lineage')\n",
    "pos_attribute_assort_coef = assortativity.attribute_assortativity_coefficient(g, 'sequence_lineage')\n",
    "print(pos_attribute_assort_coef)\n",
    "\n",
    "random_list = []\n",
    "#create permuted version\n",
    "for x in range(40):\n",
    "    for nodes in g:\n",
    "        g.nodes[nodes]['sequence_lineage'] = 0\n",
    "        choose_prob = random.uniform(0,1)\n",
    "        if (choose_prob <= (len(peacock_nodes)/len(non_peacock_nodes))):\n",
    "            g.nodes[nodes]['sequence_lineage'] = 1 \n",
    "        else:\n",
    "            g.nodes[nodes]['sequence_lineage'] = 0\n",
    "    pos_attribute_assort_coef = assortativity.attribute_assortativity_coefficient(g, 'sequence_lineage',nodes = nodes_to_consider)\n",
    "    random_list.append(pos_attribute_assort_coef)\n",
    "random_list = [item for item in random_list if not(math.isnan(item)) == True]\n",
    "lower_ci_95 = (list(scipy.stats.t.interval(alpha=0.95, df=len(random_list)-1, loc=np.mean(random_list), scale=scipy.stats.sem(random_list)))[0])\n",
    "higher_ci_95 = (list(scipy.stats.t.interval(alpha=0.95, df=len(random_list)-1, loc=np.mean(random_list), scale=scipy.stats.sem(random_list)))[1])\n",
    "print(lower_ci_95)\n",
    "print(higher_ci_95)\n",
    "\n",
    "plt.figure(figsize = (20,20))\n",
    "nx.draw(g,pos =nx.spring_layout(g,seed = 27),with_labels = False,width = 5,node_size = 100,alpha = .6,linewidths = .9,edge_color = 'grey',node_color = color_map,font_size = 14)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#nx.write_gexf(all_all_graph, \"for_gephi2.gexf\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peacock_IDs = list(peacock['barcode'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genetic_distance = pd.read_csv('genetic_distance_cleaned_refreshed.csv')\n",
    "path_info = pd.read_csv('new_full_cleaned_refreshed.csv',usecols = ['p1','p2','path dist','weighted path dist'])\n",
    "genetic_distance.columns = ['index','p1','p2','snps']\n",
    "path_info.columns = ['p1','p2','path dist','weighted path dist']\n",
    "new_gen_dist = pd.merge(genetic_distance,path_info,on=['p1','p2'],how = 'left')\n",
    "new_gen_dist.to_csv('path_and_gen_dist_refreshed.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cleaned_paths = pd.read_csv('path_and_gen_dist_refreshed.csv',usecols = ['p1','p2','snps','path dist','weighted path dist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cleaned_paths['path_inf_or_not'] = ['infinite' if final_cleaned_paths['path dist'][x] == np.inf else 'finite' for x in range(len(final_cleaned_paths))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cleaned_paths['p1 status'] = ['peacock' if final_cleaned_paths['p1'][x] in peacock_IDs else 'non-peacock' for x in range(len(final_cleaned_paths))]\n",
    "final_cleaned_paths['p2 status'] = ['peacock' if final_cleaned_paths['p2'][x] in peacock_IDs else 'non-peacock' for x in range(len(final_cleaned_paths))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cleaned_paths['peacock'] = ['peacock pair' if (final_cleaned_paths['p1 status'][x] == 'peacock' and final_cleaned_paths['p2 status'][x] == 'peacock') else 'non-peacock pair' for x in range(len(final_cleaned_paths))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barcode_2_lineage = {}\n",
    "for x in range(len(sequenced['barcode'])):\n",
    "    barcode_2_lineage[sequenced['barcode'].iloc[x]] = sequenced['pango_lineage'].iloc[x]\n",
    "    \n",
    "{key:val for key, val in barcode_2_lineage.items() if val != None}\n",
    "{key:val for key, val in barcode_2_lineage.items() if val != np.nan}\n",
    "{key:val for key, val in barcode_2_lineage.items() if val in nodes_to_hold}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cleaned_paths['status'] = final_cleaned_paths['p1 status']+\",\"+final_cleaned_paths['p2 status']\n",
    "final_cleaned_paths['status'] = [len(list(set(x.split(',')))) for x in final_cleaned_paths['status']]\n",
    "final_cleaned_paths = final_cleaned_paths[final_cleaned_paths['p2'].isin(nodes_to_hold)]\n",
    "final_cleaned_paths = final_cleaned_paths[final_cleaned_paths['p1'].isin(nodes_to_hold)]\n",
    "\n",
    "\n",
    "non_peacock_pairings = final_cleaned_paths[final_cleaned_paths['peacock'] == 'non-peacock pair' ]\n",
    "non_peacock_pairings = non_peacock_pairings[non_peacock_pairings['status'] == 1 ]\n",
    "\n",
    "peacock_pairs_df = final_cleaned_paths[final_cleaned_paths['peacock'] == 'peacock pair' ]\n",
    "\n",
    "#make box plot for inf versus non inf for peacock versus non peacock\n",
    "non_peacock_pairings_inf_snps = list(non_peacock_pairings[non_peacock_pairings['path_inf_or_not'] == 'infinite']['snps'])\n",
    "non_peacock_pairings_fin_snps = list(non_peacock_pairings[non_peacock_pairings['path_inf_or_not'] == 'finite']['snps'])\n",
    "\n",
    "peacock_pairings_inf_snps = list(peacock_pairs_df[peacock_pairs_df['path_inf_or_not'] == 'infinite']['snps'])\n",
    "peacock_pairings_fin_snps = list(peacock_pairs_df[peacock_pairs_df['path_inf_or_not'] == 'finite']['snps'])\n",
    "\n",
    "\n",
    "non_peacock_pairings_fin = non_peacock_pairings[non_peacock_pairings['path_inf_or_not'] == 'finite']\n",
    "peacock_pairings_fin = peacock_pairs_df[peacock_pairs_df['path_inf_or_not'] == 'finite']\n",
    "\n",
    "\n",
    "print(scipy.stats.mannwhitneyu(non_peacock_pairings_inf_snps,non_peacock_pairings_fin_snps))\n",
    "print(scipy.stats.mannwhitneyu(peacock_pairings_inf_snps,peacock_pairings_fin_snps))\n",
    "\n",
    "\n",
    "print(scipy.stats.mannwhitneyu(non_peacock_pairings_fin['path dist'],peacock_pairings_fin['path dist']))\n",
    "\n",
    "non_peacock_pairings_fin['type'] = 1\n",
    "peacock_pairings_fin['type'] = 0\n",
    "\n",
    "\n",
    "for_dist_compare = pd.concat([non_peacock_pairings_fin,peacock_pairings_fin], ignore_index=True)\n",
    "\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.grid(axis = 'y',alpha = .1)\n",
    "\n",
    "sns.violinplot(y = \"path dist\", x = 'type' ,data=for_dist_compare,inner = 'box',cut = 0,split = True, linewidth=2.5,width = .9,scale = 'width')\n",
    "barplot_annotate_brackets(0,1,p,[0,1],[8,8],maxasterix=3,fs = 12)\n",
    "plt.yticks(np.arange(0,10,step =1),labels = np.arange(0,10,step =1),fontsize = 14)\n",
    "plt.xticks([0,1],['B.1.429.1 Pair','Mixed Pair'],fontsize = 15)\n",
    "plt.ylabel('Path Distance',fontsize = 13)\n",
    "plt.xlabel('')\n",
    "plt.ylim(0,10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set_palette(sns.color_palette([color_pallet[5],color_pallet[1]]))\n",
    "u,p = scipy.stats.mannwhitneyu(peacock_pairings_inf_snps,peacock_pairings_fin_snps)\n",
    "print(p,u)\n",
    "plt.figure(figsize = (5,5))\n",
    "sns.violinplot(x = \"path_inf_or_not\", y = \"snps\",data=final_cleaned_paths[final_cleaned_paths['peacock'] == 'peacock pair' ],inner = 'box',cut = 0,split = True, linewidth=2.5)\n",
    "#plt.title(\"Peacock Pair\",fontsize = 15,fontweight = 'bold')\n",
    "plt.xlabel('',fontsize = 13)\n",
    "plt.ylabel('SNP Distance',fontsize = 16)\n",
    "plt.ylim(0,8)\n",
    "barplot_annotate_brackets(0,1,p,[0,1],[6,6],maxasterix=3,fs = 12)\n",
    "plt.yticks(np.arange(0,9,step =1),labels = np.arange(0,9,step =1),fontsize = 16)\n",
    "plt.xticks([],[])\n",
    "#leg = plt.legend(loc = 'upper left',labels=['Within Social Proximity Network', 'Not Within Social Proximity Network'],frameon = True, borderpad=.5,fontsize = 11)\n",
    "#leg.legendHandles[0].set_color(color_pallet[5])\n",
    "#leg.legendHandles[1].set_color(color_pallet[1])\n",
    "# set the linewidth of each legend object\n",
    "#for legobj in leg.legendHandles:\n",
    "#    legobj.set_linewidth(6.0)\n",
    "\n",
    "image_format = 'svg' # e.g .png, .svg, etc.\n",
    "image_name = 'Main5C.svg'\n",
    "\n",
    "plt.savefig(image_name, format=image_format, dpi=1200)\n",
    "plt.show()\n",
    "\n",
    "medians = final_cleaned_paths[final_cleaned_paths['peacock'] == 'peacock pair' ].groupby(['path_inf_or_not'])['snps'].median().values\n",
    "nobs = final_cleaned_paths[final_cleaned_paths['peacock'] == 'peacock pair' ]['path_inf_or_not'].value_counts().values\n",
    "nobs = [str(x) for x in nobs.tolist()]\n",
    "nobs = [\"n: \" + i for i in nobs]\n",
    " \n",
    "print(medians)\n",
    "print(nobs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#make box plot for inf versus non inf for peacock versus non peacock\n",
    "non_peacock_pairings_inf = list(non_peacock_pairings[non_peacock_pairings['path_inf_or_not'] == 'infinite']['snps'])\n",
    "non_peacock_pairings_fin = list(non_peacock_pairings[non_peacock_pairings['path_inf_or_not'] == 'finite']['snps'])\n",
    "\n",
    "peacock_pairings_inf = list(peacock_pairs_df[peacock_pairs_df['path_inf_or_not'] == 'infinite']['snps'])\n",
    "peacock_pairings_fin = list(peacock_pairs_df[peacock_pairs_df['path_inf_or_not'] == 'finite']['snps'])\n",
    "\n",
    "print(scipy.stats.mannwhitneyu(non_peacock_pairings_inf,non_peacock_pairings_fin))\n",
    "print(scipy.stats.mannwhitneyu(peacock_pairings_inf,peacock_pairings_fin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_gen_dist_path_dist = final_cleaned_paths[final_cleaned_paths['path_inf_or_not']=='infinite']\n",
    "non_inf_gen_dist_path_dist = final_cleaned_paths[final_cleaned_paths['path_inf_or_not']=='finite']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print('correlation path, snps, non infinite (all) ')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist['path dist'], b=non_inf_gen_dist_path_dist['snps']))\n",
    "\n",
    "print()\n",
    "print('correlation weighted path, snps, non infinite (all) ')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist['weighted path dist'], b=non_inf_gen_dist_path_dist['snps']))\n",
    "\n",
    "\n",
    "print()\n",
    "print('correlation path, snps, non infinite (all) snps < 40')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<40]['path dist'], b=non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<40]['snps']))\n",
    "\n",
    "print()\n",
    "print('correlation weighted path, snps, non infinite (all) snps < 40')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<40]['weighted path dist'], b=non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<40]['snps']))\n",
    "\n",
    "\n",
    "print()\n",
    "print('correlation path, snps, non infinite (all) snps < 30')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<30]['path dist'], b=non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<30]['snps']))\n",
    "\n",
    "print()\n",
    "print('correlation weighted path, snps, non infinite (all) snps < 30')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<30]['weighted path dist'], b=non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<30]['snps']))\n",
    "\n",
    "\n",
    "print()\n",
    "print('correlation path, snps, non infinite (all) snps < 25')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<25]['path dist'], b=non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<25]['snps']))\n",
    "\n",
    "print()\n",
    "print('correlation weighted path, snps, non infinite (all) snps < 25')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<25]['weighted path dist'], b=non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<25]['snps']))\n",
    "\n",
    "\n",
    "print()\n",
    "print('correlation path, snps, non infinite (all) snps < 20')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<20]['path dist'], b=non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<20]['snps']))\n",
    "\n",
    "print()\n",
    "print('correlation weighted path, snps, non infinite (all) snps < 20')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<20]['weighted path dist'], b=non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<20]['snps']))\n",
    "\n",
    "\n",
    "print()\n",
    "print('correlation path, snps, non infinite (all) snps < 15')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<15]['path dist'], b=non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<15]['snps']))\n",
    "\n",
    "print()\n",
    "print('correlation weighted path, snps, non infinite (all) snps < 15')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<15]['weighted path dist'], b=non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<15]['snps']))\n",
    "\n",
    "\n",
    "print()\n",
    "print('correlation path, snps, non infinite (all) snps < 10')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<10]['path dist'], b=non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<10]['snps']))\n",
    "\n",
    "print()\n",
    "print('correlation weighted path, snps, non infinite (all) snps < 10')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<10]['weighted path dist'], b=non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<10]['snps']))\n",
    "\n",
    "print()\n",
    "print('correlation path, snps, non infinite (all) snps < 5')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<5]['path dist'], b=non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<5]['snps']))\n",
    "\n",
    "print()\n",
    "print('correlation weighted path, snps, non infinite (all) snps < 5')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<5]['weighted path dist'], b=non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<5]['snps']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('running')\n",
    "pe_pe_sum = []\n",
    "pe_n_sum = []\n",
    "pe_pe_median = []\n",
    "pe_n_median = []\n",
    "pe_pe_ints = []\n",
    "pe_n_ints = []\n",
    "pe_p_sum = []\n",
    "pe_p_median = []\n",
    "pe_p_ints = []\n",
    "\n",
    "for j in final_spring:\n",
    "    j.columns = ['day','s1','s2','sum duration','median duration','num interactions']\n",
    "    day = j['day'].iloc[0]\n",
    "    g = nx.from_pandas_edgelist(j,'s1','s2',['sum duration','median duration','num interactions'], create_using = nx.Graph())\n",
    "    rolling_positives = by_date_pos_list_spring_dict[str(day)]\n",
    "    edges = list(g.edges(data = True))\n",
    "    for x in edges:\n",
    "        if (str(x[0]) in rolling_positives) and (str(x[0]) in peacock_IDs): \n",
    "            if (str(x[1]) in rolling_positives) and (str(x[1]) in peacock_IDs): #pea - pea\n",
    "                pe_pe_sum.append(x[2]['sum duration'])\n",
    "                pe_pe_median.append(x[2]['median duration'])\n",
    "                pe_pe_ints.append(x[2]['num interactions'])\n",
    "            elif (str(x[1]) in rolling_positives) and (str(x[1]) not in peacock_IDs): #pea - p\n",
    "                pe_p_sum.append(x[2]['sum duration'])\n",
    "                pe_p_median.append(x[2]['median duration'])\n",
    "                pe_p_ints.append(x[2]['num interactions'])\n",
    "            else:                                                               #pea - n\n",
    "                pe_n_sum.append(x[2]['sum duration'])\n",
    "                pe_n_median.append(x[2]['median duration'])\n",
    "                pe_n_ints.append(x[2]['num interactions'])\n",
    "        if (str(x[1]) in rolling_positives) and (str(x[1]) in peacock_IDs): \n",
    "            if (str(x[0]) in rolling_positives) and (str(x[0]) not in peacock_IDs): #p - pea\n",
    "                pe_p_sum.append(x[2]['sum duration'])\n",
    "                pe_p_median.append(x[2]['median duration'])\n",
    "                pe_p_ints.append(x[2]['num interactions'])\n",
    "            elif (str(x[0]) not in rolling_positives) and (str(x[0]) not in peacock_IDs):   #n - pea\n",
    "                pe_n_sum.append(x[2]['sum duration'])\n",
    "                pe_n_median.append(x[2]['median duration'])\n",
    "                pe_n_ints.append(x[2]['num interactions'])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1, p_pe_n = mannwhitneyu(pe_pe_sum, pe_n_sum)\n",
    "print(\"pe-n\",p_pe_n)\n",
    "U1, p_pe_p = mannwhitneyu(pe_pe_sum, pe_p_sum)\n",
    "print(\"pe-p\",p_pe_p)\n",
    "U1, p_p_n = mannwhitneyu(pe_n_sum, pe_p_sum)\n",
    "print(\"p-n\",p_p_n)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12,12))\n",
    "bp1 = box_plot(pe_pe_sum,color_pallet[2],1)\n",
    "bp2 = box_plot(pe_p_sum,color_pallet[6],2)\n",
    "bp3 = box_plot(pe_n_sum,color_pallet[5],3)\n",
    "ax.set_ylim(0,2500)\n",
    "ax.legend([bp1[\"boxes\"][0], bp2[\"boxes\"][0],bp3[\"boxes\"][0]],  ['Peacock/Peacock','Peacock/Positive','Peacock/Negative'],fontsize = 20,frameon=True)\n",
    "ax.set_xticks([1,2,3])\n",
    "ax.set_yticks(np.arange(0,2700,step =250))\n",
    "ax.set_yticklabels(labels = np.arange(0,2700,step =250),fontsize = 22)\n",
    "ax.set_xticklabels(labels = ['Peacock/Peacock','Peacock/Positive','Peacock/Negative'],fontsize = 17)\n",
    "plt.grid(axis = 'x',visible = False)\n",
    "ax.set_ylabel('Total Interaction Duration By Pair Per Day(minutes)',fontsize = 20)\n",
    "image_format = 'svg' # e.g .png, .svg, etc.\n",
    "image_name = 'Supp11A2.svg'\n",
    "plt.savefig(image_name, format=image_format, dpi=1200)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12,12))\n",
    "bp1 = box_plot_no_fliers(pe_pe_sum,color_pallet[2],1)\n",
    "bp2 = box_plot_no_fliers(pe_p_sum,color_pallet[6],2)\n",
    "bp3 = box_plot_no_fliers(pe_n_sum,color_pallet[5],3)\n",
    "\n",
    "ax.legend([bp1[\"boxes\"][0], bp2[\"boxes\"][0],bp3[\"boxes\"][0]],   ['Peacock/Peacock','Peacock/Positive','Peacock/Negative'],fontsize = 20,frameon=True)\n",
    "ax.set_xticks([1,2,3])\n",
    "ax.set_yticks(np.arange(0,2500,step =150))\n",
    "ax.set_ylim(0,2350)\n",
    "ax.set_yticklabels(labels = np.arange(0,2500,step =150),fontsize = 22)\n",
    "ax.set_xticklabels(labels =  ['Peacock/Peacock','Peacock/Positive','Peacock/Negative'],fontsize = 17)\n",
    "plt.grid(axis = 'x',visible = False)\n",
    "plt.grid(axis = 'y',alpha = .3)\n",
    "ax.set_ylabel('Total Interaction Duration By Pair Per Day(minutes)',fontsize = 20)\n",
    "barplot_annotate_brackets(0,1,p_pe_p,[1,1.98],[1350,5],maxasterix=3,fs = 16)\n",
    "barplot_annotate_brackets(0,1,p_p_n,[2.02,3.05],[1350,5],maxasterix=3,fs = 16)\n",
    "barplot_annotate_brackets(0,1,p_pe_n,[1.02,3.05],[1600,5],maxasterix=3,fs = 16)\n",
    "image_format = 'svg' # e.g .png, .svg, etc.\n",
    "image_name = 'Supp11A1.svg'\n",
    "plt.savefig(image_name, format=image_format, dpi=1200)\n",
    "plt.show()\n",
    "\n",
    "print('spring 2021')\n",
    "print(\"median pea-pea sum\", statistics.median(pe_pe_sum))\n",
    "print(\"stdev pea-pea sum\",statistics.stdev(pe_pe_sum))\n",
    "print(\"median pea-n sum\", statistics.median(pe_n_sum))\n",
    "print(\"stdev pea-n sum\",statistics.stdev(pe_n_sum))\n",
    "print(\"median pea-p sum\", statistics.median(pe_p_sum))\n",
    "print(\"stdev pea-p sum\",statistics.stdev(pe_p_sum))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1, p_pe_n = mannwhitneyu(pe_pe_median, pe_n_median)\n",
    "print(\"pe-n\",p_pe_n)\n",
    "U1, p_pe_p = mannwhitneyu(pe_pe_median, pe_p_median)\n",
    "print(\"pe-p\",p_pe_p)\n",
    "U1, p_p_n = mannwhitneyu(pe_n_median, pe_p_median)\n",
    "print(\"p-n\",p_p_n)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12,12))\n",
    "bp1 = box_plot(pe_pe_median,color_pallet[2],1)\n",
    "bp2 = box_plot(pe_p_median,color_pallet[6],2)\n",
    "bp3 = box_plot(pe_n_median,color_pallet[5],3)\n",
    "\n",
    "ax.legend([bp1[\"boxes\"][0], bp2[\"boxes\"][0],bp3[\"boxes\"][0]],   ['Peacock/Peacock','Peacock/Positive','Peacock/Negative'],fontsize = 20,frameon= True)\n",
    "ax.set_xticks([1,2,3])\n",
    "ax.set_yticks(np.arange(0,1500,step =100))\n",
    "ax.set_yticklabels(labels = np.arange(0,1500,step =100),fontsize = 22)\n",
    "ax.set_xticklabels(labels =  ['Peacock/Peacock','Peacock/Positive','Peacock/Negative'],fontsize = 17)\n",
    "plt.grid(axis = 'x',visible = False)\n",
    "ax.set_ylabel('Median Interaction Duration By Pair Per Day(minutes)',fontsize = 20)\n",
    "image_format = 'svg' # e.g .png, .svg, etc.\n",
    "image_name = 'Supp11B1.svg'\n",
    "plt.savefig(image_name, format=image_format, dpi=1200)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12,12))\n",
    "bp1 = box_plot_no_fliers(pe_pe_median,color_pallet[2],1)\n",
    "bp2 = box_plot_no_fliers(pe_p_median,color_pallet[6],2)\n",
    "bp3 = box_plot_no_fliers(pe_n_median,color_pallet[5],3)\n",
    "\n",
    "ax.legend([bp1[\"boxes\"][0], bp2[\"boxes\"][0],bp3[\"boxes\"][0]],  ['Peacock/Peacock','Peacock/Positive','Peacock/Negative'],fontsize = 17,frameon=True)\n",
    "ax.set_xticks([1,2,3])\n",
    "ax.set_yticks(np.arange(0,620,step =20))\n",
    "ax.set_yticklabels(labels = np.arange(0,620,step =20),fontsize = 22)\n",
    "ax.set_xticklabels(labels =  ['Peacock/Peacock','Peacock/Positive','Peacock/Negative'],fontsize = 17)\n",
    "plt.grid(axis = 'x',visible = False)\n",
    "plt.grid(axis = 'y',alpha = .3)\n",
    "ax.set_ylim(0,600)\n",
    "barplot_annotate_brackets(0,1,p_pe_p,[1,1.98],[390,5],maxasterix=3,fs = 16)\n",
    "barplot_annotate_brackets(0,1,p_p_n,[2.02,3.05],[390,5],maxasterix=3,fs = 16)\n",
    "barplot_annotate_brackets(0,1,p_pe_n,[1.0,3.05],[430,5],maxasterix=3,fs = 16)\n",
    "ax.set_ylabel('Median Interaction Duration By Pair Per Day(minutes)',fontsize = 20)\n",
    "image_format = 'svg' # e.g .png, .svg, etc.\n",
    "image_name = 'Supp11B2.svg'\n",
    "plt.savefig(image_name, format=image_format, dpi=1200)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('spring 2021')\n",
    "print(\"median pea-pea median\", statistics.median(pe_pe_median))\n",
    "print(\"stdev pea-pea median\",statistics.stdev(pe_pe_median))\n",
    "print(\"median pea-n median\", statistics.median(pe_n_median))\n",
    "print(\"stdev pea-n median\",statistics.stdev(pe_n_median))\n",
    "print(\"median p-pea median\", statistics.median(pe_p_median))\n",
    "print(\"stdev p-pea median\",statistics.stdev(pe_p_median))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4896b2645ede46043ed0510367b3623ca0de3236b96e3ab1b610ad0d6af9ff2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
