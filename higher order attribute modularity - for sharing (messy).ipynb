{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import ast\n",
    "import datetime\n",
    "import json\n",
    "from scipy.stats import mannwhitneyu\n",
    "import random\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import networkx as nx\n",
    "from scipy.stats import mannwhitneyu\n",
    "import statistics\n",
    "from networkx.algorithms import assortativity\n",
    "from scipy.signal import savgol_filter\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy import signal\n",
    "import math\n",
    "import string\n",
    "import scipy\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_format_and_replace_pos_barcodes(sem_interactions,barcode_dict,filename):\n",
    "    new_rows = []\n",
    "    count = 0\n",
    "    for row,val in sem_interactions.iterrows():\n",
    "        interactions_list = val['interactions']\n",
    "        if (pd.isna(interactions_list)):\n",
    "            user = val['user']\n",
    "            converted_user = user\n",
    "            if user in barcode_dict.keys():\n",
    "                converted_user = barcode_dict[user]\n",
    "            to_add = [val['day'],converted_user,val['building'],float(\"nan\"),float(\"nan\")]\n",
    "            new_rows.append(to_add)\n",
    "        else:\n",
    "            interactions_list = [interactions_list[1:-1]]\n",
    "            for x in interactions_list:\n",
    "                y = ast.literal_eval(x.replace('[','[\\\"').replace(',','\",').replace('\\'\\\",','\\','))\n",
    "                interactions = []\n",
    "                if type(y)!=str:\n",
    "                    for item in list(y):\n",
    "                        new_int = ast.literal_eval(item)\n",
    "                        id = new_int[0]\n",
    "                        if id in barcode_dict.keys():\n",
    "                            converted_id = barcode_dict[id]\n",
    "                            new_int = [converted_id,new_int[1]]\n",
    "                        interactions.append(new_int)\n",
    "                else:\n",
    "                    new_int = ast.literal_eval(y)\n",
    "                    id = new_int[0]\n",
    "                    if id in barcode_dict.keys():\n",
    "                        converted_id = barcode_dict[id]\n",
    "                        new_int = [converted_id,new_int[1]]\n",
    "                    interactions.append(new_int)\n",
    "            user = val['user']\n",
    "            converted_user= user\n",
    "            if user in barcode_dict.keys():\n",
    "                converted_user = barcode_dict[user]\n",
    "            prefix = [val['day'],converted_user,val['building']]\n",
    "            for x in interactions:\n",
    "                new_list = prefix + x\n",
    "                to_add = new_list\n",
    "                new_rows.append(to_add)\n",
    "    interactions_full = pd.DataFrame(new_rows,columns = ['day','user_1','building','user_2','duration'])\n",
    "    interactions_full.to_csv(filename,index = False)\n",
    "\n",
    "def make_by_ap_cleaned_interaction_lists(sem_interactions,guest_list,barcode_dict,filename,non_students_list):\n",
    "    sem_interactions = sem_interactions.drop(columns = ['start_ts','end_ts','interactions'])\n",
    "    sem_interactions = sem_interactions[~sem_interactions['user'].isin(guest_list)]\n",
    "    sem_interactions = sem_interactions[~sem_interactions['user'].isin(non_students_list)]\n",
    "\n",
    "    new_rows = []\n",
    "    for row, val in sem_interactions.iterrows():\n",
    "        user = val['user']\n",
    "        day = val['day']\n",
    "        duration = val['duration']\n",
    "        building = val['building']\n",
    "        ap = val['ap']\n",
    "        int_count = val['interaction_count']\n",
    "        if user in barcode_dict.keys():\n",
    "            user = barcode_dict[user]\n",
    "        to_add = [day,user,duration,building,ap,int_count]\n",
    "        new_rows.append(to_add)\n",
    "    sem_interactions_final = pd.DataFrame(new_rows,columns = ['day','user','duration','building','ap','int_count'])\n",
    "    sem_interactions_final.to_csv(filename,index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in all guest/student flag file\n",
    "authentication_info = pd.read_csv('authenticated_student_file.csv', compression='gzip', header=0, sep=',', quotechar='\"')\n",
    "\n",
    "students = authentication_info[authentication_info['is_student'] == True]\n",
    "students = list(students['identity'].values)\n",
    "\n",
    "non_students = authentication_info[authentication_info['is_student'] == False]\n",
    "non_students_list= list(non_students['identity'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in concatenated wifi data\n",
    "cleaned_new_full = pd.read_csv('new_full_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cut dates\n",
    "cut_summer = cleaned_new_full[cleaned_new_full['day']>='2020-08-17']\n",
    "cut_summer2 = cut_summer[cut_summer['day']<='2021-04-30']\n",
    "cut_break = cut_summer2.drop(cut_summer2[((cut_summer2['day']<='2021-01-18') & (cut_summer2['day']>='2020-11-20'))].index)\n",
    "new_cleandate_file = cut_break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull in barcode key cmu 2 da\n",
    "barcode_key = pd.read_csv('cmu_2_da.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean na barcodes\n",
    "barcode_key_cleaned = barcode_key.dropna().reset_index()\n",
    "\n",
    "#make barcode dict mapping user to cmu barcode\n",
    "barcode_dict = {}\n",
    "for row,val in barcode_key_cleaned.iterrows():\n",
    "    barcode_dict[val['user']] = val['barcode']\n",
    "\n",
    "#create guest list (ie.users that only came on campus 1-3 days during year valid dates\n",
    "guest_list = []\n",
    "for row, val in new_cleandate_file.groupby('user').count().iterrows():\n",
    "    if val['day'] < 4:\n",
    "        guest_list.append(row)\n",
    "\n",
    "#create fall interactions file\n",
    "fall_interactions = new_cleandate_file[new_cleandate_file['day'] >= '2020-08-17']\n",
    "fall_interactions = fall_interactions[fall_interactions['day'] <= '2020-11-20']\n",
    "\n",
    "#create spring interactions file\n",
    "spring_interactions = new_cleandate_file[new_cleandate_file['day'] >= '2021-01-18']\n",
    "spring_interactions = spring_interactions[spring_interactions['day'] <= '2021-04-30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count all users precleaning\n",
    "all_users = []\n",
    "for row, val in new_cleandate_file.groupby('user').count().iterrows():\n",
    "    all_users.append(row)\n",
    "\n",
    "#count all users in fall semester precleaning\n",
    "fall_users = []\n",
    "for row, val in fall_interactions.groupby('user').count().iterrows():\n",
    "    fall_users.append(row)\n",
    "\n",
    "#count all users in spring semester precleaning\n",
    "spring_users = []\n",
    "for row, val in spring_interactions.groupby('user').count().iterrows():\n",
    "    spring_users.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count all users before and after cleaning with no repeats of users by using sets\n",
    "users_after_cleaning_new = set(all_users) - (set(non_students_list + guest_list))\n",
    "print('all users before cleaning',len(all_users))\n",
    "print('new_cleaning', len(users_after_cleaning_new))\n",
    "\n",
    "#count fall users before and after cleaning with no repeats of users by using sets\n",
    "users_after_cleaning_new = set(fall_users) - (set(non_students_list + guest_list))\n",
    "print('fall users before cleaning',len(fall_users))\n",
    "print('new user count fall', len(users_after_cleaning_new))\n",
    "\n",
    "\n",
    "#count spring users before and after cleaning with no repeats of users by using sets\n",
    "users_after_cleaning_new = set(spring_users) - (set(non_students_list + guest_list))\n",
    "print('spring users before cleaning',len(spring_users))\n",
    "print('new user count spring', len(users_after_cleaning_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing solo ints\n",
    "fall_interactions_no_solos = fall_interactions.dropna().reset_index()\n",
    "spring_interactions_no_solos = spring_interactions.dropna().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix_format_and_replace_pos_barcodes(fall_interactions,barcode_dict,'fall_interactions_full.csv')\n",
    "fall_interactions_full = pd.read_csv('fall_interactions_full.csv')\n",
    "\n",
    "#fix_format_and_replace_pos_barcodes(spring_interactions,barcode_dict,'spring_interactions_full.csv')\n",
    "spring_interactions_full = pd.read_csv('spring_interactions_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('running')\n",
    "#make_by_ap_cleaned_interaction_lists(fall_interactions,guest_list,barcode_dict,'fall_interactions_by_ap_cleaned.csv',non_students_list)\n",
    "#print('halfway')\n",
    "#make_by_ap_cleaned_interaction_lists(spring_interactions,guest_list,barcode_dict,'spring_interactions_by_ap_cleaned.csv',non_students_list)\n",
    "#print('just reading')\n",
    "fall_interactions_by_ap = pd.read_csv('fall_interactions_by_ap_cleaned.csv')\n",
    "spring_interactions_by_ap = pd.read_csv('spring_interactions_by_ap_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove guests and stagant devices from spring and fall full un-duped interactions files\n",
    "fall_interactions_cleaned = fall_interactions_full[~fall_interactions_full['user_1'].isin(guest_list)]\n",
    "fall_interactions_cleaned = fall_interactions_cleaned[~fall_interactions_cleaned['user_2'].isin(guest_list)]\n",
    "fall_interactions_cleaned = fall_interactions_cleaned[~fall_interactions_cleaned['user_1'].isin(non_students_list)]\n",
    "fall_interactions_cleaned = fall_interactions_cleaned[~fall_interactions_cleaned['user_2'].isin(non_students_list)]\n",
    "\n",
    "spring_interactions_cleaned = spring_interactions_full[~spring_interactions_full['user_1'].isin(guest_list)]\n",
    "spring_interactions_cleaned = spring_interactions_cleaned[~spring_interactions_cleaned['user_2'].isin(guest_list)]\n",
    "spring_interactions_cleaned = spring_interactions_cleaned[~spring_interactions_cleaned['user_1'].isin(non_students_list)]\n",
    "spring_interactions_cleaned = spring_interactions_cleaned[~spring_interactions_cleaned['user_2'].isin(non_students_list)]\n",
    "\n",
    "print(len(fall_interactions_cleaned),len(fall_interactions_full))\n",
    "print(len(spring_interactions_cleaned),len(spring_interactions_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('metadata_all.csv')\n",
    "sequenced = metadata[~metadata['Sequence_ID'].isnull()]\n",
    "peacock = metadata[metadata['pango_lineage'] == 'B.1.429.1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fall:\n",
    "fall_interactions_cleaned_barcodes_user1s = list(fall_interactions_cleaned['user_1'].values)\n",
    "fall_interactions_cleaned_barcodes_user2s = list(fall_interactions_cleaned['user_2'].values)\n",
    "fall_interactions_cleaned_barcodes = list(set(fall_interactions_cleaned_barcodes_user1s + fall_interactions_cleaned_barcodes_user2s))\n",
    "fall_interactions_cleaned_barcodes = [x for x in fall_interactions_cleaned_barcodes if pd.isna(x) == False]\n",
    "\n",
    "#spring:\n",
    "spring_interactions_cleaned_barcodes_user1s = list(spring_interactions_cleaned['user_1'].values)\n",
    "spring_interactions_cleaned_barcodes_user2s = list(spring_interactions_cleaned['user_2'].values)\n",
    "spring_interactions_cleaned_barcodes = list(set(spring_interactions_cleaned_barcodes_user1s + spring_interactions_cleaned_barcodes_user2s))\n",
    "spring_interactions_cleaned_barcodes = [x for x in spring_interactions_cleaned_barcodes if pd.isna(x) == False]\n",
    "\n",
    "sequenced_barcodes = sequenced.dropna(subset = ['barcode'])['barcode'].values\n",
    "positive_barcodes = metadata.dropna(subset = ['barcode'])['barcode'].values\n",
    "\n",
    "metadata_of_positives = metadata.dropna(subset = ['Class_Year'])\n",
    "fall_metadata_of_positives = metadata_of_positives[metadata_of_positives['Test_day'] <= '2020-11-20']\n",
    "fall_metadata_of_positives = fall_metadata_of_positives[fall_metadata_of_positives['Test_day'] >= '2020-08-17'].reset_index(drop = True)\n",
    "fall_metadata_of_positives_barcodes = fall_metadata_of_positives.dropna(subset = ['barcode'])['barcode'].values\n",
    "\n",
    "spring_metadata_of_positives = metadata_of_positives[metadata_of_positives['Test_day'] <= '2021-04-30']\n",
    "spring_metadata_of_positives = spring_metadata_of_positives[spring_metadata_of_positives['Test_day'] >= '2021-01-18'].reset_index(drop = True)\n",
    "spring_metadata_of_positives_barcodes = spring_metadata_of_positives.dropna(subset = ['barcode'])['barcode'].values\n",
    "\n",
    "sequenced_with_classyear = sequenced.dropna(subset = ['Class_Year'])\n",
    "sequenced_with_classyear = sequenced_with_classyear[sequenced_with_classyear['Class_Year'] != 'Year E']\n",
    "\n",
    "fall_sequenced = sequenced_with_classyear[sequenced_with_classyear['Test Date'] <= '2020-11-20']\n",
    "fall_sequenced = fall_sequenced[fall_sequenced['Test Date'] >= '2020-08-17'].reset_index(drop = True)\n",
    "fall_sequenced_barcodes = fall_sequenced.dropna(subset = ['barcode'])['barcode'].values\n",
    "\n",
    "spring_sequenced = sequenced_with_classyear[sequenced_with_classyear['Test Date'] <= '2021-04-30']\n",
    "spring_sequenced = spring_sequenced[spring_sequenced['Test Date'] >= '2021-01-18'].reset_index(drop = True)\n",
    "spring_sequenced_barcodes = spring_sequenced.dropna(subset = ['barcode'])['barcode'].values\n",
    "\n",
    "fall_wifi_count_positives = []\n",
    "fall_wifi_count_sequenced = []\n",
    "fall_wifi_count_pos_and_seq = []\n",
    "\n",
    "for user in fall_interactions_cleaned_barcodes:\n",
    "    if (user in fall_metadata_of_positives_barcodes) and (user in fall_sequenced_barcodes):\n",
    "        fall_wifi_count_pos_and_seq.append(user)\n",
    "    if user in fall_sequenced_barcodes:\n",
    "        fall_wifi_count_sequenced.append(user)\n",
    "    if user in fall_metadata_of_positives_barcodes:\n",
    "        fall_wifi_count_positives.append(user)\n",
    "\n",
    "\n",
    "print('all fall seq regardless of wifi', len(fall_sequenced_barcodes))\n",
    "print('all fall pos regardless of wifi', len(fall_metadata_of_positives_barcodes))\n",
    "print('fall pos', len(fall_wifi_count_positives))\n",
    "print('fall seq', len(fall_wifi_count_sequenced))\n",
    "print('fall pos and seq', len(fall_wifi_count_pos_and_seq))\n",
    "print()\n",
    "print()\n",
    "\n",
    "spring_wifi_count_positives = []\n",
    "spring_wifi_count_sequenced = []\n",
    "spring_wifi_count_pos_and_seq = []\n",
    "\n",
    "for user in spring_interactions_cleaned_barcodes:\n",
    "    if (user in spring_metadata_of_positives_barcodes) and (user in spring_sequenced_barcodes):\n",
    "        spring_wifi_count_pos_and_seq.append(user)\n",
    "    if user in spring_sequenced_barcodes:\n",
    "        spring_wifi_count_sequenced.append(user)\n",
    "    if user in spring_metadata_of_positives_barcodes:\n",
    "        spring_wifi_count_positives.append(user)\n",
    "\n",
    "\n",
    "print('all spring seq regardless of wifi', len(spring_sequenced_barcodes))\n",
    "print('all spring pos regardless of wifi', len(spring_metadata_of_positives_barcodes))\n",
    "print('spring pos', len(spring_wifi_count_positives))\n",
    "print('spring seq', len(spring_wifi_count_sequenced))\n",
    "print('spring pos and seq', len(spring_wifi_count_pos_and_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dedup Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_sem_no_solos(semester_interactions_cleaned):\n",
    "    interactions_no_solos = semester_interactions_cleaned.dropna().reset_index()\n",
    "    interactions_no_solos['pair'] = '[\\\"' + interactions_no_solos['user_1']+'\\\",\\\"'+interactions_no_solos['user_2'] +'\\\"]'\n",
    "    interactions_no_solos['pair'] = [json.loads(x) for x in interactions_no_solos['pair']]\n",
    "    interactions_no_solos['pair'] = [str(sorted(x)) for x in interactions_no_solos['pair']]\n",
    "    interactions_no_solos = interactions_no_solos.reset_index(drop = True)\n",
    "    interactions_no_solos = interactions_no_solos[interactions_no_solos['user_1'] != interactions_no_solos['user_2']]\n",
    "    grouped_by_pair = interactions_no_solos.groupby(['day','pair','user_1']).agg({'duration':['sum','median','count']}).reset_index()\n",
    "    grouped_by_pair.columns = ['day','pair','user1','duration_sum','duration_median','int_count']\n",
    "    return grouped_by_pair,interactions_no_solos\n",
    "\n",
    "def run_dedup_and_clean_fall(grouped_by_pair_fall,fall_interactions_no_solos):\n",
    "    #producing only pairs to see if we have two for each pair (what we would expect)\n",
    "    pair_counts = grouped_by_pair_fall.groupby(['day','pair']).size().reset_index()\n",
    "\n",
    "    #finding any pairs that don't have 2 interactions when all interaction info is summed\n",
    "    pair_counts.columns = ['day','pair','count']\n",
    "    faulty_pair_counts = pair_counts[pair_counts['count'] != 2]\n",
    "\n",
    "    new_holder_cleaned = grouped_by_pair_fall\n",
    "\n",
    "    #spliting into two lists that create pairwise comparisons of the same pairings\n",
    "    new_holder1 = new_holder_cleaned[::2]\n",
    "    new_holder2 =  new_holder_cleaned[1::2]\n",
    "\n",
    "    #checking pairwise matching after clean of wonky 1-ways\n",
    "    vals_h1 = list(new_holder1['pair'].values)\n",
    "    vals_h2 = list(new_holder2['pair'].values)\n",
    "    print(vals_h1 == vals_h2)\n",
    "\n",
    "    new_rows_deduped = []\n",
    "    for x in range(len(new_holder1)):\n",
    "        if new_holder1['int_count'].iloc[x] != new_holder2['int_count'].iloc[x]:\n",
    "            if new_holder1['int_count'].iloc[x] > new_holder2['int_count'].iloc[x]:\n",
    "                file = new_holder1.iloc[x]\n",
    "            else : #new_holder1.iloc['int_count'][x] < new_holder2.iloc['int_count'][x]\n",
    "                file = new_holder2.iloc[x]\n",
    "        else: #new_holder1.iloc['int_count'][x] == new_holder2.iloc['int_count']\n",
    "            file = new_holder1.iloc[x]\n",
    "        date = file['day']\n",
    "        pair = file['pair']\n",
    "        sum = file['duration_sum']\n",
    "        median = file['duration_median']\n",
    "        count = file['int_count']\n",
    "        to_add = [date,pair,sum,median,count]\n",
    "        new_rows_deduped.append(to_add)\n",
    "    final_semester_deduped = pd.DataFrame(data = new_rows_deduped,columns = ['day','pair','sum','median','count'])\n",
    "\n",
    "    #checking to make sure half were removed\n",
    "    print(len(final_semester_deduped))\n",
    "    print(len(new_holder_cleaned))\n",
    "    print(len(new_holder_cleaned)/2)\n",
    "\n",
    "    return final_semester_deduped\n",
    "\n",
    "def run_dedup_and_clean_spring(grouped_by_pair_spring,spring_interactions_no_solos):\n",
    "    #producing only pairs to see if we have two for each pair (what we would expect)\n",
    "    pair_counts = grouped_by_pair_spring.groupby(['day','pair']).size().reset_index()\n",
    "\n",
    "    #finding any pairs that don't have 2 interactions when all interaction info is summed\n",
    "    pair_counts.columns = ['day','pair','count']\n",
    "\n",
    "    new_holder_cleaned = grouped_by_pair_spring\n",
    "\n",
    "    #spliting into two lists that create pairwise comparisons of the same pairings\n",
    "    new_holder1 = new_holder_cleaned[::2]\n",
    "    new_holder2 =  new_holder_cleaned[1::2]\n",
    "\n",
    "    #checking pairwise matching after clean of wonky 1-ways\n",
    "    vals_h1 = list(new_holder1['pair'].values)\n",
    "    vals_h2 = list(new_holder2['pair'].values)\n",
    "    print(vals_h1 == vals_h2)\n",
    "\n",
    "    new_rows_deduped = []\n",
    "    for x in range(len(new_holder1)):\n",
    "        if new_holder1['int_count'].iloc[x] != new_holder2['int_count'].iloc[x]:\n",
    "            if new_holder1['int_count'].iloc[x] > new_holder2['int_count'].iloc[x]:\n",
    "                file = new_holder1.iloc[x]\n",
    "            else : #new_holder1.iloc['int_count'][x] < new_holder2.iloc['int_count'][x]\n",
    "                file = new_holder2.iloc[x]\n",
    "        else: #new_holder1.iloc['int_count'][x] == new_holder2.iloc['int_count']\n",
    "            file = new_holder1.iloc[x]\n",
    "        date = file['day']\n",
    "        pair = file['pair']\n",
    "        sum = file['duration_sum']\n",
    "        median = file['duration_median']\n",
    "        count = file['int_count']\n",
    "        to_add = [date,pair,sum,median,count]\n",
    "        new_rows_deduped.append(to_add)\n",
    "    final_semester_deduped = pd.DataFrame(data = new_rows_deduped,columns = ['day','pair','sum','median','count'])\n",
    "\n",
    "    #checking to make sure half were removed\n",
    "    print(len(final_semester_deduped))\n",
    "    print(len(new_holder_cleaned))\n",
    "    print(len(new_holder_cleaned)/2)\n",
    "    return final_semester_deduped\n",
    "    \n",
    "def create_by_date_pos_list(semester_final,barcode_to_test_date,window_size):\n",
    "    barcode_to_test_date_dict = {}\n",
    "    by_date_pos_list = {}\n",
    "    days = list(set(semester_final['day']))\n",
    "    positives = list(set(barcode_to_test_date['barcode']))\n",
    "    for row,index in barcode_to_test_date.iterrows():\n",
    "        barcode_to_test_date_dict[index['barcode']] = index['Test Date']\n",
    "    for day in days:\n",
    "        by_date_pos_list[day] = []\n",
    "        for positive in positives:\n",
    "            test_date = datetime.fromisoformat(barcode_to_test_date_dict[positive])\n",
    "            current_date = datetime.fromisoformat(day)\n",
    "            diff = (test_date-current_date).days\n",
    "            if diff <=window_size and diff >= 0:\n",
    "                by_date_pos_list[day].append(positive)\n",
    "    return by_date_pos_list, positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barcode_to_test_date = metadata[['barcode','Test Date']]\n",
    "barcode_to_symptom_onset = metadata[['barcode','Symptom Onset']] #not currently used\n",
    "\n",
    "barcode_to_test_date_dict = pd.Series(barcode_to_test_date['Test Date'].values,index = barcode_to_test_date['barcode']).to_dict()\n",
    "barcode_to_symptom_onset_dict = pd.Series(barcode_to_symptom_onset['Symptom Onset'].values,index = barcode_to_symptom_onset['barcode']).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouped_by_pair_fall,fall_interactions_no_solos = group_sem_no_solos(fall_interactions_cleaned)\n",
    "#final_fall = run_dedup_and_clean_fall(grouped_by_pair_fall,fall_interactions_no_solos)\n",
    "#final_fall['pair'] = [x.replace('\\'','') for x in final_fall['pair']]\n",
    "#final_fall['pair'] = [x.strip('][').split(', ') for x in final_fall['pair']]\n",
    "#final_fall['user1'] = [x[0] for x in final_fall['pair']]\n",
    "#final_fall['user2'] = [x[1] for x in final_fall['pair']]\n",
    "#final_fall = final_fall.drop(columns = 'pair')\n",
    "#final_fall = final_fall.reindex(columns = ['day','user1','user2','sum','median','count'])\n",
    "#final_fall.to_csv('final_fall.csv',index = False)\n",
    "\n",
    "final_fall = pd.read_csv('final_fall.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouped_by_pair_spring,spring_interactions_no_solos = group_sem_no_solos(spring_interactions_cleaned)\n",
    "#final_spring = run_dedup_and_clean_spring(grouped_by_pair_spring,spring_interactions_no_solos)\n",
    "#final_spring['pair'] = [x.replace('\\'','') for x in final_spring['pair']]\n",
    "#final_spring['pair'] = [x.strip('][').split(', ') for x in final_spring['pair']]\n",
    "#final_spring['user1'] = [x[0] for x in final_spring['pair']]\n",
    "#final_spring['user2'] = [x[1] for x in final_spring['pair']]\n",
    "#final_spring = final_spring.drop(columns = 'pair')\n",
    "#final_spring = final_spring.reindex(columns = ['day','user1','user2','sum','median','count'])\n",
    "#final_spring.to_csv('final_spring.csv',index = False)\n",
    "\n",
    "\n",
    "final_spring = pd.read_csv('final_spring.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager as fm\n",
    "fe = fm.FontEntry(\n",
    "    fname='C:/Users/brync/anaconda3/Lib/site-packages/matplotlib/mpl-data/fonts/ttf/Montserrat.ttf',\n",
    "    name='Montserrat')\n",
    "fm.fontManager.ttflist.insert(0, fe)\n",
    "mpl.rcParams['font.family'] = fe.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counts for Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_date_pos_list_fall,positives = create_by_date_pos_list(final_fall,barcode_to_test_date,10)\n",
    "by_date_pos_list_spring,positives = create_by_date_pos_list(final_spring,barcode_to_test_date,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fall = final_fall.groupby(['day'])\n",
    "final_fall = [group for x, group in final_fall]\n",
    "final_spring = final_spring.groupby(['day'])\n",
    "final_spring = [group for x, group in final_spring]\n",
    "\n",
    "fall_interactions_cleaned = fall_interactions_cleaned.groupby(['day'])\n",
    "fall_interactions_cleaned = [group for x, group in fall_interactions_cleaned]\n",
    "spring_interactions_cleaned = spring_interactions_cleaned.groupby(['day'])\n",
    "spring_interactions_cleaned = [group for x, group in spring_interactions_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_network_to_make_groups(interactions_by_pairing):\n",
    "    days = []\n",
    "    node_count = []\n",
    "    edge_count = []\n",
    "    node_list = {}\n",
    "    for x in interactions_by_pairing:\n",
    "        x.columns = ['day','user_1','user_2','sum','median','count']\n",
    "        day = str(x['day'].iloc[0])\n",
    "        days.append(day)\n",
    "        g = nx.from_pandas_edgelist(x,'user_1','user_2', ['sum','median','count'], create_using = nx.Graph())\n",
    "        for node in g:\n",
    "            if node in node_list:\n",
    "                node_list[node][1].append(1)\n",
    "            else:\n",
    "                node_list[node] = [[day],[1]]\n",
    "        num_nodes = g.number_of_nodes()\n",
    "        num_edges = g.number_of_edges()\n",
    "\n",
    "        node_count.append(num_nodes)\n",
    "        edge_count.append(num_edges)\n",
    "    return node_list,node_count,edge_count,days\n",
    "\n",
    "def run_network_to_make_groups_largerfile(interactions_by_pairing):\n",
    "    days = []\n",
    "    node_count = []\n",
    "    edge_count = []\n",
    "    node_list = {}\n",
    "    for x in interactions_by_pairing:\n",
    "        day = str(x['day'].iloc[0])\n",
    "        days.append(day)\n",
    "        g = nx.from_pandas_edgelist(x,'user_1','user_2', ['duration'], create_using = nx.Graph())\n",
    "        for node in g:\n",
    "            if node in node_list:\n",
    "                node_list[node][1].append(1)\n",
    "            else:\n",
    "                node_list[node] = [[day],[1]]\n",
    "        num_nodes = g.number_of_nodes()\n",
    "        num_edges = g.number_of_edges()\n",
    "\n",
    "        node_count.append(num_nodes)\n",
    "        edge_count.append(num_edges)\n",
    "    return node_list,node_count,edge_count,days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fall_node_list, fall_node_count, fall_edge_count, days_fall = run_network_to_make_groups(final_fall)\n",
    "spring_node_list, spring_node_count, spring_edge_count, days_spring = run_network_to_make_groups(final_spring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_by_date_pos_list_without10days(by_date_pos_list,sem_node_list,positives,barcode_to_test_date_dict):\n",
    "    sem_remove = []\n",
    "    for x in sem_node_list:\n",
    "        if x in positives:\n",
    "            first = datetime.fromisoformat(sem_node_list[x][0][0])\n",
    "            second = datetime.strptime(barcode_to_test_date_dict[x],\"%Y-%m-%d\")\n",
    "            diff = (first-second).days\n",
    "            if (diff < 1 and diff > -10):\n",
    "                sem_remove.append(x) \n",
    "\n",
    "    by_date_pos_list_dict_new = {}\n",
    "    for x in by_date_pos_list:\n",
    "        new_list = []\n",
    "        for code in by_date_pos_list[x]:\n",
    "            if code not in sem_remove:\n",
    "                new_list.append(code)\n",
    "        by_date_pos_list_dict_new[x] = new_list\n",
    "    \n",
    "    return by_date_pos_list_dict_new\n",
    "\n",
    "def create_numvisits_per_node_chart_sem1(node_list_sem1,node_list_sem2):\n",
    "    \n",
    "    values = []\n",
    "    for x in node_list_sem1.values():\n",
    "        values.append(x[1])\n",
    "    sum_visits_sem1 = []\n",
    "    for x in values:\n",
    "        sum_visits_sem1.append(sum(x))\n",
    "        \n",
    "    values = []\n",
    "    for x in node_list_sem2.values():\n",
    "        values.append(x[1])\n",
    "    sum_visits_sem2 = []\n",
    "    for x in values:\n",
    "        sum_visits_sem2.append(sum(x))\n",
    "    \n",
    "    plt.figure(figsize = (10,8))\n",
    "    \n",
    "    plt.hist(sum_visits_sem1,bins = range(100),alpha = .6,color = color_pallet[3],label = 'Fall 2020')\n",
    "    plt.hist(sum_visits_sem2,bins = range(100),alpha = .6, color = color_pallet[5], label = 'Spring 2021')\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.xlabel('Number of Days on Campus During Semester',fontsize = 16)\n",
    "    plt.ylabel('Number of Users',fontsize = 16)\n",
    "    plt.legend(loc = 'best', fontsize = 16,frameon = True)\n",
    "\n",
    "    image_format = 'svg' # e.g .png, .svg, etc.\n",
    "    image_name = 'Supp5B.svg'\n",
    "\n",
    "    plt.savefig(image_name, format=image_format, dpi=1200)\n",
    "    plt.show()\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fall_interactions_full = fall_interactions_full.groupby(['day'])\n",
    "fall_interactions_full = [group for x, group in fall_interactions_full]\n",
    "spring_interactions_full = spring_interactions_full.groupby(['day'])\n",
    "spring_interactions_full = [group for x, group in spring_interactions_full]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_date_pos_list_fall_dict = create_by_date_pos_list_without10days(by_date_pos_list_fall,fall_node_list,positives,barcode_to_test_date_dict)  \n",
    "by_date_pos_list_spring_dict = create_by_date_pos_list_without10days(by_date_pos_list_spring,spring_node_list,positives,barcode_to_test_date_dict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peacock_list = list(peacock['barcode'])\n",
    "sequenced_list = list(sequenced['barcode'])\n",
    "positiveIDs = list(metadata['barcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_year_positives = pd.DataFrame(columns = ['day','p1','p2','day sum duration','day median duration','day interaction count'])\n",
    "pos_IDs = positiveIDs\n",
    "print('running')\n",
    "\n",
    "for j in final_fall:\n",
    "    j.columns = ['day','s1','s2','sum','median','count']\n",
    "    day = j['day'].iloc[0]\n",
    "    print(day)\n",
    "    g = nx.from_pandas_edgelist(j,'s1','s2',['sum','median','count'], create_using = nx.Graph())\n",
    "    if str(day) in by_date_pos_list_fall_dict:\n",
    "        rolling_positives = by_date_pos_list_fall_dict[str(day)]\n",
    "    elif str(day) not in by_date_pos_list_fall_dict:\n",
    "        by_date_pos_list_fall_dict[str(day)] = []\n",
    "    \n",
    "    rolling_positives = by_date_pos_list_fall_dict[str(day)]\n",
    "    edges = list(g.edges(data = True))\n",
    "    for x in edges:\n",
    "        if (str(x[0]) in rolling_positives) and (str(x[0]) in pos_IDs):\n",
    "            if (str(x[1]) in rolling_positives) and (str(x[1]) in pos_IDs):\n",
    "                all_year_positives.loc[len(all_year_positives.index)] = [day,x[0],x[1],x[2]['sum'],x[2]['median'],x[2]['count']]\n",
    "\n",
    "print('running2')\n",
    "\n",
    "for j in final_spring:\n",
    "    j.columns = ['day','s1','s2','sum','median','count']\n",
    "    day = j['day'].iloc[0]\n",
    "    print(day)\n",
    "    g = nx.from_pandas_edgelist(j,'s1','s2',['sum','median','count'], create_using = nx.Graph())\n",
    "    if str(day) in by_date_pos_list_spring_dict:\n",
    "        rolling_positives = by_date_pos_list_spring_dict[str(day)]\n",
    "    elif str(day) not in by_date_pos_list_spring_dict:\n",
    "        by_date_pos_list_spring_dict[str(day)] = []\n",
    "    \n",
    "    rolling_positives = by_date_pos_list_spring_dict[str(day)]\n",
    "    edges = list(g.edges(data = True))\n",
    "    for x in edges:\n",
    "        if (str(x[0]) in rolling_positives) and (str(x[0]) in pos_IDs):\n",
    "            if (str(x[1]) in rolling_positives) and (str(x[1]) in pos_IDs):\n",
    "                all_year_positives.loc[len(all_year_positives.index)] = [day,x[0],x[1],x[2]['sum'],x[2]['median'],x[2]['count']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_year_pos_aggregated = all_year_positives.groupby(['p1','p2']).agg({'day sum duration':'sum','day median duration':'median','day interaction count':['sum','size']}).reset_index()\n",
    "new_columns = [''.join(t) for t in all_year_pos_aggregated.columns]\n",
    "all_year_pos_aggregated.columns = new_columns\n",
    "all_year_pos_aggregated.columns = ['p1','p2','total interactions','median duration median','sum time spent','total days interacted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_year_pos_aggregated['inverse total interaction time'] = 1/all_year_pos_aggregated['sum time spent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_year_pos_aggregated.to_csv('all_year_pos_aggregated_refreshed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_year_pos_aggregated = all_year_pos_aggregated[all_year_pos_aggregated['p1'].isin(metadata['barcode'].values)]\n",
    "all_year_pos_aggregated = all_year_pos_aggregated[all_year_pos_aggregated['p2'].isin(metadata['barcode'].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_all_graph = nx.from_pandas_edgelist(all_year_pos_aggregated,'p1','p2',['sum time spent','median duration median', 'total interactions', 'total days interacted','inverse total interaction time'],create_using = nx.Graph())\n",
    "weight = nx.get_edge_attributes(all_all_graph,'inverse total interaction time')\n",
    "weight = list(weight.values())\n",
    "weight = [x/(max(weight))*2 for x in weight]\n",
    "#weight = .5\n",
    "plt.figure(figsize = (20,20))\n",
    "color_map = []\n",
    "node_color_dict = {}\n",
    "peacock_nodes = []\n",
    "non_peacock_nodes = []\n",
    "for node in all_all_graph:\n",
    "    if node in peacock_list:\n",
    "        node_color_dict[node] = 'orange'\n",
    "        color_map.append('orange')\n",
    "        peacock_nodes.append(node)\n",
    "    elif node in sequenced_list:\n",
    "        node_color_dict[node] = 'skyblue'\n",
    "        color_map.append('skyblue')\n",
    "        non_peacock_nodes.append(node)\n",
    "    else: \n",
    "        node_color_dict[node] = 'black'\n",
    "        color_map.append('black')   \n",
    "\n",
    "nodes_to_hold = all_all_graph.nodes\n",
    "\n",
    "nx.set_node_attributes(all_all_graph,node_color_dict,'sequence_status')\n",
    "\n",
    "\n",
    "node_list_dict_peacock = {}\n",
    "node_list_dict_nonpeacock = {}\n",
    "\n",
    "peacock_dist = []\n",
    "non_peacock_dist = []\n",
    "for node in all_all_graph:\n",
    "    for node2 in all_all_graph:\n",
    "        if node != node2:\n",
    "            if nx.has_path(all_all_graph,node,node2):\n",
    "                if (str([node,node2]) not in node_list_dict_peacock) and (str([node2,node]) not in node_list_dict_peacock) and (str([node,node2]) not in node_list_dict_nonpeacock) and (str([node2,node]) not in node_list_dict_nonpeacock):\n",
    "                    if node in peacock_list and node2 in peacock_list:\n",
    "                        node_list_dict_peacock[str([node,node2])] = nx.shortest_path_length(all_all_graph,node,node2)\n",
    "                    elif (node not in peacock_list and node2 in peacock_list) or (node in peacock_list and node2 not in peacock_list):\n",
    "                        node_list_dict_nonpeacock[str([node,node2])] = nx.shortest_path_length(all_all_graph,node,node2)\n",
    "\n",
    "print(len(node_list_dict_peacock))\n",
    "print(len(node_list_dict_nonpeacock))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nx.draw(all_all_graph,pos =nx.spring_layout(all_all_graph,seed = 4,k=0.089, iterations=45),width = weight,with_labels = False,node_size = 100,alpha = .6,linewidths = .9,edge_color = 'grey',node_color = color_map,font_size = 14)\n",
    "#nx.draw(all_all_graph,pos =nx.kamada_kawai_layout(all_all_graph,scale = 4),width = weight,with_labels = False,node_size = 100,alpha = .6,linewidths = .9,edge_color = 'royalblue',node_color = color_map,font_size = 14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "barcode_2_lineage = {}\n",
    "for x in range(len(sequenced['barcode'])):\n",
    "    barcode_2_lineage[sequenced['barcode'].iloc[x]] = sequenced['pango_lineage'].iloc[x]\n",
    "    \n",
    "{key:val for key, val in barcode_2_lineage.items() if val != None}\n",
    "{key:val for key, val in barcode_2_lineage.items() if val != np.nan}\n",
    "\n",
    "for node in all_all_graph:\n",
    "    if node not in barcode_2_lineage:\n",
    "        barcode_2_lineage[node] = 'NOT SEQUENCED'\n",
    "\n",
    "\n",
    "nx.set_node_attributes(all_all_graph,barcode_2_lineage,'sequence_lineage')\n",
    "pos_attribute_assort_coef = assortativity.attribute_assortativity_coefficient(all_all_graph, 'sequence_lineage')\n",
    "print(pos_attribute_assort_coef)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (20,20))\n",
    "nx.draw(all_all_graph,pos =nx.spring_layout(all_all_graph,seed = 27),with_labels = False,width = 5,node_size = 100,alpha = .6,linewidths = .9,edge_color = 'grey',node_color = color_map,font_size = 14)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(all_all_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lineage_names = nx.get_node_attributes(all_all_graph, \"sequence_lineage\")\n",
    "new_lineage_names = {key:str(value).replace('nan','None').replace('NOT SEQUENCED','None') for (key,value) in new_lineage_names.items()}\n",
    "nx.set_node_attributes(all_all_graph, new_lineage_names, name=\"sequence_lineage\")\n",
    "graph_attributes = list(set(new_lineage_names.values()))\n",
    "\n",
    "#[[PL],[MinPL,MaxPl,MedPL] [C, S,KT,KA] \n",
    "# KT = percent total, KA = percent found attributes, S = structure size, c = count, PL = path length\n",
    "all_node_details = []\n",
    "num_levels = 3\n",
    "total_in_structure = 0\n",
    "for node in all_all_graph.nodes(data = True):\n",
    "    node_details = {node[0]:{i:[[],[],[],[]] for i in graph_attributes}}\n",
    "    node_details[node[0]][node[1]['sequence_lineage']][0].append(0)\n",
    "    actual_lineage = node[1]['sequence_lineage']\n",
    "    for counter in range(1,num_levels+1):\n",
    "        level = nx.descendants_at_distance(all_all_graph,node[0],counter)\n",
    "        total_in_structure += len(level)+1\n",
    "        for secondary_node in all_all_graph.nodes(data = True):\n",
    "            if secondary_node[0] in level:\n",
    "                node_details[node[0]][secondary_node[1]['sequence_lineage']][0].append(counter)\n",
    "    for attribute in node_details[node[0]]:\n",
    "        node_details[node[0]][attribute][1]= [min(node_details[node[0]][attribute][0],default = np.nan),max(node_details[node[0]][attribute][0],default = np.nan)]\n",
    "        if len(node_details[node[0]][attribute][0]) == 0:\n",
    "            node_details[node[0]][attribute][1].append(0)\n",
    "        else:\n",
    "            node_details[node[0]][attribute][1].append(float(sum(node_details[node[0]][attribute][0])/len(node_details[node[0]][attribute][0])))\n",
    "        node_details[node[0]][attribute][2].append(len(node_details[node[0]][attribute][0]))\n",
    "        node_details[node[0]][attribute][2].append(total_in_structure)\n",
    "        node_details[node[0]][attribute][2].append(abs((len(node_details[node[0]][attribute][0]))/(total_in_structure)))\n",
    "        if (total_in_structure - len(node_details[node[0]]['None'][0])) != 0:\n",
    "            node_details[node[0]][attribute][2].append(abs(len(node_details[node[0]][attribute][0])/((total_in_structure) - len(node_details[node[0]]['None'][0]))))\n",
    "        else:\n",
    "            node_details[node[0]][attribute][2].append(0.0)\n",
    "        node_details[node[0]][attribute][3].append(actual_lineage)\n",
    "    node_details[node[0]]['None'][2][3]=0.0\n",
    "    all_node_details.append(node_details)\n",
    "\n",
    "\n",
    "\n",
    "new_frames = []\n",
    "for node_details in all_node_details:\n",
    "    for x in node_details:\n",
    "        for y in list(node_details[x]):\n",
    "            node = x\n",
    "            lineage = y\n",
    "            path_dists = node_details[x][y][0]\n",
    "            min_path_dist = node_details[x][y][1][0]\n",
    "            max_path_dist = node_details[x][y][1][1]\n",
    "            median_path_dist = node_details[x][y][1][2]\n",
    "            saturation = node_details[x][y][2][0]\n",
    "            structure_size = node_details[x][y][2][1]\n",
    "            percent_total = node_details[x][y][2][2]\n",
    "            percent_known = node_details[x][y][2][3]\n",
    "            actual_lineage = node_details[x][y][3][0]\n",
    "            new_frames.append([node,lineage,path_dists,min_path_dist,max_path_dist,median_path_dist,saturation,structure_size,percent_total,percent_known,actual_lineage])\n",
    "        \n",
    "decision_frame = pd.DataFrame(data = new_frames, columns = ['node','lineage','path dists','min path dist','max path dist','avg path dist','saturation','structure size','percent total','percent known','actual_lineage'])\n",
    "\n",
    "print(decision_frame)\n",
    "decision_frames_by_node = decision_frame.groupby(['node'])\n",
    "new_decision_frames = []\n",
    "for x in decision_frames_by_node:\n",
    "    decision_list = x[1]\n",
    "    new_decision_frames.append([decision_list.dropna()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in new_decision_frames:\n",
    "    dataframe = x[0]\n",
    "    dataframe = dataframe.reset_index(drop = True)\n",
    "    dataframe['lineage presence on campus'] = 'to be calculated'\n",
    "    dataframe['level considered to'] = num_levels\n",
    "    dataframe['Test Date'] = barcode_to_test_date_dict[dataframe['node'][0]]\n",
    "    dataframe['Symptom Onset'] = barcode_to_symptom_onset_dict[dataframe['node'][0]]\n",
    "    dataframe['Wastewater Presence Binary'] = 'to be calculated'\n",
    "    dataframe['Wastewater Presence Proportion'] = 'to be calculated'\n",
    "    print(dataframe)\n",
    "    break\n",
    "\n",
    "#to do, path dist with test date and symptom onset date for network structure\n",
    "#not strict neighborhood size, dynamic for each individual based on likelihood of transmission \n",
    "#   ie. (stop searching subnetwork if it is just not feasible anymore)\n",
    "#look at differences in time for each node in the subnetworks to define feasibility\n",
    "#   ie. this will avoid choosing depth and prepositive window statically\n",
    "#add mean field approx and metadata about node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mavilys = metadata[['barcode','Mavily']]\n",
    "mavilys = mavilys.dropna()\n",
    "mavilys_dict = dict(mavilys.values)\n",
    "\n",
    "sports = metadata[['barcode','Sports_team']]\n",
    "sports = sports.dropna()\n",
    "sports_dict = dict(sports.values)\n",
    "\n",
    "residency = metadata[['barcode','Lives_on_campus']]\n",
    "residency = residency.dropna()\n",
    "residency_dict = dict(residency.values)\n",
    "\n",
    "residence_hall = metadata[['barcode','Residence_hall']]\n",
    "residence_hall = residence_hall.dropna()\n",
    "residence_hall_dict = dict(residence_hall.values)\n",
    "\n",
    "gender = metadata[['barcode','Sex']]\n",
    "gender = gender.dropna()\n",
    "gender_dict = dict(gender.values)\n",
    "\n",
    "class_year = metadata[['barcode','Class_Year']].dropna()\n",
    "class_year_dict = dict(class_year.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_att_network_sem_test_rolling(by_date_all_interactions_bypairing,positives_list_rolling):\n",
    "    days = []\n",
    "    class_year = []\n",
    "    gender = []\n",
    "    residence_hall = []\n",
    "    residency = []\n",
    "    sports = []\n",
    "    mavilys = []\n",
    "    \n",
    "    for x in by_date_all_interactions_bypairing:\n",
    "        day = str(x['day'].iloc[0])\n",
    "        positives = positives_list_rolling[str(day)]\n",
    "        days.append(day)\n",
    "        x = x[x['s1'].isin(positives)]\n",
    "        x = x[x['s2'].isin(positives)]\n",
    "        g = nx.from_pandas_edgelist(x,'s1','s2',['sum','median', 'count'], create_using = nx.Graph())\n",
    "        for nodes in g:\n",
    "            if nodes in list(class_year_dict.keys()): \n",
    "                g.nodes[nodes]['class_year'] = class_year_dict[nodes]\n",
    "            else: \n",
    "                g.nodes[nodes]['class_year'] = 0\n",
    "            if nodes in list(gender_dict.keys()): \n",
    "                g.nodes[nodes]['gender'] = gender_dict[nodes]\n",
    "            else: \n",
    "                g.nodes[nodes]['gender'] = 0\n",
    "            if nodes in list(residence_hall_dict.keys()): \n",
    "                g.nodes[nodes]['residence_hall'] = residence_hall_dict[nodes]\n",
    "            else: \n",
    "                g.nodes[nodes]['residence_hall'] = 0\n",
    "            if nodes in list(residency_dict.keys()): \n",
    "                g.nodes[nodes]['residency'] = residency_dict[nodes]\n",
    "            else: \n",
    "                g.nodes[nodes]['residency'] = 0\n",
    "            if nodes in list(sports_dict.keys()): \n",
    "                g.nodes[nodes]['sports'] = sports_dict[nodes]\n",
    "            else: \n",
    "                g.nodes[nodes]['sports'] = 0\n",
    "            if nodes in list(mavilys_dict.keys()): \n",
    "                g.nodes[nodes]['mavilys'] = mavilys_dict[nodes]\n",
    "            else: \n",
    "                g.nodes[nodes]['mavilys'] = 0\n",
    "\n",
    "        class_year_attribute_assort_coef = assortativity.attribute_assortativity_coefficient(g, 'class_year')\n",
    "        gender_attribute_assort_coef = assortativity.attribute_assortativity_coefficient(g, 'gender')\n",
    "        res_hall_attribute_assort_coef = assortativity.attribute_assortativity_coefficient(g, 'residence_hall')\n",
    "        residency_attribute_assort_coef = assortativity.attribute_assortativity_coefficient(g, 'residency')\n",
    "        sports_attribute_assort_coef = assortativity.attribute_assortativity_coefficient(g, 'sports')\n",
    "        mavilys_attribute_assort_coef = assortativity.attribute_assortativity_coefficient(g, 'mavilys')\n",
    "        \n",
    "        class_year.append(class_year_attribute_assort_coef)\n",
    "        gender.append(gender_attribute_assort_coef)\n",
    "        residence_hall.append(res_hall_attribute_assort_coef)\n",
    "        residency.append(residency_attribute_assort_coef)\n",
    "        sports.append(sports_attribute_assort_coef)\n",
    "        mavilys.append(mavilys_attribute_assort_coef)\n",
    "      \n",
    "    return class_year,gender,residence_hall,residency,sports,mavilys,days\n",
    "\n",
    "def pos_att_network_sem_test(by_date_all_interactions_bypairing,positives_list):\n",
    "    days = []\n",
    "    class_year = []\n",
    "    gender = []\n",
    "    residence_hall = []\n",
    "    residency = []\n",
    "    sports = []\n",
    "    mavilys = []\n",
    "    \n",
    "    for x in by_date_all_interactions_bypairing:\n",
    "        day = str(x['day'].iloc[0])\n",
    "        positives = list(positives_list)\n",
    "        days.append(day)\n",
    "        x = x[x['s1'].isin(positives)]\n",
    "        x = x[x['s2'].isin(positives)]\n",
    "        g = nx.from_pandas_edgelist(x,'s1','s2',['sum','median', 'count'], create_using = nx.Graph())\n",
    "        for nodes in g:\n",
    "            if nodes in list(class_year_dict.keys()): \n",
    "                g.nodes[nodes]['class_year'] = class_year_dict[nodes]\n",
    "            else: \n",
    "                g.nodes[nodes]['class_year'] = 0\n",
    "            if nodes in list(gender_dict.keys()): \n",
    "                g.nodes[nodes]['gender'] = gender_dict[nodes]\n",
    "            else: \n",
    "                g.nodes[nodes]['gender'] = 0\n",
    "            if nodes in list(residence_hall_dict.keys()): \n",
    "                g.nodes[nodes]['residence_hall'] = residence_hall_dict[nodes]\n",
    "            else: \n",
    "                g.nodes[nodes]['residence_hall'] = 0\n",
    "            if nodes in list(residency_dict.keys()): \n",
    "                g.nodes[nodes]['residency'] = residency_dict[nodes]\n",
    "            else: \n",
    "                g.nodes[nodes]['residency'] = 0\n",
    "            if nodes in list(sports_dict.keys()): \n",
    "                g.nodes[nodes]['sports'] = sports_dict[nodes]\n",
    "            else: \n",
    "                g.nodes[nodes]['sports'] = 0\n",
    "            if nodes in list(mavilys_dict.keys()): \n",
    "                g.nodes[nodes]['mavilys'] = mavilys_dict[nodes]\n",
    "            else: \n",
    "                g.nodes[nodes]['mavilys'] = 0\n",
    "\n",
    "        class_year_attribute_assort_coef = assortativity.attribute_assortativity_coefficient(g, 'class_year')\n",
    "        gender_attribute_assort_coef = assortativity.attribute_assortativity_coefficient(g, 'gender')\n",
    "        res_hall_attribute_assort_coef = assortativity.attribute_assortativity_coefficient(g, 'residence_hall')\n",
    "        residency_attribute_assort_coef = assortativity.attribute_assortativity_coefficient(g, 'residency')\n",
    "        sports_attribute_assort_coef = assortativity.attribute_assortativity_coefficient(g, 'sports')\n",
    "        mavilys_attribute_assort_coef = assortativity.attribute_assortativity_coefficient(g, 'mavilys')\n",
    "        \n",
    "        class_year.append(class_year_attribute_assort_coef)\n",
    "        gender.append(gender_attribute_assort_coef)\n",
    "        residence_hall.append(res_hall_attribute_assort_coef)\n",
    "        residency.append(residency_attribute_assort_coef)\n",
    "        sports.append(sports_attribute_assort_coef)\n",
    "        mavilys.append(mavilys_attribute_assort_coef)\n",
    "      \n",
    "    return class_year,gender,residence_hall,residency,sports,mavilys,days\n",
    "\n",
    "def make_AA_charts(days,pos1,type_name1,pos2,type_name2,pos3,type_name3,pos4,type_name4,pos5,type_name5,pos6,type_name6):\n",
    "    formatted_dates = []\n",
    "    not_formatted_dates = []\n",
    "    for x in days:\n",
    "        new = datetime.strptime(x,'%Y-%m-%d')\n",
    "        not_formatted_dates.append(new)\n",
    "        new_time = datetime.strftime(new, '%b-%d')\n",
    "        formatted_dates.append(new_time)\n",
    "    \n",
    "    plt.figure(figsize = (19,10))\n",
    "    plt.plot(formatted_dates,pos1, color = 'blue',label = type_name1, linewidth = 3.5)\n",
    "    plt.plot(formatted_dates,pos2, color = 'orange',label = type_name2, linewidth = 3.5)\n",
    "    plt.plot(formatted_dates,pos3, color = 'seagreen',label = type_name3, linewidth = 3.5)\n",
    "    plt.plot(formatted_dates,pos4, color = 'mediumpurple',label = type_name4, linewidth = 3.5)\n",
    "    plt.plot(formatted_dates,pos5, color = 'yellowgreen',label = type_name5, linewidth = 3.5)\n",
    "    plt.plot(formatted_dates,pos6, color = 'salmon',label = type_name6, linewidth = 3.5)\n",
    "    plt.legend(loc = 'best',fontsize = 16)\n",
    "\n",
    "    plt.xticks(np.arange(0, len(formatted_dates), 7),fontsize = 20,rotation = 70)\n",
    "    plt.yticks(fontsize = 20)\n",
    "    plt.ylabel('Assortativity Coefficient',fontsize = 20)\n",
    "    plt.xlabel('Fall 2020',fontsize = 20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_year,gender,residence_hall,residency,sports,mavilys,days = pos_att_network_sem_test(final_fall,fall_metadata_of_positives_barcodes)\n",
    "make_AA_charts(days,class_year,'Class Year',gender,'Gender',residence_hall,'Residence Hall',residency,'On/Off Campus Residency',sports,'Primary Sports Team',mavilys,'Mavily')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_year,gender,residence_hall,residency,sports,mavilys,days = pos_att_network_sem_test_rolling(final_fall,by_date_pos_list_fall_dict)\n",
    "\n",
    "make_AA_charts(days,class_year,'Class Year',gender,'Gender',residence_hall,'Residence Hall',residency,'On/Off Campus Residency',sports,'Primary Sports Team',mavilys,'Mavily')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = pd.date_range(start='2020-08-17',end='2021-04-30')\n",
    "days = [str(x.strftime('%Y-%m-%d')) for x in days]\n",
    "\n",
    "for_prevalence = metadata[['pango_lineage','Test Date','Symptom Onset']]\n",
    "for_prevalence = for_prevalence.dropna(subset = ['pango_lineage'])\n",
    "for_prevalence = for_prevalence[for_prevalence['pango_lineage'] != 'None']\n",
    "for_prevalence['date'] = [for_prevalence['Symptom Onset'].iloc[x] if (for_prevalence['Symptom Onset'].iloc[x] != 'asymptomatic' and for_prevalence['Symptom Onset'].iloc[x] != 'unknown') else for_prevalence['Test Date'].iloc[x] for x in range(len(for_prevalence['Test Date']))]\n",
    "for_prevalence = for_prevalence.drop(columns = ['Test Date','Symptom Onset'])\n",
    "\n",
    "all_lineages = for_prevalence['pango_lineage'].unique()\n",
    "for_prevalence['count_of_sequenced'] = 1\n",
    "\n",
    "counting = for_prevalence.groupby(['date','pango_lineage']).count().reset_index(drop = False)\n",
    "\n",
    "days_holder = days.copy()\n",
    "new_vals = []\n",
    "for x in counting.groupby(['date']):\n",
    "    new_df = x[1]\n",
    "    holder_lineages = all_lineages\n",
    "    found_lineages = list(new_df['pango_lineage'].values)\n",
    "    needed_lineages = [x for x in holder_lineages if x not in found_lineages]\n",
    "    for x in needed_lineages:\n",
    "        new_row = [new_df['date'].iloc[0],x,0]\n",
    "        new_df.loc[len(new_df.index)] = new_row\n",
    "    string_v = new_df['date'].iloc[0]\n",
    "    if string_v in days_holder:\n",
    "        days_holder.remove(string_v)\n",
    "    new_vals.append(new_df)\n",
    "new_vals_full = pd.concat(new_vals)\n",
    "new_vals_full = new_vals_full.reset_index(drop = True)\n",
    "\n",
    "days_holder2 = days_holder\n",
    "while days_holder != []:\n",
    "    for x in days_holder:\n",
    "        holder_lineages = all_lineages.copy()\n",
    "        for y in holder_lineages:\n",
    "            new_row = [x,y,0]\n",
    "            new_vals_full.loc[len(new_vals_full.index)] = new_row\n",
    "        string_v = x\n",
    "        if string_v in days_holder:\n",
    "            days_holder.remove(string_v)\n",
    "            \n",
    "new_vals_full = new_vals_full.sort_values(by='date')\n",
    "new_vals_full['date'] = [datetime.strptime(x,'%Y-%m-%d') for x in new_vals_full['date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_be_added = []\n",
    "to_be_expanded = new_vals_full[new_vals_full['count_of_sequenced'] > 1].reset_index(drop = True)\n",
    "vals_full = new_vals_full[new_vals_full['count_of_sequenced']<=1].reset_index(drop = True)\n",
    "for index,row in to_be_expanded.iterrows():\n",
    "    for x in range(row[2]):\n",
    "        rows_to_be_added.append(list(row))\n",
    "\n",
    "for x in rows_to_be_added:\n",
    "    vals_full.loc[len(vals_full.index)] = x\n",
    "    \n",
    "vals_full = vals_full.drop(columns = ['count_of_sequenced'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_full = vals_full.sort_values(by = 'date')\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "sns.histplot(data = vals_full, x = 'date', hue = 'pango_lineage',common_norm = True,alpha = .8,kde = True,stat = 'probability',multiple = 'layer',shrink = .5,fill = False,palette = 'nipy_spectral')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "sns.kdeplot(data = vals_full, x = 'date', hue = 'pango_lineage',alpha = .1,multiple = 'layer',fill = False,palette = 'nipy_spectral',shade = True)\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(1234)  # Seed random generator\n",
    "dens = sm.nonparametric.KDEUnivariate(vals_full[vals_full['pango_lineage'] == 'B.1.2'])\n",
    "dens.fit()\n",
    "plt.plot(dens.cdf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_prevalence = for_prevalence.sort_values(by = 'date')\n",
    "#for_prevalence['date'] = [datetime.strptime(x,'%Y-%m-%d') for x in for_prevalence['date']]\n",
    "plt.figure(figsize=(20,15))\n",
    "sns.histplot(data = for_prevalence, x = 'date', hue = 'pango_lineage',alpha = .8,kde = True,stat = 'probability',multiple = 'layer',shrink = .5,fill = False,palette = 'nipy_spectral')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "sns.kdeplot(data = for_prevalence, x = 'date', hue = 'pango_lineage',alpha = .15,multiple = 'layer',common_norm = True,fill = False,palette = 'nipy_spectral',shade = True)\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KernelDensity\n",
    "densities = []\n",
    "Xs = []\n",
    "for j in all_lineages:\n",
    "    dataframe = new_vals_full[new_vals_full['pango_lineage'] == j]\n",
    "    X=dataframe.loc[:,'count_of_sequenced'].values[:, np.newaxis]\n",
    "\n",
    "\n",
    "    kde = KernelDensity(kernel='gaussian').fit(X)  \n",
    "    log_density_values=kde.score_samples(X)\n",
    "    density=np.exp(log_density_values)\n",
    "    densities.append(density)\n",
    "    Xs.append(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['red','orange','yellow','green','blue','purple','violet','gold','hotpink','olive']\n",
    "counter = 0\n",
    "new = []\n",
    "for x in range(len(densities)):\n",
    "    new_d = [abs(y - max(densities[x])) for y in densities[x]]\n",
    "    new.append(new_d)\n",
    "plt.figure(figsize = (15,15))\n",
    "for x in new:\n",
    "    plt.plot(x,alpha = .5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from dateutil.parser import parse\n",
    "\n",
    "\n",
    "# Multiplicative Decomposition \n",
    "result_mul = seasonal_decompose(for_prevalence['value'], model='multiplicative', extrapolate_trend='freq')\n",
    "\n",
    "# Additive Decomposition\n",
    "result_add = seasonal_decompose(df['value'], model='additive', extrapolate_trend='freq')\n",
    "\n",
    "# Plot\n",
    "plt.rcParams.update({'figure.figsize': (10,10)})\n",
    "result_mul.plot().suptitle('Multiplicative Decompose', fontsize=22)\n",
    "result_add.plot().suptitle('Additive Decompose', fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in days:\n",
    "    if x not in for_prevalence['date']:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lineage_names = nx.get_node_attributes(all_all_graph, \"sequence_lineage\")\n",
    "new_lineage_names = {key:str(value).replace('nan','None').replace('NOT SEQUENCED','None') for (key,value) in new_lineage_names.items()}\n",
    "nx.set_node_attributes(all_all_graph, new_lineage_names, name=\"sequence_lineage\")\n",
    "graph_attributes = list(set(new_lineage_names.values()))\n",
    "\n",
    "#[[PL],[MinPL,MaxPl,MedPL] [C, S,KT,KA] \n",
    "# KT = percent total, KA = percent found attributes, S = structure size, c = count, PL = path length\n",
    "all_node_details = []\n",
    "num_levels = 1\n",
    "total_in_structure = 0\n",
    "for node in all_all_graph.nodes(data = True):\n",
    "    node_details = {node[0]:{i:[[],[],[],[]] for i in graph_attributes}}\n",
    "    node_details[node[0]][node[1]['sequence_lineage']][0].append(0)\n",
    "    actual_lineage = node[1]['sequence_lineage']\n",
    "    for counter in range(1,num_levels+1):\n",
    "        level = nx.descendants_at_distance(all_all_graph,node[0],counter)\n",
    "        total_in_structure += len(level)+1\n",
    "        for secondary_node in all_all_graph.nodes(data = True):\n",
    "            if secondary_node[0] in level:\n",
    "                test_date_node = barcode_to_test_date_dict[node[0]]\n",
    "                test_date_secondary_node =  barcode_to_test_date_dict[secondary_node[0]]\n",
    "                symp_date_node = barcode_to_symptom_onset_dict[node[0]]\n",
    "                symp_date_secondary_node = barcode_to_symptom_onset_dict[secondary_node[0]] \n",
    "                if symp_date_node == 'asymptomatic' or symp_date_node == 'unknown':\n",
    "                    node_date = test_date_node\n",
    "                else:\n",
    "                    node_date = symp_date_node\n",
    "                if symp_date_secondary_node == 'asymptomatic' or symp_date_secondary_node == 'unknown':\n",
    "                    secondary_node_date = test_date_secondary_node\n",
    "                else:\n",
    "                    secondary_node_date = symp_date_secondary_node\n",
    "                \n",
    "                days_diff = (datetime.strptime(node_date, '%Y-%m-%d') - datetime.strptime(secondary_node_date, '%Y-%m-%d')).days\n",
    "                if days_diff < 10:\n",
    "                    #check if interaction is in this time span\n",
    "                    node_details[node[0]][secondary_node[1]['sequence_lineage']][0].append(counter)\n",
    "    for attribute in node_details[node[0]]:\n",
    "        node_details[node[0]][attribute][1]= [min(node_details[node[0]][attribute][0],default = np.nan),max(node_details[node[0]][attribute][0],default = np.nan)]\n",
    "        if len(node_details[node[0]][attribute][0]) == 0:\n",
    "            node_details[node[0]][attribute][1].append(0)\n",
    "        else:\n",
    "            node_details[node[0]][attribute][1].append(float(sum(node_details[node[0]][attribute][0])/len(node_details[node[0]][attribute][0])))\n",
    "        node_details[node[0]][attribute][2].append(len(node_details[node[0]][attribute][0]))\n",
    "        node_details[node[0]][attribute][2].append(total_in_structure)\n",
    "        node_details[node[0]][attribute][2].append(abs((len(node_details[node[0]][attribute][0]))/(total_in_structure)))\n",
    "        if (total_in_structure - len(node_details[node[0]]['None'][0])) != 0:\n",
    "            node_details[node[0]][attribute][2].append(abs(len(node_details[node[0]][attribute][0])/((total_in_structure) - len(node_details[node[0]]['None'][0]))))\n",
    "        else:\n",
    "            node_details[node[0]][attribute][2].append(0.0)\n",
    "        node_details[node[0]][attribute][3].append(actual_lineage)\n",
    "    node_details[node[0]]['None'][2][3]=0.0\n",
    "    all_node_details.append(node_details)\n",
    "\n",
    "\n",
    "\n",
    "new_frames = []\n",
    "for node_details in all_node_details:\n",
    "    for x in node_details:\n",
    "        for y in list(node_details[x]):\n",
    "            node = x\n",
    "            lineage = y\n",
    "            path_dists = node_details[x][y][0]\n",
    "            min_path_dist = node_details[x][y][1][0]\n",
    "            max_path_dist = node_details[x][y][1][1]\n",
    "            median_path_dist = node_details[x][y][1][2]\n",
    "            saturation = node_details[x][y][2][0]\n",
    "            structure_size = node_details[x][y][2][1]\n",
    "            percent_total = node_details[x][y][2][2]\n",
    "            percent_known = node_details[x][y][2][3]\n",
    "            actual_lineage = node_details[x][y][3][0]\n",
    "            new_frames.append([node,lineage,path_dists,min_path_dist,max_path_dist,median_path_dist,saturation,structure_size,percent_total,percent_known,actual_lineage])\n",
    "        \n",
    "decision_frame = pd.DataFrame(data = new_frames, columns = ['node','lineage','path dists','min path dist','max path dist','avg path dist','saturation','structure size','percent total','percent known','actual_lineage'])\n",
    "\n",
    "print(decision_frame)\n",
    "decision_frames_by_node = decision_frame.groupby(['node'])\n",
    "new_decision_frames = []\n",
    "for x in decision_frames_by_node:\n",
    "    decision_list = x[1]\n",
    "    new_decision_frames.append([decision_list.dropna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in new_decision_frames:\n",
    "    dataframe = x[0]\n",
    "    dataframe = dataframe.reset_index(drop = True)\n",
    "    dataframe['lineage presence on campus'] = 'to be calculated'\n",
    "    dataframe['level considered to'] = num_levels\n",
    "    dataframe['Test Date'] = barcode_to_test_date_dict[dataframe['node'][0]]\n",
    "    dataframe['Symptom Onset'] = barcode_to_symptom_onset_dict[dataframe['node'][0]]\n",
    "    dataframe['Wastewater Presence Binary'] = 'to be calculated'\n",
    "    dataframe['Wastewater Presence Proportion'] = 'to be calculated'\n",
    "    print(dataframe)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peacock_IDs = list(peacock['barcode'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genetic_distance = pd.read_csv('genetic_distance_cleaned_refreshed.csv')\n",
    "path_info = pd.read_csv('new_full_cleaned_refreshed.csv',usecols = ['p1','p2','path dist','weighted path dist'])\n",
    "genetic_distance.columns = ['index','p1','p2','snps']\n",
    "path_info.columns = ['p1','p2','path dist','weighted path dist']\n",
    "new_gen_dist = pd.merge(genetic_distance,path_info,on=['p1','p2'],how = 'left')\n",
    "new_gen_dist.to_csv('path_and_gen_dist_refreshed.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cleaned_paths = pd.read_csv('path_and_gen_dist_refreshed.csv',usecols = ['p1','p2','snps','path dist','weighted path dist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cleaned_paths['path_inf_or_not'] = ['infinite' if final_cleaned_paths['path dist'][x] == np.inf else 'finite' for x in range(len(final_cleaned_paths))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cleaned_paths['p1 status'] = ['peacock' if final_cleaned_paths['p1'][x] in peacock_IDs else 'non-peacock' for x in range(len(final_cleaned_paths))]\n",
    "final_cleaned_paths['p2 status'] = ['peacock' if final_cleaned_paths['p2'][x] in peacock_IDs else 'non-peacock' for x in range(len(final_cleaned_paths))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cleaned_paths['peacock'] = ['peacock pair' if (final_cleaned_paths['p1 status'][x] == 'peacock' and final_cleaned_paths['p2 status'][x] == 'peacock') else 'non-peacock pair' for x in range(len(final_cleaned_paths))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barcode_2_lineage = {}\n",
    "for x in range(len(sequenced['barcode'])):\n",
    "    barcode_2_lineage[sequenced['barcode'].iloc[x]] = sequenced['pango_lineage'].iloc[x]\n",
    "    \n",
    "{key:val for key, val in barcode_2_lineage.items() if val != None}\n",
    "{key:val for key, val in barcode_2_lineage.items() if val != np.nan}\n",
    "{key:val for key, val in barcode_2_lineage.items() if val in nodes_to_hold}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cleaned_paths['status'] = final_cleaned_paths['p1 status']+\",\"+final_cleaned_paths['p2 status']\n",
    "final_cleaned_paths['status'] = [len(list(set(x.split(',')))) for x in final_cleaned_paths['status']]\n",
    "final_cleaned_paths = final_cleaned_paths[final_cleaned_paths['p2'].isin(nodes_to_hold)]\n",
    "final_cleaned_paths = final_cleaned_paths[final_cleaned_paths['p1'].isin(nodes_to_hold)]\n",
    "\n",
    "\n",
    "non_peacock_pairings = final_cleaned_paths[final_cleaned_paths['peacock'] == 'non-peacock pair' ]\n",
    "non_peacock_pairings = non_peacock_pairings[non_peacock_pairings['status'] == 1 ]\n",
    "\n",
    "peacock_pairs_df = final_cleaned_paths[final_cleaned_paths['peacock'] == 'peacock pair' ]\n",
    "\n",
    "#make box plot for inf versus non inf for peacock versus non peacock\n",
    "non_peacock_pairings_inf_snps = list(non_peacock_pairings[non_peacock_pairings['path_inf_or_not'] == 'infinite']['snps'])\n",
    "non_peacock_pairings_fin_snps = list(non_peacock_pairings[non_peacock_pairings['path_inf_or_not'] == 'finite']['snps'])\n",
    "\n",
    "peacock_pairings_inf_snps = list(peacock_pairs_df[peacock_pairs_df['path_inf_or_not'] == 'infinite']['snps'])\n",
    "peacock_pairings_fin_snps = list(peacock_pairs_df[peacock_pairs_df['path_inf_or_not'] == 'finite']['snps'])\n",
    "\n",
    "\n",
    "non_peacock_pairings_fin = non_peacock_pairings[non_peacock_pairings['path_inf_or_not'] == 'finite']\n",
    "peacock_pairings_fin = peacock_pairs_df[peacock_pairs_df['path_inf_or_not'] == 'finite']\n",
    "\n",
    "\n",
    "print(scipy.stats.mannwhitneyu(non_peacock_pairings_inf_snps,non_peacock_pairings_fin_snps))\n",
    "print(scipy.stats.mannwhitneyu(peacock_pairings_inf_snps,peacock_pairings_fin_snps))\n",
    "\n",
    "\n",
    "print(scipy.stats.mannwhitneyu(non_peacock_pairings_fin['path dist'],peacock_pairings_fin['path dist']))\n",
    "\n",
    "non_peacock_pairings_fin['type'] = 1\n",
    "peacock_pairings_fin['type'] = 0\n",
    "\n",
    "\n",
    "for_dist_compare = pd.concat([non_peacock_pairings_fin,peacock_pairings_fin], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "u,p = scipy.stats.mannwhitneyu(peacock_pairings_inf_snps,peacock_pairings_fin_snps)\n",
    "print(p,u)\n",
    "\n",
    "medians = final_cleaned_paths[final_cleaned_paths['peacock'] == 'peacock pair' ].groupby(['path_inf_or_not'])['snps'].median().values\n",
    "nobs = final_cleaned_paths[final_cleaned_paths['peacock'] == 'peacock pair' ]['path_inf_or_not'].value_counts().values\n",
    "nobs = [str(x) for x in nobs.tolist()]\n",
    "nobs = [\"n: \" + i for i in nobs]\n",
    " \n",
    "print(medians)\n",
    "print(nobs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#make box plot for inf versus non inf for peacock versus non peacock\n",
    "non_peacock_pairings_inf = list(non_peacock_pairings[non_peacock_pairings['path_inf_or_not'] == 'infinite']['snps'])\n",
    "non_peacock_pairings_fin = list(non_peacock_pairings[non_peacock_pairings['path_inf_or_not'] == 'finite']['snps'])\n",
    "\n",
    "peacock_pairings_inf = list(peacock_pairs_df[peacock_pairs_df['path_inf_or_not'] == 'infinite']['snps'])\n",
    "peacock_pairings_fin = list(peacock_pairs_df[peacock_pairs_df['path_inf_or_not'] == 'finite']['snps'])\n",
    "\n",
    "print(scipy.stats.mannwhitneyu(non_peacock_pairings_inf,non_peacock_pairings_fin))\n",
    "print(scipy.stats.mannwhitneyu(peacock_pairings_inf,peacock_pairings_fin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_gen_dist_path_dist = final_cleaned_paths[final_cleaned_paths['path_inf_or_not']=='infinite']\n",
    "non_inf_gen_dist_path_dist = final_cleaned_paths[final_cleaned_paths['path_inf_or_not']=='finite']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print('correlation path, snps, non infinite (all) ')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist['path dist'], b=non_inf_gen_dist_path_dist['snps']))\n",
    "\n",
    "print()\n",
    "print('correlation weighted path, snps, non infinite (all) ')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist['weighted path dist'], b=non_inf_gen_dist_path_dist['snps']))\n",
    "\n",
    "\n",
    "print()\n",
    "print('correlation path, snps, non infinite (all) snps < 40')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<40]['path dist'], b=non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<40]['snps']))\n",
    "\n",
    "print()\n",
    "print('correlation weighted path, snps, non infinite (all) snps < 40')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<40]['weighted path dist'], b=non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<40]['snps']))\n",
    "\n",
    "\n",
    "print()\n",
    "print('correlation path, snps, non infinite (all) snps < 30')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<30]['path dist'], b=non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<30]['snps']))\n",
    "\n",
    "print()\n",
    "print('correlation weighted path, snps, non infinite (all) snps < 30')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<30]['weighted path dist'], b=non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<30]['snps']))\n",
    "\n",
    "\n",
    "print()\n",
    "print('correlation path, snps, non infinite (all) snps < 25')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<25]['path dist'], b=non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<25]['snps']))\n",
    "\n",
    "print()\n",
    "print('correlation weighted path, snps, non infinite (all) snps < 25')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<25]['weighted path dist'], b=non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<25]['snps']))\n",
    "\n",
    "\n",
    "print()\n",
    "print('correlation path, snps, non infinite (all) snps < 20')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<20]['path dist'], b=non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<20]['snps']))\n",
    "\n",
    "print()\n",
    "print('correlation weighted path, snps, non infinite (all) snps < 20')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<20]['weighted path dist'], b=non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<20]['snps']))\n",
    "\n",
    "\n",
    "print()\n",
    "print('correlation path, snps, non infinite (all) snps < 15')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<15]['path dist'], b=non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<15]['snps']))\n",
    "\n",
    "print()\n",
    "print('correlation weighted path, snps, non infinite (all) snps < 15')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<15]['weighted path dist'], b=non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<15]['snps']))\n",
    "\n",
    "\n",
    "print()\n",
    "print('correlation path, snps, non infinite (all) snps < 10')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<10]['path dist'], b=non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<10]['snps']))\n",
    "\n",
    "print()\n",
    "print('correlation weighted path, snps, non infinite (all) snps < 10')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<10]['weighted path dist'], b=non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<10]['snps']))\n",
    "\n",
    "print()\n",
    "print('correlation path, snps, non infinite (all) snps < 5')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<5]['path dist'], b=non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<5]['snps']))\n",
    "\n",
    "print()\n",
    "print('correlation weighted path, snps, non infinite (all) snps < 5')\n",
    "print(scipy.stats.spearmanr(non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<5]['weighted path dist'], b=non_inf_gen_dist_path_dist[non_inf_gen_dist_path_dist['snps']<5]['snps']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('running')\n",
    "pe_pe_sum = []\n",
    "pe_n_sum = []\n",
    "pe_pe_median = []\n",
    "pe_n_median = []\n",
    "pe_pe_ints = []\n",
    "pe_n_ints = []\n",
    "pe_p_sum = []\n",
    "pe_p_median = []\n",
    "pe_p_ints = []\n",
    "\n",
    "for j in final_spring:\n",
    "    j.columns = ['day','s1','s2','sum duration','median duration','num interactions']\n",
    "    day = j['day'].iloc[0]\n",
    "    g = nx.from_pandas_edgelist(j,'s1','s2',['sum duration','median duration','num interactions'], create_using = nx.Graph())\n",
    "    rolling_positives = by_date_pos_list_spring_dict[str(day)]\n",
    "    edges = list(g.edges(data = True))\n",
    "    for x in edges:\n",
    "        if (str(x[0]) in rolling_positives) and (str(x[0]) in peacock_IDs): \n",
    "            if (str(x[1]) in rolling_positives) and (str(x[1]) in peacock_IDs): #pea - pea\n",
    "                pe_pe_sum.append(x[2]['sum duration'])\n",
    "                pe_pe_median.append(x[2]['median duration'])\n",
    "                pe_pe_ints.append(x[2]['num interactions'])\n",
    "            elif (str(x[1]) in rolling_positives) and (str(x[1]) not in peacock_IDs): #pea - p\n",
    "                pe_p_sum.append(x[2]['sum duration'])\n",
    "                pe_p_median.append(x[2]['median duration'])\n",
    "                pe_p_ints.append(x[2]['num interactions'])\n",
    "            else:                                                               #pea - n\n",
    "                pe_n_sum.append(x[2]['sum duration'])\n",
    "                pe_n_median.append(x[2]['median duration'])\n",
    "                pe_n_ints.append(x[2]['num interactions'])\n",
    "        if (str(x[1]) in rolling_positives) and (str(x[1]) in peacock_IDs): \n",
    "            if (str(x[0]) in rolling_positives) and (str(x[0]) not in peacock_IDs): #p - pea\n",
    "                pe_p_sum.append(x[2]['sum duration'])\n",
    "                pe_p_median.append(x[2]['median duration'])\n",
    "                pe_p_ints.append(x[2]['num interactions'])\n",
    "            elif (str(x[0]) not in rolling_positives) and (str(x[0]) not in peacock_IDs):   #n - pea\n",
    "                pe_n_sum.append(x[2]['sum duration'])\n",
    "                pe_n_median.append(x[2]['median duration'])\n",
    "                pe_n_ints.append(x[2]['num interactions'])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "058e3c7b87c7a589515811820a5e79ea24e73f048f8140cc0f8df6e32bbae3e0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
